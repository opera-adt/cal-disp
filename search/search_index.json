{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#cli","title":"CLI","text":""},{"location":"api/#cal_disp.cli","title":"cal_disp.cli","text":""},{"location":"api/#cal_disp.cli.cli_app","title":"cli_app","text":"<pre><code>cli_app(ctx: Context, debug: bool) -&gt; None\n</code></pre> <p>Run a DISP calibration workflow.</p> Source code in <code>src/cal_disp/cli/__init__.py</code> <pre><code>@click.group(name=\"cal-disp\")\n@click.version_option()\n@click.option(\"--debug\", is_flag=True, help=\"Add debug messages to the log.\")\n@click.pass_context\ndef cli_app(ctx: click.Context, debug: bool) -&gt; None:\n    \"\"\"Run a DISP calibration workflow.\"\"\"\n    # https://click.palletsprojects.com/en/8.1.x/commands/#nested-handling-and-contexts\n    ctx.ensure_object(dict)\n    ctx.obj[\"debug\"] = debug\n</code></pre>"},{"location":"api/#cal_disp.cli.download","title":"download","text":""},{"location":"api/#cal_disp.cli.download.burst_bounds","title":"burst_bounds","text":"<pre><code>burst_bounds(input_file: Path, output_dir: Path) -&gt; None\n</code></pre> <p>Download S1 CSLC boundary tiles for a DISP-S1 file.</p> <p>Extracts frame ID and sensing times from the DISP-S1 filename, then downloads corresponding Sentinel-1 burst boundary geometries. Output directory is created if it doesn't exist.</p> <p>Only supports DISP-S1 products (sensor must be S1).</p> <p>Examples:</p> <p>Basic usage:     $ cal-disp download burst-bounds -i disp_product.nc -o ./burst_data</p> <p>Full path example:     $ cal-disp download burst-bounds \\     -i OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0.nc \\     -o ./burst_bounds</p> Source code in <code>src/cal_disp/cli/download.py</code> <pre><code>@download_group.command(name=\"burst-bounds\")\n@click.option(\n    \"--input-file\",\n    \"-i\",\n    type=click.Path(exists=True, dir_okay=False, path_type=Path),\n    required=True,\n    help=\"Path to OPERA DISP-S1 product (.nc file).\",\n)\n@click.option(\n    \"--output-dir\",\n    \"-o\",\n    type=click.Path(file_okay=False, path_type=Path),\n    required=True,\n    help=\"Directory to save burst boundary tiles.\",\n)\ndef burst_bounds(\n    input_file: Path,\n    output_dir: Path,\n) -&gt; None:\n    r\"\"\"Download S1 CSLC boundary tiles for a DISP-S1 file.\n\n    Extracts frame ID and sensing times from the DISP-S1 filename,\n    then downloads corresponding Sentinel-1 burst boundary geometries.\n    Output directory is created if it doesn't exist.\n\n    Only supports DISP-S1 products (sensor must be S1).\n\n    Examples\n    --------\n    Basic usage:\n        $ cal-disp download burst-bounds -i disp_product.nc -o ./burst_data\n\n    Full path example:\n        $ cal-disp download burst-bounds \\\\\n        -i OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0.nc \\\\\n        -o ./burst_bounds\n\n    \"\"\"\n    from cal_disp.download import generate_s1_burst_tiles\n\n    # Parse filename: OPERA_L3_DISP-S1_IW_F{frame}_VV_{dates}...\n    parts = input_file.stem.split(\"_\")\n    sensor = parts[2].split(\"-\")[1]  # DISP-S1 -&gt; S1\n    frame_id = int(parts[4].lstrip(\"F\"))  # F08882 -&gt; 8882\n\n    if sensor != \"S1\":\n        raise click.ClickException(f\"Only DISP-S1 products supported, got: {sensor}\")\n\n    sensing_times = extract_sensing_times_from_file(input_file)\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    for sensing_time in sensing_times:\n        click.echo(f\"Downloading burst bounds for {sensing_time}\")\n        generate_s1_burst_tiles(\n            frame_id=frame_id,\n            sensing_time=sensing_time,\n            output_dir=output_dir,\n        )\n\n    click.echo(f\"Download complete: {output_dir}\")\n</code></pre>"},{"location":"api/#cal_disp.cli.download.disp_s1","title":"disp_s1","text":"<pre><code>disp_s1(frame_id: int, output_dir: Path, start: datetime | None, end: datetime | None, num_workers: int) -&gt; None\n</code></pre> <p>Download OPERA DISP-S1 products for a frame.</p> <p>Downloads displacement products from the OPERA DISP-S1 archive for the specified frame and date range. Products are filtered based on the secondary date of each interferogram.</p> <p>Examples:</p> <p>Download all products for a frame:     $ cal-disp download disp-s1 --frame-id 8882 -o ./disp_data</p> <p>Download products for specific date range:     $ cal-disp download disp-s1 --frame-id 8882 -o ./disp_data \\         --start 2022-07-01 --end 2022-07-31</p> <p>Use more workers for faster downloads:     $ cal-disp download disp-s1 --frame-id 8882 -o ./disp_data -n 8</p> Source code in <code>src/cal_disp/cli/download.py</code> <pre><code>@download_group.command()\n@click.option(\n    \"--frame-id\",\n    type=int,\n    required=True,\n    help=\"OPERA DISP-S1 frame ID.\",\n    show_default=True,\n)\n@click.option(\n    \"--output-dir\",\n    \"-o\",\n    type=click.Path(file_okay=False, path_type=Path),\n    required=True,\n    help=\"Directory to save downloaded DISP products.\",\n)\n@click.option(\n    \"--start\",\n    \"-s\",\n    type=click.DateTime(formats=[\"%Y-%m-%d\"]),\n    default=None,\n    help=\"Start date (YYYY-MM-DD). Based on secondary date of DISP.\",\n)\n@click.option(\n    \"--end\",\n    \"-e\",\n    type=click.DateTime(formats=[\"%Y-%m-%d\"]),\n    default=None,\n    help=\"End date (YYYY-MM-DD). Based on secondary date of DISP.\",\n)\n@click.option(\n    \"--num-workers\",\n    \"-n\",\n    type=int,\n    default=1,\n    help=\"Number of parallel download workers.\",\n    show_default=True,\n)\ndef disp_s1(\n    frame_id: int,\n    output_dir: Path,\n    start: datetime | None,\n    end: datetime | None,\n    num_workers: int,\n) -&gt; None:\n    r\"\"\"Download OPERA DISP-S1 products for a frame.\n\n    Downloads displacement products from the OPERA DISP-S1 archive for\n    the specified frame and date range. Products are filtered based on\n    the secondary date of each interferogram.\n\n    Examples\n    --------\n    Download all products for a frame:\n        $ cal-disp download disp-s1 --frame-id 8882 -o ./disp_data\n\n    Download products for specific date range:\n        $ cal-disp download disp-s1 --frame-id 8882 -o ./disp_data \\\\\n            --start 2022-07-01 --end 2022-07-31\n\n    Use more workers for faster downloads:\n        $ cal-disp download disp-s1 --frame-id 8882 -o ./disp_data -n 8\n\n    \"\"\"\n    from cal_disp.download import download_disp\n\n    download_disp(\n        frame_id=frame_id,\n        output_dir=output_dir,\n        start=start,\n        end=end,\n        num_workers=num_workers,\n    )\n    click.echo(f\"Download complete: {output_dir}\")\n</code></pre>"},{"location":"api/#cal_disp.cli.download.download_group","title":"download_group","text":"<pre><code>download_group()\n</code></pre> <p>Sub-commands for downloading prerequisite data.</p> Source code in <code>src/cal_disp/cli/download.py</code> <pre><code>@click.group(name=\"download\")\ndef download_group():\n    \"\"\"Sub-commands for downloading prerequisite data.\"\"\"\n    from cal_disp._log import setup_logging\n\n    setup_logging(logger_name=\"cal_disp\")\n</code></pre>"},{"location":"api/#cal_disp.cli.download.tropo","title":"tropo","text":"<pre><code>tropo(input_file: Path, output_dir: Path, num_workers: int, interp: bool) -&gt; None\n</code></pre> <p>Download OPERA TROPO for a DISP-S1 file.</p> <p>Extracts sensing times from the input DISP-S1 product filename and downloads corresponding OPERA TROPO data. Output directory is created if it doesn't exist.</p> <p>The input file must follow OPERA naming convention: OPERA_L3_DISP-S1_IW_F{frame}VV{ref_date}{sec_date}_v1.0{proc_date}.nc</p> <p>Examples:</p> <p>Basic usage:     $ cal-disp download tropo -i disp_product.nc -o ./tropo_data</p> <p>With temporal interpolation:     $ cal-disp download tropo -i disp_product.nc -o ./tropo_data --interp</p> <p>Using more workers:     $ cal-disp download tropo -i disp_product.nc -o ./tropo_data -n 8</p> Source code in <code>src/cal_disp/cli/download.py</code> <pre><code>@download_group.command()\n@click.option(\n    \"--input-file\",\n    \"-i\",\n    type=click.Path(exists=True, dir_okay=False, path_type=Path),\n    required=True,\n    help=\"Path to OPERA DISP-S1 product (.nc file).\",\n)\n@click.option(\n    \"--output-dir\",\n    \"-o\",\n    type=click.Path(file_okay=False, path_type=Path),\n    required=True,\n    help=\"Directory to save downloaded TROPO products.\",\n)\n@click.option(\n    \"--num-workers\",\n    \"-n\",\n    type=int,\n    default=4,\n    help=\"Number of parallel download workers.\",\n    show_default=True,\n)\n@click.option(\n    \"--interp\",\n    is_flag=True,\n    help=\"Download 2 scenes per date for temporal interpolation.\",\n)\ndef tropo(\n    input_file: Path,\n    output_dir: Path,\n    num_workers: int,\n    interp: bool,\n) -&gt; None:\n    \"\"\"Download OPERA TROPO for a DISP-S1 file.\n\n    Extracts sensing times from the input DISP-S1 product filename and\n    downloads corresponding OPERA TROPO data. Output directory\n    is created if it doesn't exist.\n\n    The input file must follow OPERA naming convention:\n    OPERA_L3_DISP-S1_IW_F{frame}_VV_{ref_date}_{sec_date}_v1.0_{proc_date}.nc\n\n    Examples\n    --------\n    Basic usage:\n        $ cal-disp download tropo -i disp_product.nc -o ./tropo_data\n\n    With temporal interpolation:\n        $ cal-disp download tropo -i disp_product.nc -o ./tropo_data --interp\n\n    Using more workers:\n        $ cal-disp download tropo -i disp_product.nc -o ./tropo_data -n 8\n\n    \"\"\"\n    from cal_disp.download import download_tropo\n\n    sensing_times = extract_sensing_times_from_file(input_file)\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    download_tropo(\n        disp_times=sensing_times,\n        output_dir=output_dir,\n        num_workers=num_workers,\n        interp=interp,\n    )\n    click.echo(f\"Download complete: {output_dir}\")\n</code></pre>"},{"location":"api/#cal_disp.cli.download.unr","title":"unr","text":"<pre><code>unr(frame_id: int, output_dir: Path, start: datetime | None, end: datetime | None, margin: float) -&gt; None\n</code></pre> <p>Download UNR GPS timeseries data for a DISP-S1 frame.</p> <p>Downloads GPS timeseries grid from the Nevada Geodetic Laboratory (UNR) within the frame's bounding box (expanded by margin). Data is saved as a parquet file for efficient loading. Output directory is created if it doesn't exist.</p> <p>Examples:</p> <p>Download UNR data for a frame:     $ cal-disp download unr --frame-id 8882 -o ./unr_data</p> <p>With date range:     $ cal-disp download unr --frame-id 8882 -o ./unr_data \\         --start 2022-01-01 --end 2023-12-31</p> <p>Expand bounding box by 1 degree:     $ cal-disp download unr --frame-id 8882 -o ./unr_data -m 1.0</p> Source code in <code>src/cal_disp/cli/download.py</code> <pre><code>@download_group.command()\n@click.option(\n    \"--frame-id\",\n    type=int,\n    required=True,\n    help=\"OPERA DISP-S1 frame ID.\",\n    show_default=True,\n)\n@click.option(\n    \"--output-dir\",\n    \"-o\",\n    type=click.Path(file_okay=False, path_type=Path),\n    required=True,\n    help=\"Directory to save downloaded UNR parquet file.\",\n)\n@click.option(\n    \"--start\",\n    \"-s\",\n    type=click.DateTime(formats=[\"%Y-%m-%d\"]),\n    default=None,\n    help=\"Start date of timeseries (YYYY-MM-DD).\",\n)\n@click.option(\n    \"--end\",\n    \"-e\",\n    type=click.DateTime(formats=[\"%Y-%m-%d\"]),\n    default=None,\n    help=\"End date of timeseries (YYYY-MM-DD).\",\n)\n@click.option(\n    \"--margin\",\n    \"-m\",\n    type=float,\n    default=0.5,\n    help=\"Margin in degrees to expand frame bounding box.\",\n    show_default=True,\n)\ndef unr(\n    frame_id: int,\n    output_dir: Path,\n    start: datetime | None,\n    end: datetime | None,\n    margin: float,\n) -&gt; None:\n    r\"\"\"Download UNR GPS timeseries data for a DISP-S1 frame.\n\n    Downloads GPS timeseries grid from the Nevada Geodetic Laboratory (UNR)\n    within the frame's bounding box (expanded by margin). Data is saved\n    as a parquet file for efficient loading. Output directory is created\n    if it doesn't exist.\n\n    Examples\n    --------\n    Download UNR data for a frame:\n        $ cal-disp download unr --frame-id 8882 -o ./unr_data\n\n    With date range:\n        $ cal-disp download unr --frame-id 8882 -o ./unr_data \\\\\n            --start 2022-01-01 --end 2023-12-31\n\n    Expand bounding box by 1 degree:\n        $ cal-disp download unr --frame-id 8882 -o ./unr_data -m 1.0\n\n    \"\"\"\n    from cal_disp.download import download_unr_grid\n\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    download_unr_grid(\n        frame_id=frame_id,\n        output_dir=output_dir,\n        start=start,\n        end=end,\n        margin_deg=margin,\n    )\n    click.echo(f\"Download complete: {output_dir}\")\n</code></pre>"},{"location":"api/#download","title":"Download","text":""},{"location":"api/#cal_disp.download","title":"cal_disp.download","text":""},{"location":"api/#cal_disp.download.download_disp","title":"download_disp","text":"<pre><code>download_disp(frame_id: int, output_dir: Path, start: datetime | None = None, end: datetime | None = None, num_workers: int = DEFAULT_NUM_WORKERS) -&gt; None\n</code></pre> <p>Download DISP-S1 products for a frame.</p> <p>Downloads displacement products from the OPERA DISP-S1 archive for the specified frame and date range. Products are filtered based on the secondary date of each interferogram.</p> <p>Parameters:</p> Name Type Description Default <code>frame_id</code> <code>int</code> <p>OPERA frame identifier.</p> required <code>output_dir</code> <code>Path</code> <p>Directory where products will be saved.</p> required <code>start</code> <code>datetime or None</code> <p>Start date for query (based on secondary date). Default is None (no start limit).</p> <code>None</code> <code>end</code> <code>datetime or None</code> <p>End date for query (based on secondary date). Default is None (no end limit).</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>Number of parallel download workers. Default is 2.</p> <code>DEFAULT_NUM_WORKERS</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If frame ID is not in the database or no products are found.</p> Notes <p>Date queries are based on the secondary (later) date of each interferometric pair. If start and end dates are identical, the range is automatically expanded by \u00b11 day and num_workers is set to 1 to ensure the specific product is captured.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; download_frame_products(\n...     frame_id=8887,\n...     output_dir=Path(\"./data\"),\n...     start=datetime(2024, 1, 1),\n...     end=datetime(2024, 12, 31)\n... )\n</code></pre> Source code in <code>src/cal_disp/download/_stage_disp.py</code> <pre><code>def download_disp(\n    frame_id: int,\n    output_dir: Path,\n    start: datetime | None = None,\n    end: datetime | None = None,\n    num_workers: int = DEFAULT_NUM_WORKERS,\n) -&gt; None:\n    \"\"\"Download DISP-S1 products for a frame.\n\n    Downloads displacement products from the OPERA DISP-S1 archive for\n    the specified frame and date range. Products are filtered based on\n    the secondary date of each interferogram.\n\n    Parameters\n    ----------\n    frame_id : int\n        OPERA frame identifier.\n    output_dir : Path\n        Directory where products will be saved.\n    start : datetime or None, optional\n        Start date for query (based on secondary date).\n        Default is None (no start limit).\n    end : datetime or None, optional\n        End date for query (based on secondary date).\n        Default is None (no end limit).\n    num_workers : int, optional\n        Number of parallel download workers. Default is 2.\n\n    Raises\n    ------\n    ValueError\n        If frame ID is not in the database or no products are found.\n\n    Notes\n    -----\n    Date queries are based on the secondary (later) date of each\n    interferometric pair. If start and end dates are identical,\n    the range is automatically expanded by \u00b11 day and num_workers\n    is set to 1 to ensure the specific product is captured.\n\n    Examples\n    --------\n    &gt;&gt;&gt; download_frame_products(\n    ...     frame_id=8887,\n    ...     output_dir=Path(\"./data\"),\n    ...     start=datetime(2024, 1, 1),\n    ...     end=datetime(2024, 12, 31)\n    ... )\n\n    \"\"\"\n    logger.info(f\"Downloading DISP-S1 products for frame {frame_id}\")\n\n    start_adjusted, end_adjusted = _adjust_single_date_range(start, end)\n    logger.info(f\"Search window: {start_adjusted} - {end_adjusted}\")\n\n    workers = 1 if (start and end and start == end) else num_workers\n\n    run_download(\n        frame_id,\n        start_datetime=start_adjusted,\n        end_datetime=end_adjusted,\n        output_dir=Path(output_dir),\n        num_workers=workers,\n    )\n\n    logger.info(f\"Download complete: files saved to {output_dir}\")\n</code></pre>"},{"location":"api/#cal_disp.download.download_tropo","title":"download_tropo","text":"<pre><code>download_tropo(disp_times: list[datetime | Timestamp], output_dir: Path | str, num_workers: int = 4, interp: bool = True) -&gt; None\n</code></pre> <p>Download tropospheric correction data for displacement times.</p> <p>Parameters:</p> Name Type Description Default <code>disp_times</code> <code>list of datetime or pd.Timestamp</code> <p>Displacement measurement times</p> required <code>output_dir</code> <code>Path or str</code> <p>Output directory for downloads</p> required <code>num_workers</code> <code>int</code> <p>Parallel download workers</p> <code>4</code> <code>interp</code> <code>bool</code> <p>If True, get 2 scenes per time (for interpolation). If False, get single nearest scene.</p> <code>True</code> Source code in <code>src/cal_disp/download/_stage_tropo.py</code> <pre><code>def download_tropo(\n    disp_times: list[datetime | pd.Timestamp],\n    output_dir: Path | str,\n    num_workers: int = 4,\n    interp: bool = True,\n) -&gt; None:\n    \"\"\"Download tropospheric correction data for displacement times.\n\n    Parameters\n    ----------\n    disp_times : list of datetime or pd.Timestamp\n        Displacement measurement times\n    output_dir : Path or str\n        Output directory for downloads\n    num_workers : int\n        Parallel download workers\n    interp : bool\n        If True, get 2 scenes per time (for interpolation).\n        If False, get single nearest scene.\n\n    \"\"\"\n    out = Path(output_dir)\n    out.mkdir(exist_ok=True, parents=True)\n\n    n_scenes = 2 if interp else 1\n    all_scenes = []\n\n    for t in disp_times:\n        scenes = find_nearest_scenes(t, num_scenes=n_scenes)\n        all_scenes.extend(scenes)\n\n    combined = asf.ASFSearchResults(all_scenes)\n    logger.info(f\"Downloading {len(combined)} scenes to {out}\")\n    combined.download(path=out, processes=num_workers)\n</code></pre>"},{"location":"api/#cal_disp.download.download_unr_grid","title":"download_unr_grid","text":"<pre><code>download_unr_grid(frame_id: int, output_dir: Path, start: datetime | None = None, end: datetime | None = None, margin_deg: float = 0.5, plate: Literal['NA', 'PA', 'IGS14', 'IGS20'] = 'IGS20', version: Literal['0.1', '0.2'] = '0.2') -&gt; None\n</code></pre> <p>Download UNR gridded GNSS timeseries for a given frame.</p> <p>Parameters:</p> Name Type Description Default <code>frame_id</code> <code>int</code> <p>OPERA frame identifier.</p> required <code>output_dir</code> <code>Path</code> <p>Output directory for downloaded data.</p> required <code>start</code> <code>datetime or None</code> <p>Start date for timeseries. If None, downloads from beginning.</p> <code>None</code> <code>end</code> <code>datetime or None</code> <p>End date for timeseries. If None, downloads until present.</p> <code>None</code> <code>margin_deg</code> <code>float</code> <p>Margin in degrees to expand frame bounding box.</p> <code>0.5</code> <code>plate</code> <code>(NA, PA, IGS14, IGS20)</code> <p>Reference plate for velocity computation.</p> <code>\"NA\"</code> <code>version</code> <code>(0.1, 0.2)</code> <p>UNR grid version to download.</p> <code>\"0.1\"</code> Source code in <code>src/cal_disp/download/_stage_unr.py</code> <pre><code>def download_unr_grid(\n    frame_id: int,\n    output_dir: Path,\n    start: datetime | None = None,\n    end: datetime | None = None,\n    margin_deg: float = 0.5,\n    plate: Literal[\"NA\", \"PA\", \"IGS14\", \"IGS20\"] = \"IGS20\",\n    version: Literal[\"0.1\", \"0.2\"] = \"0.2\",\n) -&gt; None:\n    \"\"\"Download UNR gridded GNSS timeseries for a given frame.\n\n    Parameters\n    ----------\n    frame_id : int\n        OPERA frame identifier.\n    output_dir : Path\n        Output directory for downloaded data.\n    start : datetime or None, optional\n        Start date for timeseries. If None, downloads from beginning.\n    end : datetime or None, optional\n        End date for timeseries. If None, downloads until present.\n    margin_deg : float, default=0.5\n        Margin in degrees to expand frame bounding box.\n    plate : {\"NA\", \"PA\", \"IGS14\", \"IGS20\"}, default=\"IGS20\"\n        Reference plate for velocity computation.\n    version : {\"0.1\", \"0.2\"}, default=\"0.2\"\n        UNR grid version to download.\n\n    \"\"\"\n    logger.info(\n        f\"Downloading UNR gridded GNSS timeseries for frame {frame_id} \"\n        f\"from {start or 'beginning'} to {end or 'present'}\"\n    )\n\n    selected_frame = get_frame_geojson([frame_id], as_geodataframe=True)\n    frame_bounds = tuple(selected_frame.bounds.values[0])\n\n    extended_bbox = (\n        frame_bounds[0] - margin_deg,\n        frame_bounds[1] - margin_deg,\n        frame_bounds[2] + margin_deg,\n        frame_bounds[3] + margin_deg,\n    )\n\n    grid = UnrGridSource(version=version)\n    ts_grid_df = grid.timeseries_many(bbox=extended_bbox)\n    ts_grid_df[\"date\"] = pd.to_datetime(ts_grid_df[\"date\"])\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    _save_parquet(ts_grid_df, frame_id, plate, output_dir)\n</code></pre>"},{"location":"api/#cal_disp.download.generate_s1_burst_tiles","title":"generate_s1_burst_tiles","text":"<pre><code>generate_s1_burst_tiles(frame_id: int, sensing_time: datetime, output_dir: Path, time_window_hours: float = 2.0, n_download_processes: int = 5) -&gt; Path\n</code></pre> <p>Generate non-overlapping burst tiles for a frame and sensing time.</p> <p>Downloads CSLC data, processes bursts to create non-overlapping polygons, and saves to GeoJSON. Priority: IW1 &gt; IW2 &gt; IW3, lower burst_id first.</p> <p>Parameters:</p> Name Type Description Default <code>frame_id</code> <code>int</code> <p>OPERA frame identifier.</p> required <code>sensing_time</code> <code>datetime</code> <p>Sensing time to search for CSLC products.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save output GeoJSON and temporary files.</p> required <code>time_window_hours</code> <code>float</code> <p>Time window in hours for searching CSLC products. Default is 2.0.</p> <code>2.0</code> <code>n_download_processes</code> <code>int</code> <p>Number of parallel download processes. Default is 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the generated GeoJSON file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no bursts are found or download fails.</p> Source code in <code>src/cal_disp/download/_stage_burst_bounds.py</code> <pre><code>def generate_s1_burst_tiles(\n    frame_id: int,\n    sensing_time: datetime,\n    output_dir: Path,\n    time_window_hours: float = 2.0,\n    n_download_processes: int = 5,\n) -&gt; Path:\n    \"\"\"Generate non-overlapping burst tiles for a frame and sensing time.\n\n    Downloads CSLC data, processes bursts to create non-overlapping polygons,\n    and saves to GeoJSON. Priority: IW1 &gt; IW2 &gt; IW3, lower burst_id first.\n\n    Parameters\n    ----------\n    frame_id : int\n        OPERA frame identifier.\n    sensing_time : datetime\n        Sensing time to search for CSLC products.\n    output_dir : Path\n        Directory to save output GeoJSON and temporary files.\n    time_window_hours : float, optional\n        Time window in hours for searching CSLC products. Default is 2.0.\n    n_download_processes : int, optional\n        Number of parallel download processes. Default is 5.\n\n    Returns\n    -------\n    Path\n        Path to the generated GeoJSON file.\n\n    Raises\n    ------\n    ValueError\n        If no bursts are found or download fails.\n\n    \"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.info(f\"Starting burst tile generation for frame {frame_id}\")\n\n    # Get burst IDs and EPSG for frame\n    burst_ids = opera_utils.get_burst_ids_for_frame(frame_id)\n    burst_ids = [b.upper() for b in burst_ids]\n    logger.info(f\"Found {len(burst_ids)} burst IDs for frame {frame_id}\")\n\n    epsg = opera_utils.get_frame_bbox(frame_id)[0]\n    logger.debug(f\"Target CRS for frame: EPSG:{epsg}\")\n\n    # Search for CSLC products\n    results = search_cslc_bursts(burst_ids, sensing_time, time_window_hours)\n    logger.info(f\"Found {len(results)} CSLC products\")\n\n    # Download files\n    target_date = sensing_time.date()\n    cslc_files = download_cslc_files(\n        results, output_dir, target_date, n_download_processes\n    )\n    logger.info(f\"Downloaded {len(cslc_files)} CSLC files\")\n\n    # Process bounds\n    cslc_gdf = process_cslc_bounds(cslc_files, epsg=epsg)\n\n    # Create non-overlapping tiles\n    burst_gdf = create_nonoverlapping_tiles(cslc_gdf)\n\n    # Log summary statistics\n    swath_counts = burst_gdf.groupby(\"swath\").size()\n    logger.info(f\"Generated {len(burst_gdf)} non-overlapping tiles\")\n    for swath_num, count in swath_counts.items():\n        logger.info(f\"  IW{swath_num}: {count} tiles\")\n\n    # Save to GeoJSON\n    output_file = output_dir / f\"{target_date}_tiles.geojson\"\n    burst_gdf.to_file(output_file)\n    logger.info(f\"Saved tiles to {output_file}\")\n\n    # Clean up temp directory\n    temp_dir = output_dir / \"tmp\"\n    if temp_dir.exists():\n        shutil.rmtree(temp_dir)\n        logger.debug(f\"Cleaned up temporary directory: {temp_dir}\")\n\n    return output_file\n</code></pre>"},{"location":"api/#cal_disp.download.utils","title":"utils","text":""},{"location":"api/#cal_disp.download.utils.extract_sensing_times_from_file","title":"extract_sensing_times_from_file","text":"<pre><code>extract_sensing_times_from_file(disp_file: Path) -&gt; list[datetime]\n</code></pre> <p>Extract sensing times from DISP-S1 NetCDF filename.</p> <p>Parses DISP-S1 filename to extract reference and secondary dates. Expected format: OPERA_L3_DISP-S1_IW_F{frame}VV{ref_date}{sec_date}_v{version}{prod_date}.nc</p> <p>Parameters:</p> Name Type Description Default <code>disp_file</code> <code>Path</code> <p>Path to DISP-S1 NetCDF file.</p> required <p>Returns:</p> Type Description <code>list[datetime]</code> <p>List of unique sensing times from reference and secondary dates.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If DISP file does not exist.</p> <code>ValueError</code> <p>If dates cannot be parsed from filename.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; filename = Path(\"OPERA_L3_*.nc\")\n&gt;&gt;&gt; times = extract_sensing_times_from_file(filename)\n&gt;&gt;&gt; len(times)\n2\n</code></pre> Source code in <code>src/cal_disp/download/utils.py</code> <pre><code>def extract_sensing_times_from_file(disp_file: Path) -&gt; list[datetime]:\n    \"\"\"Extract sensing times from DISP-S1 NetCDF filename.\n\n    Parses DISP-S1 filename to extract reference and secondary dates.\n    Expected format:\n    OPERA_L3_DISP-S1_IW_F{frame}_VV_{ref_date}_{sec_date}_v{version}_{prod_date}.nc\n\n    Parameters\n    ----------\n    disp_file : Path\n        Path to DISP-S1 NetCDF file.\n\n    Returns\n    -------\n    list[datetime]\n        List of unique sensing times from reference and secondary dates.\n\n    Raises\n    ------\n    FileNotFoundError\n        If DISP file does not exist.\n    ValueError\n        If dates cannot be parsed from filename.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt; filename = Path(\"OPERA_L3_*.nc\")\n    &gt;&gt;&gt; times = extract_sensing_times_from_file(filename)  # doctest: +SKIP\n    &gt;&gt;&gt; len(times)  # doctest: +SKIP\n    2\n\n    \"\"\"\n    if not disp_file.exists():\n        msg = f\"DISP file not found: {disp_file}\"\n        raise FileNotFoundError(msg)\n\n    logger.info(f\"Extracting sensing times from {disp_file.name}\")\n\n    filename = disp_file.name\n    parts = filename.split(\"_\")\n\n    # Find parts matching datetime format (YYYYMMDDTHHMMSSZ)\n    datetime_parts = [p for p in parts if len(p) == 16 and p.endswith(\"Z\") and \"T\" in p]\n\n    if len(datetime_parts) &lt; 2:\n        msg = (\n            f\"Cannot parse reference and secondary dates from filename: {filename}. \"\n            \"Expected format: \"\n            \"OPERA_L3_DISP-S1_IW_F{{frame}}_VV_{{ref_date}}_{{sec_date}}_\"\n            \"v{{version}}_{{prod_date}}.nc\"\n        )\n        raise ValueError(msg)\n\n    ref_date_str = datetime_parts[0]\n    sec_date_str = datetime_parts[1]\n\n    try:\n        ref_date = datetime.strptime(ref_date_str, \"%Y%m%dT%H%M%SZ\")\n        sec_date = datetime.strptime(sec_date_str, \"%Y%m%dT%H%M%SZ\")\n    except ValueError as e:\n        msg = f\"Failed to parse dates from filename {filename}: {e}\"\n        raise ValueError(msg) from e\n\n    sensing_times = sorted({ref_date, sec_date})\n    logger.info(\n        f\"Parsed sensing times: {ref_date.isoformat()} (ref), \"\n        f\"{sec_date.isoformat()} (sec)\"\n    )\n\n    return sensing_times\n</code></pre>"},{"location":"api/#product","title":"Product","text":""},{"location":"api/#cal_disp.product","title":"cal_disp.product","text":""},{"location":"api/#cal_disp.product.CalProduct","title":"CalProduct  <code>dataclass</code>","text":"<p>Calibrated OPERA DISP displacement product.</p> <p>Represents a calibration correction product that should be subtracted from OPERA DISP products. Main group contains calibration at full resolution. Optional model_3d group contains 3D displacement components at coarser resolution.</p> Groups <p>Main group:     - calibration: Correction to subtract from DISP (full resolution)     - calibration_std: Calibration uncertainty (full resolution)</p> <p>model_3d group (optional):     - north_south: North-south displacement component (coarse resolution)     - east_west: East-west displacement component (coarse resolution)     - up_down: Up-down displacement component (coarse resolution)     - north_south_std: Uncertainty in north-south     - east_west_std: Uncertainty in east-west     - up_down_std: Uncertainty in up-down</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the calibration product NetCDF file.</p> required <code>frame_id</code> <code>int</code> <p>OPERA frame identifier.</p> required <code>primary_date</code> <code>datetime</code> <p>Earlier acquisition date (reference).</p> required <code>secondary_date</code> <code>datetime</code> <p>Later acquisition date.</p> required <code>polarization</code> <code>str</code> <p>Radar polarization (e.g., \"VV\", \"VH\").</p> required <code>sensor</code> <code>str</code> <p>Sensor type: \"S1\" (Sentinel-1) or \"NI\" (NISAR).</p> required <code>version</code> <code>str</code> <p>Product version string.</p> required <code>production_date</code> <code>datetime</code> <p>Date when product was generated.</p> required <code>mode</code> <code>str</code> <p>Acquisition mode (e.g., \"IW\" for S1, \"LSAR\" for NI).</p> <code>'IW'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create product with both groups\n&gt;&gt;&gt; cal = CalProduct.create(\n...     calibration=cal_correction,\n...     disp_product=disp,\n...     output_dir=\"output/\",\n...     model_3d={\"north_south\": vel_ns, \"east_west\": vel_ew, \"up_down\": vel_ud},\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Access main calibration\n&gt;&gt;&gt; ds_main = cal.open_dataset()\n&gt;&gt;&gt; calibration = ds_main[\"calibration\"]\n</code></pre> <pre><code>&gt;&gt;&gt; # Access 3D model (coarse resolution)\n&gt;&gt;&gt; ds_model = cal.open_model_3d()\n&gt;&gt;&gt; model_up = ds_model[\"up_down\"]\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>@dataclass\nclass CalProduct:\n    \"\"\"Calibrated OPERA DISP displacement product.\n\n    Represents a calibration correction product that should be subtracted\n    from OPERA DISP products. Main group contains calibration at full\n    resolution. Optional model_3d group contains 3D displacement\n    components at coarser resolution.\n\n    Groups\n    ------\n    Main group:\n        - calibration: Correction to subtract from DISP (full resolution)\n        - calibration_std: Calibration uncertainty (full resolution)\n\n    model_3d group (optional):\n        - north_south: North-south displacement component (coarse resolution)\n        - east_west: East-west displacement component (coarse resolution)\n        - up_down: Up-down displacement component (coarse resolution)\n        - north_south_std: Uncertainty in north-south\n        - east_west_std: Uncertainty in east-west\n        - up_down_std: Uncertainty in up-down\n\n    Parameters\n    ----------\n    path : Path\n        Path to the calibration product NetCDF file.\n    frame_id : int\n        OPERA frame identifier.\n    primary_date : datetime\n        Earlier acquisition date (reference).\n    secondary_date : datetime\n        Later acquisition date.\n    polarization : str\n        Radar polarization (e.g., \"VV\", \"VH\").\n    sensor : str\n        Sensor type: \"S1\" (Sentinel-1) or \"NI\" (NISAR).\n    version : str\n        Product version string.\n    production_date : datetime\n        Date when product was generated.\n    mode : str\n        Acquisition mode (e.g., \"IW\" for S1, \"LSAR\" for NI).\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Create product with both groups\n    &gt;&gt;&gt; cal = CalProduct.create(\n    ...     calibration=cal_correction,\n    ...     disp_product=disp,\n    ...     output_dir=\"output/\",\n    ...     model_3d={\"north_south\": vel_ns, \"east_west\": vel_ew, \"up_down\": vel_ud},\n    ... )\n\n    &gt;&gt;&gt; # Access main calibration\n    &gt;&gt;&gt; ds_main = cal.open_dataset()\n    &gt;&gt;&gt; calibration = ds_main[\"calibration\"]\n\n    &gt;&gt;&gt; # Access 3D model (coarse resolution)\n    &gt;&gt;&gt; ds_model = cal.open_model_3d()\n    &gt;&gt;&gt; model_up = ds_model[\"up_down\"]\n\n    \"\"\"\n\n    path: Path\n    frame_id: int\n    primary_date: datetime\n    secondary_date: datetime\n    polarization: str\n    sensor: str\n    version: str\n    production_date: datetime\n    mode: str = \"IW\"\n\n    # Filename pattern supporting both S1 and NI sensors\n    _PATTERN = re.compile(\n        r\"OPERA_L4_CAL-DISP-(?P&lt;sensor&gt;S1|NI)_\"\n        r\"(?P&lt;mode&gt;\\w+)_\"\n        r\"F(?P&lt;frame_id&gt;\\d+)_\"\n        r\"(?P&lt;pol&gt;\\w+)_\"\n        r\"(?P&lt;primary&gt;\\d{8}T\\d{6}Z)_\"\n        r\"(?P&lt;secondary&gt;\\d{8}T\\d{6}Z)_\"\n        r\"v(?P&lt;version&gt;[\\d.]+)_\"\n        r\"(?P&lt;production&gt;\\d{8}T\\d{6}Z)\"\n        r\"\\.nc$\"\n    )\n\n    # Main group layers (full resolution)\n    CAL_LAYERS = [\n        \"calibration\",  # Main correction (subtract from DISP)\n        \"calibration_std\",  # Uncertainty\n    ]\n\n    # model_3d group layers (coarse resolution)\n    MODEL_3D_LAYERS = [\n        \"north_south\",  # North-south displacement component\n        \"east_west\",  # East-west displacement component\n        \"up_down\",  # Up-down displacement component\n        \"north_south_std\",  # Uncertainty - north\n        \"east_west_std\",  # Uncertainty - east\n        \"up_down_std\",  # Uncertainty - up\n    ]\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate product after construction.\"\"\"\n        self.path = Path(self.path)\n\n        if self.frame_id &lt;= 0:\n            raise ValueError(f\"frame_id must be positive, got {self.frame_id}\")\n\n        if self.secondary_date &lt;= self.primary_date:\n            raise ValueError(\n                f\"Secondary date ({self.secondary_date}) must be after \"\n                f\"primary date ({self.primary_date})\"\n            )\n\n        if self.polarization not in {\"VV\", \"VH\", \"HH\", \"HV\"}:\n            raise ValueError(f\"Invalid polarization: {self.polarization}\")\n\n        if self.sensor not in {\"S1\", \"NI\"}:\n            raise ValueError(f\"Invalid sensor: {self.sensor}. Must be 'S1' or 'NI'\")\n\n    @classmethod\n    def from_path(cls, path: Path | str) -&gt; \"CalProduct\":\n        \"\"\"Parse product metadata from filename.\n\n        Parameters\n        ----------\n        path : Path or str\n            Path to calibration product NetCDF file.\n\n        Returns\n        -------\n        CalProduct\n            Parsed calibration product instance.\n\n        Raises\n        ------\n        ValueError\n            If filename doesn't match OPERA CAL-DISP pattern.\n\n        Examples\n        --------\n        &gt;&gt;&gt; cal = CalProduct.from_path(\n        \"OPERA_L4_CAL-DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251227T123456Z.nc\")\n        &gt;&gt;&gt; cal.sensor\n        'S1'\n\n        \"\"\"\n        path = Path(path)\n        match = cls._PATTERN.match(path.name)\n\n        if not match:\n            raise ValueError(\n                f\"Filename does not match OPERA CAL-DISP pattern: {path.name}\"\n            )\n\n        return cls(\n            path=path,\n            frame_id=int(match.group(\"frame_id\")),\n            primary_date=datetime.strptime(match.group(\"primary\"), \"%Y%m%dT%H%M%SZ\"),\n            secondary_date=datetime.strptime(\n                match.group(\"secondary\"), \"%Y%m%dT%H%M%SZ\"\n            ),\n            polarization=match.group(\"pol\"),\n            sensor=match.group(\"sensor\"),\n            version=match.group(\"version\"),\n            production_date=datetime.strptime(\n                match.group(\"production\"), \"%Y%m%dT%H%M%SZ\"\n            ),\n            mode=match.group(\"mode\"),\n        )\n\n    @classmethod\n    def create(\n        cls,\n        calibration: xr.DataArray,\n        disp_product: \"DispProduct\",\n        output_dir: Path | str,\n        sensor: str = \"S1\",\n        calibration_std: xr.DataArray | None = None,\n        model_3d: dict[str, xr.DataArray] | None = None,\n        model_3d_std: dict[str, xr.DataArray] | None = None,\n        metadata: dict[str, str] | None = None,\n        version: str = \"1.0\",\n    ) -&gt; \"CalProduct\":\n        \"\"\"Create calibration product with optional model_3d group.\n\n        Parameters\n        ----------\n        calibration : xr.DataArray\n            Calibration correction at full DISP resolution.\n        disp_product : DispProduct\n            Original DISP product (for metadata).\n        output_dir : Path or str\n            Output directory for NetCDF file.\n        sensor : str, optional\n            Sensor type: \"S1\" or \"NI\". Default is \"S1\".\n        calibration_std : xr.DataArray or None, optional\n            Calibration uncertainty at full resolution. Default is None.\n        model_3d : dict[str, xr.DataArray] or None, optional\n            3D displacement components (coarse resolution) with keys:\n            \"north_south\", \"east_west\", \"up_down\". Default is None.\n        model_3d_std : dict[str, xr.DataArray] or None, optional\n            3D displacement uncertainties (coarse resolution). Default is None.\n        metadata : dict[str, str] or None, optional\n            Additional metadata (e.g., GNSS reference epoch). Default is None.\n        version : str, optional\n            Product version. Default is \"1.0\".\n\n        Returns\n        -------\n        CalProduct\n            Created calibration product.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from product import DispProduct, CalProduct\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; disp = DispProduct.from_path(\n        \"OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Full resolution calibration\n        &gt;&gt;&gt; cal_full = calibration_at_30m_resolution\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Coarse resolution 3D model (e.g., 90m from GNSS interpolation)\n        &gt;&gt;&gt; model_coarse = {\n        ...     \"north_south\": disp_ns_90m,\n        ...     \"east_west\": disp_ew_90m,\n        ...     \"up_down\": disp_ud_90m,\n        ... }\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; cal = CalProduct.create(\n        ...     calibration=cal_full,\n        ...     disp_product=disp,\n        ...     output_dir=\"output/\",\n        ...     calibration_std=cal_std,\n        ...     model_3d=model_coarse,\n        ...     model_3d_std={\n        ...         \"north_south_std\": disp_ns_std_90m,\n        ...         \"east_west_std\": disp_ew_std_90m,\n        ...         \"up_down_std\": disp_ud_std_90m,\n        ...     },\n        ...     metadata={\n        ...         \"gnss_reference_epoch\": \"2020-01-01T00:00:00Z\",\n        ...         \"model_3d_resolution\": \"90m\",\n        ...     },\n        ... )\n\n        \"\"\"\n        if sensor not in {\"S1\", \"NI\"}:\n            raise ValueError(f\"Invalid sensor: {sensor}. Must be 'S1' or 'NI'\")\n\n        output_dir = Path(output_dir)\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Generate OPERA-compliant filename\n        production_date = datetime.utcnow()\n        filename = (\n            f\"OPERA_L4_CAL-DISP-{sensor}_\"\n            f\"{disp_product.mode}_\"\n            f\"F{disp_product.frame_id:05d}_\"\n            f\"{disp_product.polarization}_\"\n            f\"{disp_product.primary_date:%Y%m%dT%H%M%S}Z_\"\n            f\"{disp_product.secondary_date:%Y%m%dT%H%M%S}Z_\"\n            f\"v{version}_\"\n            f\"{production_date:%Y%m%dT%H%M%S}Z\"\n            \".nc\"\n        )\n\n        output_path = output_dir / filename\n\n        # Build main group dataset (full resolution)\n        data_vars = {\"calibration\": calibration}\n\n        if calibration_std is not None:\n            data_vars[\"calibration_std\"] = calibration_std\n\n        # Create main dataset\n        ds = xr.Dataset(data_vars)\n\n        # Add global attributes\n        ds.attrs.update(\n            {\n                \"product_type\": f\"OPERA_L4_CAL-DISP-{sensor}\",\n                \"sensor\": sensor,\n                \"frame_id\": disp_product.frame_id,\n                \"mode\": disp_product.mode,\n                \"polarization\": disp_product.polarization,\n                \"primary_datetime\": disp_product.primary_date.isoformat(),\n                \"secondary_datetime\": disp_product.secondary_date.isoformat(),\n                \"production_datetime\": production_date.isoformat(),\n                \"product_version\": version,\n                \"description\": (\n                    f\"Calibration correction for {sensor} InSAR displacement (subtract\"\n                    \" from DISP)\"\n                ),\n                \"source_product\": disp_product.filename,\n                \"usage\": (\n                    \"Subtract calibration layer from DISP displacement to obtain\"\n                    \" calibrated displacement\"\n                ),\n            }\n        )\n\n        # Add custom metadata\n        if metadata:\n            ds.attrs.update(metadata)\n\n        # Save main group\n        ds.to_netcdf(output_path, engine=\"h5netcdf\")\n\n        # Create model_3d group if 3D components provided (coarse resolution)\n        if model_3d or model_3d_std:\n            model_data_vars = {}\n\n            # Add 3D displacement components\n            if model_3d:\n                for comp in [\"north_south\", \"east_west\", \"up_down\"]:\n                    if comp in model_3d:\n                        model_data_vars[comp] = model_3d[comp]\n\n            # Add 3D displacement uncertainties\n            if model_3d_std:\n                for comp in [\"north_south_std\", \"east_west_std\", \"up_down_std\"]:\n                    if comp in model_3d_std:\n                        model_data_vars[comp] = model_3d_std[comp]\n\n            if model_data_vars:\n                ds_model = xr.Dataset(model_data_vars)\n\n                # Add model group attributes\n                ds_model.attrs.update(\n                    {\n                        \"description\": (\n                            \"3D displacement model at coarse resolution (e.g., from\"\n                            \" GNSS interpolation or deformation model)\"\n                        ),\n                        \"units\": \"meters\",\n                        \"reference_frame\": \"ENU (East-North-Up)\",\n                    }\n                )\n\n                # Append to existing file as model_3d group\n                ds_model.to_netcdf(\n                    output_path,\n                    mode=\"a\",\n                    group=\"model_3d\",\n                    engine=\"h5netcdf\",\n                )\n\n        return cls(\n            path=output_path,\n            frame_id=disp_product.frame_id,\n            primary_date=disp_product.primary_date,\n            secondary_date=disp_product.secondary_date,\n            polarization=disp_product.polarization,\n            sensor=sensor,\n            version=version,\n            production_date=production_date,\n            mode=disp_product.mode,\n        )\n\n    def open_dataset(self, group: str | None = None) -&gt; xr.Dataset:\n        \"\"\"Open calibration dataset.\n\n        Parameters\n        ----------\n        group : str or None, optional\n            Group to open: None for main, \"model_3d\" for 3D model.\n            Default is None (main group).\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing requested group.\n\n        Raises\n        ------\n        FileNotFoundError\n            If product file does not exist.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Open main calibration (full resolution)\n        &gt;&gt;&gt; ds_main = cal.open_dataset()\n        &gt;&gt;&gt; calibration = ds_main[\"calibration\"]\n\n        &gt;&gt;&gt; # Open model_3d group (coarse resolution)\n        &gt;&gt;&gt; ds_model = cal.open_dataset(group=\"model_3d\")\n        &gt;&gt;&gt; model_up = ds_model[\"up_down\"]\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n        if group == \"model_3d\":\n            return xr.open_dataset(self.path, group=\"model_3d\", engine=\"h5netcdf\")\n\n        return xr.open_dataset(self.path, engine=\"h5netcdf\")\n\n    def open_model_3d(self) -&gt; xr.Dataset:\n        \"\"\"Open model_3d group dataset.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing 3D displacement model at coarse resolution.\n\n        Raises\n        ------\n        FileNotFoundError\n            If product file does not exist.\n        ValueError\n            If model_3d group does not exist.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds_model = cal.open_model_3d()\n        &gt;&gt;&gt; disp_ns = ds_model[\"north_south\"]\n        &gt;&gt;&gt; disp_ew = ds_model[\"east_west\"]\n        &gt;&gt;&gt; disp_up = ds_model[\"up_down\"]\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n        try:\n            return xr.open_dataset(self.path, group=\"model_3d\", engine=\"h5netcdf\")\n        except (OSError, ValueError) as e:\n            raise ValueError(\n                f\"model_3d group not found in {self.filename}. \"\n                \"Product may not contain 3D displacement model.\"\n            ) from e\n\n    def has_model_3d(self) -&gt; bool:\n        \"\"\"Check if product contains model_3d group.\n\n        Returns\n        -------\n        bool\n            True if model_3d group exists.\n\n        \"\"\"\n        try:\n            self.open_model_3d()\n            return True\n        except (FileNotFoundError, ValueError):\n            return False\n\n    def get_epsg(self) -&gt; int | None:\n        \"\"\"Get EPSG code from spatial reference.\"\"\"\n        ds = self.open_dataset()\n\n        if \"spatial_ref\" in ds:\n            crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n            if crs_wkt:\n                crs = CRS.from_wkt(crs_wkt)\n                return crs.to_epsg()\n\n        return None\n\n    def get_bounds(self) -&gt; dict[str, float]:\n        \"\"\"Get bounds in native projection.\"\"\"\n        ds = self.open_dataset()\n\n        x = ds.x.values\n        y = ds.y.values\n\n        return {\n            \"left\": float(x.min()),\n            \"bottom\": float(y.min()),\n            \"right\": float(x.max()),\n            \"top\": float(y.max()),\n        }\n\n    def get_bounds_wgs84(self) -&gt; dict[str, float]:\n        \"\"\"Get bounds transformed to WGS84.\"\"\"\n        ds = self.open_dataset()\n\n        x = ds.x.values\n        y = ds.y.values\n        left = float(x.min())\n        bottom = float(y.min())\n        right = float(x.max())\n        top = float(y.max())\n\n        if \"spatial_ref\" not in ds:\n            raise ValueError(\"Dataset missing spatial_ref\")\n\n        crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n        if not crs_wkt:\n            raise ValueError(\"spatial_ref missing crs_wkt\")\n\n        src_crs = CRS.from_wkt(crs_wkt)\n\n        west, south, east, north = transform_bounds(\n            src_crs,\n            CRS.from_epsg(4326),\n            left,\n            bottom,\n            right,\n            top,\n        )\n\n        return {\n            \"west\": west,\n            \"south\": south,\n            \"east\": east,\n            \"north\": north,\n        }\n\n    def to_geotiff(\n        self,\n        layer: str,\n        output_path: Path | str,\n        group: str | None = None,\n        compress: str = \"DEFLATE\",\n        **kwargs,\n    ) -&gt; Path:\n        \"\"\"Export layer to GeoTIFF.\n\n        Parameters\n        ----------\n        layer : str\n            Name of layer to export.\n        output_path : Path or str\n            Output GeoTIFF path.\n        group : str or None, optional\n            Group containing layer. Default is None (main group).\n        compress : str, optional\n            Compression method. Default is \"DEFLATE\".\n        **kwargs\n            Additional rasterio creation options.\n\n        Returns\n        -------\n        Path\n            Path to created GeoTIFF.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Export main calibration\n        &gt;&gt;&gt; cal.to_geotiff(\"calibration\", \"calibration.tif\")\n\n        &gt;&gt;&gt; # Export 3D model component\n        &gt;&gt;&gt; cal.to_geotiff(\"up_down\", \"model_up.tif\", group=\"model_3d\")\n\n        \"\"\"\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        ds = self.open_dataset(group=group)\n\n        if layer not in ds:\n            available = list(ds.data_vars)\n            group_str = f\" in {group} group\" if group else \"\"\n            raise ValueError(\n                f\"Layer '{layer}' not found{group_str}. Available: {available}\"\n            )\n\n        da = ds[layer]\n        data = da.values\n\n        # Extract spatial information\n        if \"spatial_ref\" in ds:\n            transform = self._get_transform(ds)\n            crs = self._get_crs(ds)\n        else:\n            transform = Affine.translation(\n                float(ds.x.values[0]),\n                float(ds.y.values[0]),\n            ) * Affine.scale(\n                float(ds.x.values[1] - ds.x.values[0]),\n                float(ds.y.values[1] - ds.y.values[0]),\n            )\n            crs = None\n\n        # Write GeoTIFF\n        profile = {\n            \"driver\": \"GTiff\",\n            \"height\": data.shape[0],\n            \"width\": data.shape[1],\n            \"count\": 1,\n            \"dtype\": np.float32,\n            \"transform\": transform,\n            \"compress\": compress,\n            \"tiled\": True,\n            \"blockxsize\": 512,\n            \"blockysize\": 512,\n            **kwargs,\n        }\n\n        if crs:\n            profile[\"crs\"] = crs\n\n        with rasterio.open(output_path, \"w\", **profile) as dst:\n            dst.write(data.astype(np.float32), 1)\n            dst.set_band_description(1, layer)\n\n            # Add OPERA metadata tags\n            dst.update_tags(\n                product_type=f\"OPERA_L4_CAL-DISP-{self.sensor}\",\n                sensor=self.sensor,\n                frame_id=self.frame_id,\n                polarization=self.polarization,\n                primary_date=self.primary_date.isoformat(),\n                secondary_date=self.secondary_date.isoformat(),\n                layer=layer,\n                group=group if group else \"main\",\n            )\n\n        return output_path\n\n    def get_calibration_summary(self) -&gt; dict[str, dict[str, float]]:\n        \"\"\"Get summary statistics of all layers.\n\n        Returns\n        -------\n        dict[str, dict[str, float]]\n            Statistics for main and model_3d groups.\n\n        Examples\n        --------\n        &gt;&gt;&gt; summary = cal.get_calibration_summary()\n        &gt;&gt;&gt; summary[\"main\"][\"calibration\"]\n        {'mean': 0.023, 'std': 0.015, 'min': -0.05, 'max': 0.08}\n        &gt;&gt;&gt; summary[\"model_3d\"][\"up_down\"]\n        {'mean': 0.001, 'std': 0.003, 'min': -0.01, 'max': 0.02}\n\n        \"\"\"\n        summary: dict = {\"main\": {}}\n\n        # Main group\n        ds = self.open_dataset()\n        for var in ds.data_vars:\n            data = ds[var].values\n            valid_data = data[~np.isnan(data)]\n\n            if len(valid_data) &gt; 0:\n                summary[\"main\"][var] = {\n                    \"mean\": float(np.mean(valid_data)),\n                    \"std\": float(np.std(valid_data)),\n                    \"min\": float(np.min(valid_data)),\n                    \"max\": float(np.max(valid_data)),\n                }\n\n        # model_3d group if exists\n        if self.has_model_3d():\n            summary[\"model_3d\"] = {}\n            ds_model = self.open_model_3d()\n\n            for var in ds_model.data_vars:\n                data = ds_model[var].values\n                valid_data = data[~np.isnan(data)]\n\n                if len(valid_data) &gt; 0:\n                    summary[\"model_3d\"][var] = {\n                        \"mean\": float(np.mean(valid_data)),\n                        \"std\": float(np.std(valid_data)),\n                        \"min\": float(np.min(valid_data)),\n                        \"max\": float(np.max(valid_data)),\n                    }\n\n        return summary\n\n    def _get_transform(self, ds: xr.Dataset) -&gt; Affine:\n        \"\"\"Extract affine transform from dataset.\"\"\"\n        gt = ds.spatial_ref.attrs.get(\"GeoTransform\")\n        if gt is None:\n            raise ValueError(\"No GeoTransform found in spatial_ref\")\n\n        vals = [float(x) for x in gt.split()]\n        return Affine(vals[1], vals[2], vals[0], vals[4], vals[5], vals[3])\n\n    def _get_crs(self, ds: xr.Dataset) -&gt; str:\n        \"\"\"Extract CRS from dataset.\"\"\"\n        crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n        if crs_wkt is None:\n            raise ValueError(\"No crs_wkt found in spatial_ref\")\n        return crs_wkt\n\n    @property\n    def baseline_days(self) -&gt; int:\n        \"\"\"Temporal baseline in days.\"\"\"\n        return (self.secondary_date - self.primary_date).days\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Product filename.\"\"\"\n        return self.path.name\n\n    @property\n    def exists(self) -&gt; bool:\n        \"\"\"Check if product file exists.\"\"\"\n        return self.path.exists()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation.\"\"\"\n        model_str = \"+model_3d\" if self.exists and self.has_model_3d() else \"\"\n        return (\n            f\"CalProduct(sensor={self.sensor}, frame={self.frame_id}, \"\n            f\"{self.primary_date.date()} \u2192 {self.secondary_date.date()}, \"\n            f\"{self.polarization}{model_str})\"\n        )\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.baseline_days","title":"baseline_days  <code>property</code>","text":"<pre><code>baseline_days: int\n</code></pre> <p>Temporal baseline in days.</p>"},{"location":"api/#cal_disp.product.CalProduct.exists","title":"exists  <code>property</code>","text":"<pre><code>exists: bool\n</code></pre> <p>Check if product file exists.</p>"},{"location":"api/#cal_disp.product.CalProduct.filename","title":"filename  <code>property</code>","text":"<pre><code>filename: str\n</code></pre> <p>Product filename.</p>"},{"location":"api/#cal_disp.product.CalProduct.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(calibration: DataArray, disp_product: DispProduct, output_dir: Path | str, sensor: str = 'S1', calibration_std: DataArray | None = None, model_3d: dict[str, DataArray] | None = None, model_3d_std: dict[str, DataArray] | None = None, metadata: dict[str, str] | None = None, version: str = '1.0') -&gt; CalProduct\n</code></pre> <p>Create calibration product with optional model_3d group.</p> <p>Parameters:</p> Name Type Description Default <code>calibration</code> <code>DataArray</code> <p>Calibration correction at full DISP resolution.</p> required <code>disp_product</code> <code>DispProduct</code> <p>Original DISP product (for metadata).</p> required <code>output_dir</code> <code>Path or str</code> <p>Output directory for NetCDF file.</p> required <code>sensor</code> <code>str</code> <p>Sensor type: \"S1\" or \"NI\". Default is \"S1\".</p> <code>'S1'</code> <code>calibration_std</code> <code>DataArray or None</code> <p>Calibration uncertainty at full resolution. Default is None.</p> <code>None</code> <code>model_3d</code> <code>dict[str, DataArray] or None</code> <p>3D displacement components (coarse resolution) with keys: \"north_south\", \"east_west\", \"up_down\". Default is None.</p> <code>None</code> <code>model_3d_std</code> <code>dict[str, DataArray] or None</code> <p>3D displacement uncertainties (coarse resolution). Default is None.</p> <code>None</code> <code>metadata</code> <code>dict[str, str] or None</code> <p>Additional metadata (e.g., GNSS reference epoch). Default is None.</p> <code>None</code> <code>version</code> <code>str</code> <p>Product version. Default is \"1.0\".</p> <code>'1.0'</code> <p>Returns:</p> Type Description <code>CalProduct</code> <p>Created calibration product.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from product import DispProduct, CalProduct\n&gt;&gt;&gt;\n&gt;&gt;&gt; disp = DispProduct.from_path(\n\"OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Full resolution calibration\n&gt;&gt;&gt; cal_full = calibration_at_30m_resolution\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Coarse resolution 3D model (e.g., 90m from GNSS interpolation)\n&gt;&gt;&gt; model_coarse = {\n...     \"north_south\": disp_ns_90m,\n...     \"east_west\": disp_ew_90m,\n...     \"up_down\": disp_ud_90m,\n... }\n&gt;&gt;&gt;\n&gt;&gt;&gt; cal = CalProduct.create(\n...     calibration=cal_full,\n...     disp_product=disp,\n...     output_dir=\"output/\",\n...     calibration_std=cal_std,\n...     model_3d=model_coarse,\n...     model_3d_std={\n...         \"north_south_std\": disp_ns_std_90m,\n...         \"east_west_std\": disp_ew_std_90m,\n...         \"up_down_std\": disp_ud_std_90m,\n...     },\n...     metadata={\n...         \"gnss_reference_epoch\": \"2020-01-01T00:00:00Z\",\n...         \"model_3d_resolution\": \"90m\",\n...     },\n... )\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    calibration: xr.DataArray,\n    disp_product: \"DispProduct\",\n    output_dir: Path | str,\n    sensor: str = \"S1\",\n    calibration_std: xr.DataArray | None = None,\n    model_3d: dict[str, xr.DataArray] | None = None,\n    model_3d_std: dict[str, xr.DataArray] | None = None,\n    metadata: dict[str, str] | None = None,\n    version: str = \"1.0\",\n) -&gt; \"CalProduct\":\n    \"\"\"Create calibration product with optional model_3d group.\n\n    Parameters\n    ----------\n    calibration : xr.DataArray\n        Calibration correction at full DISP resolution.\n    disp_product : DispProduct\n        Original DISP product (for metadata).\n    output_dir : Path or str\n        Output directory for NetCDF file.\n    sensor : str, optional\n        Sensor type: \"S1\" or \"NI\". Default is \"S1\".\n    calibration_std : xr.DataArray or None, optional\n        Calibration uncertainty at full resolution. Default is None.\n    model_3d : dict[str, xr.DataArray] or None, optional\n        3D displacement components (coarse resolution) with keys:\n        \"north_south\", \"east_west\", \"up_down\". Default is None.\n    model_3d_std : dict[str, xr.DataArray] or None, optional\n        3D displacement uncertainties (coarse resolution). Default is None.\n    metadata : dict[str, str] or None, optional\n        Additional metadata (e.g., GNSS reference epoch). Default is None.\n    version : str, optional\n        Product version. Default is \"1.0\".\n\n    Returns\n    -------\n    CalProduct\n        Created calibration product.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from product import DispProduct, CalProduct\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; disp = DispProduct.from_path(\n    \"OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc\")\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Full resolution calibration\n    &gt;&gt;&gt; cal_full = calibration_at_30m_resolution\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Coarse resolution 3D model (e.g., 90m from GNSS interpolation)\n    &gt;&gt;&gt; model_coarse = {\n    ...     \"north_south\": disp_ns_90m,\n    ...     \"east_west\": disp_ew_90m,\n    ...     \"up_down\": disp_ud_90m,\n    ... }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; cal = CalProduct.create(\n    ...     calibration=cal_full,\n    ...     disp_product=disp,\n    ...     output_dir=\"output/\",\n    ...     calibration_std=cal_std,\n    ...     model_3d=model_coarse,\n    ...     model_3d_std={\n    ...         \"north_south_std\": disp_ns_std_90m,\n    ...         \"east_west_std\": disp_ew_std_90m,\n    ...         \"up_down_std\": disp_ud_std_90m,\n    ...     },\n    ...     metadata={\n    ...         \"gnss_reference_epoch\": \"2020-01-01T00:00:00Z\",\n    ...         \"model_3d_resolution\": \"90m\",\n    ...     },\n    ... )\n\n    \"\"\"\n    if sensor not in {\"S1\", \"NI\"}:\n        raise ValueError(f\"Invalid sensor: {sensor}. Must be 'S1' or 'NI'\")\n\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Generate OPERA-compliant filename\n    production_date = datetime.utcnow()\n    filename = (\n        f\"OPERA_L4_CAL-DISP-{sensor}_\"\n        f\"{disp_product.mode}_\"\n        f\"F{disp_product.frame_id:05d}_\"\n        f\"{disp_product.polarization}_\"\n        f\"{disp_product.primary_date:%Y%m%dT%H%M%S}Z_\"\n        f\"{disp_product.secondary_date:%Y%m%dT%H%M%S}Z_\"\n        f\"v{version}_\"\n        f\"{production_date:%Y%m%dT%H%M%S}Z\"\n        \".nc\"\n    )\n\n    output_path = output_dir / filename\n\n    # Build main group dataset (full resolution)\n    data_vars = {\"calibration\": calibration}\n\n    if calibration_std is not None:\n        data_vars[\"calibration_std\"] = calibration_std\n\n    # Create main dataset\n    ds = xr.Dataset(data_vars)\n\n    # Add global attributes\n    ds.attrs.update(\n        {\n            \"product_type\": f\"OPERA_L4_CAL-DISP-{sensor}\",\n            \"sensor\": sensor,\n            \"frame_id\": disp_product.frame_id,\n            \"mode\": disp_product.mode,\n            \"polarization\": disp_product.polarization,\n            \"primary_datetime\": disp_product.primary_date.isoformat(),\n            \"secondary_datetime\": disp_product.secondary_date.isoformat(),\n            \"production_datetime\": production_date.isoformat(),\n            \"product_version\": version,\n            \"description\": (\n                f\"Calibration correction for {sensor} InSAR displacement (subtract\"\n                \" from DISP)\"\n            ),\n            \"source_product\": disp_product.filename,\n            \"usage\": (\n                \"Subtract calibration layer from DISP displacement to obtain\"\n                \" calibrated displacement\"\n            ),\n        }\n    )\n\n    # Add custom metadata\n    if metadata:\n        ds.attrs.update(metadata)\n\n    # Save main group\n    ds.to_netcdf(output_path, engine=\"h5netcdf\")\n\n    # Create model_3d group if 3D components provided (coarse resolution)\n    if model_3d or model_3d_std:\n        model_data_vars = {}\n\n        # Add 3D displacement components\n        if model_3d:\n            for comp in [\"north_south\", \"east_west\", \"up_down\"]:\n                if comp in model_3d:\n                    model_data_vars[comp] = model_3d[comp]\n\n        # Add 3D displacement uncertainties\n        if model_3d_std:\n            for comp in [\"north_south_std\", \"east_west_std\", \"up_down_std\"]:\n                if comp in model_3d_std:\n                    model_data_vars[comp] = model_3d_std[comp]\n\n        if model_data_vars:\n            ds_model = xr.Dataset(model_data_vars)\n\n            # Add model group attributes\n            ds_model.attrs.update(\n                {\n                    \"description\": (\n                        \"3D displacement model at coarse resolution (e.g., from\"\n                        \" GNSS interpolation or deformation model)\"\n                    ),\n                    \"units\": \"meters\",\n                    \"reference_frame\": \"ENU (East-North-Up)\",\n                }\n            )\n\n            # Append to existing file as model_3d group\n            ds_model.to_netcdf(\n                output_path,\n                mode=\"a\",\n                group=\"model_3d\",\n                engine=\"h5netcdf\",\n            )\n\n    return cls(\n        path=output_path,\n        frame_id=disp_product.frame_id,\n        primary_date=disp_product.primary_date,\n        secondary_date=disp_product.secondary_date,\n        polarization=disp_product.polarization,\n        sensor=sensor,\n        version=version,\n        production_date=production_date,\n        mode=disp_product.mode,\n    )\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(path: Path | str) -&gt; CalProduct\n</code></pre> <p>Parse product metadata from filename.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path or str</code> <p>Path to calibration product NetCDF file.</p> required <p>Returns:</p> Type Description <code>CalProduct</code> <p>Parsed calibration product instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If filename doesn't match OPERA CAL-DISP pattern.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cal = CalProduct.from_path(\n\"OPERA_L4_CAL-DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251227T123456Z.nc\")\n&gt;&gt;&gt; cal.sensor\n'S1'\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>@classmethod\ndef from_path(cls, path: Path | str) -&gt; \"CalProduct\":\n    \"\"\"Parse product metadata from filename.\n\n    Parameters\n    ----------\n    path : Path or str\n        Path to calibration product NetCDF file.\n\n    Returns\n    -------\n    CalProduct\n        Parsed calibration product instance.\n\n    Raises\n    ------\n    ValueError\n        If filename doesn't match OPERA CAL-DISP pattern.\n\n    Examples\n    --------\n    &gt;&gt;&gt; cal = CalProduct.from_path(\n    \"OPERA_L4_CAL-DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251227T123456Z.nc\")\n    &gt;&gt;&gt; cal.sensor\n    'S1'\n\n    \"\"\"\n    path = Path(path)\n    match = cls._PATTERN.match(path.name)\n\n    if not match:\n        raise ValueError(\n            f\"Filename does not match OPERA CAL-DISP pattern: {path.name}\"\n        )\n\n    return cls(\n        path=path,\n        frame_id=int(match.group(\"frame_id\")),\n        primary_date=datetime.strptime(match.group(\"primary\"), \"%Y%m%dT%H%M%SZ\"),\n        secondary_date=datetime.strptime(\n            match.group(\"secondary\"), \"%Y%m%dT%H%M%SZ\"\n        ),\n        polarization=match.group(\"pol\"),\n        sensor=match.group(\"sensor\"),\n        version=match.group(\"version\"),\n        production_date=datetime.strptime(\n            match.group(\"production\"), \"%Y%m%dT%H%M%SZ\"\n        ),\n        mode=match.group(\"mode\"),\n    )\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.get_bounds","title":"get_bounds","text":"<pre><code>get_bounds() -&gt; dict[str, float]\n</code></pre> <p>Get bounds in native projection.</p> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def get_bounds(self) -&gt; dict[str, float]:\n    \"\"\"Get bounds in native projection.\"\"\"\n    ds = self.open_dataset()\n\n    x = ds.x.values\n    y = ds.y.values\n\n    return {\n        \"left\": float(x.min()),\n        \"bottom\": float(y.min()),\n        \"right\": float(x.max()),\n        \"top\": float(y.max()),\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.get_bounds_wgs84","title":"get_bounds_wgs84","text":"<pre><code>get_bounds_wgs84() -&gt; dict[str, float]\n</code></pre> <p>Get bounds transformed to WGS84.</p> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def get_bounds_wgs84(self) -&gt; dict[str, float]:\n    \"\"\"Get bounds transformed to WGS84.\"\"\"\n    ds = self.open_dataset()\n\n    x = ds.x.values\n    y = ds.y.values\n    left = float(x.min())\n    bottom = float(y.min())\n    right = float(x.max())\n    top = float(y.max())\n\n    if \"spatial_ref\" not in ds:\n        raise ValueError(\"Dataset missing spatial_ref\")\n\n    crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n    if not crs_wkt:\n        raise ValueError(\"spatial_ref missing crs_wkt\")\n\n    src_crs = CRS.from_wkt(crs_wkt)\n\n    west, south, east, north = transform_bounds(\n        src_crs,\n        CRS.from_epsg(4326),\n        left,\n        bottom,\n        right,\n        top,\n    )\n\n    return {\n        \"west\": west,\n        \"south\": south,\n        \"east\": east,\n        \"north\": north,\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.get_calibration_summary","title":"get_calibration_summary","text":"<pre><code>get_calibration_summary() -&gt; dict[str, dict[str, float]]\n</code></pre> <p>Get summary statistics of all layers.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, float]]</code> <p>Statistics for main and model_3d groups.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; summary = cal.get_calibration_summary()\n&gt;&gt;&gt; summary[\"main\"][\"calibration\"]\n{'mean': 0.023, 'std': 0.015, 'min': -0.05, 'max': 0.08}\n&gt;&gt;&gt; summary[\"model_3d\"][\"up_down\"]\n{'mean': 0.001, 'std': 0.003, 'min': -0.01, 'max': 0.02}\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def get_calibration_summary(self) -&gt; dict[str, dict[str, float]]:\n    \"\"\"Get summary statistics of all layers.\n\n    Returns\n    -------\n    dict[str, dict[str, float]]\n        Statistics for main and model_3d groups.\n\n    Examples\n    --------\n    &gt;&gt;&gt; summary = cal.get_calibration_summary()\n    &gt;&gt;&gt; summary[\"main\"][\"calibration\"]\n    {'mean': 0.023, 'std': 0.015, 'min': -0.05, 'max': 0.08}\n    &gt;&gt;&gt; summary[\"model_3d\"][\"up_down\"]\n    {'mean': 0.001, 'std': 0.003, 'min': -0.01, 'max': 0.02}\n\n    \"\"\"\n    summary: dict = {\"main\": {}}\n\n    # Main group\n    ds = self.open_dataset()\n    for var in ds.data_vars:\n        data = ds[var].values\n        valid_data = data[~np.isnan(data)]\n\n        if len(valid_data) &gt; 0:\n            summary[\"main\"][var] = {\n                \"mean\": float(np.mean(valid_data)),\n                \"std\": float(np.std(valid_data)),\n                \"min\": float(np.min(valid_data)),\n                \"max\": float(np.max(valid_data)),\n            }\n\n    # model_3d group if exists\n    if self.has_model_3d():\n        summary[\"model_3d\"] = {}\n        ds_model = self.open_model_3d()\n\n        for var in ds_model.data_vars:\n            data = ds_model[var].values\n            valid_data = data[~np.isnan(data)]\n\n            if len(valid_data) &gt; 0:\n                summary[\"model_3d\"][var] = {\n                    \"mean\": float(np.mean(valid_data)),\n                    \"std\": float(np.std(valid_data)),\n                    \"min\": float(np.min(valid_data)),\n                    \"max\": float(np.max(valid_data)),\n                }\n\n    return summary\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.get_epsg","title":"get_epsg","text":"<pre><code>get_epsg() -&gt; int | None\n</code></pre> <p>Get EPSG code from spatial reference.</p> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def get_epsg(self) -&gt; int | None:\n    \"\"\"Get EPSG code from spatial reference.\"\"\"\n    ds = self.open_dataset()\n\n    if \"spatial_ref\" in ds:\n        crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n        if crs_wkt:\n            crs = CRS.from_wkt(crs_wkt)\n            return crs.to_epsg()\n\n    return None\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.has_model_3d","title":"has_model_3d","text":"<pre><code>has_model_3d() -&gt; bool\n</code></pre> <p>Check if product contains model_3d group.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if model_3d group exists.</p> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def has_model_3d(self) -&gt; bool:\n    \"\"\"Check if product contains model_3d group.\n\n    Returns\n    -------\n    bool\n        True if model_3d group exists.\n\n    \"\"\"\n    try:\n        self.open_model_3d()\n        return True\n    except (FileNotFoundError, ValueError):\n        return False\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.open_dataset","title":"open_dataset","text":"<pre><code>open_dataset(group: str | None = None) -&gt; xr.Dataset\n</code></pre> <p>Open calibration dataset.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>str or None</code> <p>Group to open: None for main, \"model_3d\" for 3D model. Default is None (main group).</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing requested group.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If product file does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Open main calibration (full resolution)\n&gt;&gt;&gt; ds_main = cal.open_dataset()\n&gt;&gt;&gt; calibration = ds_main[\"calibration\"]\n</code></pre> <pre><code>&gt;&gt;&gt; # Open model_3d group (coarse resolution)\n&gt;&gt;&gt; ds_model = cal.open_dataset(group=\"model_3d\")\n&gt;&gt;&gt; model_up = ds_model[\"up_down\"]\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def open_dataset(self, group: str | None = None) -&gt; xr.Dataset:\n    \"\"\"Open calibration dataset.\n\n    Parameters\n    ----------\n    group : str or None, optional\n        Group to open: None for main, \"model_3d\" for 3D model.\n        Default is None (main group).\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing requested group.\n\n    Raises\n    ------\n    FileNotFoundError\n        If product file does not exist.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Open main calibration (full resolution)\n    &gt;&gt;&gt; ds_main = cal.open_dataset()\n    &gt;&gt;&gt; calibration = ds_main[\"calibration\"]\n\n    &gt;&gt;&gt; # Open model_3d group (coarse resolution)\n    &gt;&gt;&gt; ds_model = cal.open_dataset(group=\"model_3d\")\n    &gt;&gt;&gt; model_up = ds_model[\"up_down\"]\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n    if group == \"model_3d\":\n        return xr.open_dataset(self.path, group=\"model_3d\", engine=\"h5netcdf\")\n\n    return xr.open_dataset(self.path, engine=\"h5netcdf\")\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.open_model_3d","title":"open_model_3d","text":"<pre><code>open_model_3d() -&gt; xr.Dataset\n</code></pre> <p>Open model_3d group dataset.</p> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing 3D displacement model at coarse resolution.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If product file does not exist.</p> <code>ValueError</code> <p>If model_3d group does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds_model = cal.open_model_3d()\n&gt;&gt;&gt; disp_ns = ds_model[\"north_south\"]\n&gt;&gt;&gt; disp_ew = ds_model[\"east_west\"]\n&gt;&gt;&gt; disp_up = ds_model[\"up_down\"]\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def open_model_3d(self) -&gt; xr.Dataset:\n    \"\"\"Open model_3d group dataset.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing 3D displacement model at coarse resolution.\n\n    Raises\n    ------\n    FileNotFoundError\n        If product file does not exist.\n    ValueError\n        If model_3d group does not exist.\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds_model = cal.open_model_3d()\n    &gt;&gt;&gt; disp_ns = ds_model[\"north_south\"]\n    &gt;&gt;&gt; disp_ew = ds_model[\"east_west\"]\n    &gt;&gt;&gt; disp_up = ds_model[\"up_down\"]\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n    try:\n        return xr.open_dataset(self.path, group=\"model_3d\", engine=\"h5netcdf\")\n    except (OSError, ValueError) as e:\n        raise ValueError(\n            f\"model_3d group not found in {self.filename}. \"\n            \"Product may not contain 3D displacement model.\"\n        ) from e\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.to_geotiff","title":"to_geotiff","text":"<pre><code>to_geotiff(layer: str, output_path: Path | str, group: str | None = None, compress: str = 'DEFLATE', **kwargs) -&gt; Path\n</code></pre> <p>Export layer to GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>str</code> <p>Name of layer to export.</p> required <code>output_path</code> <code>Path or str</code> <p>Output GeoTIFF path.</p> required <code>group</code> <code>str or None</code> <p>Group containing layer. Default is None (main group).</p> <code>None</code> <code>compress</code> <code>str</code> <p>Compression method. Default is \"DEFLATE\".</p> <code>'DEFLATE'</code> <code>**kwargs</code> <p>Additional rasterio creation options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to created GeoTIFF.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Export main calibration\n&gt;&gt;&gt; cal.to_geotiff(\"calibration\", \"calibration.tif\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Export 3D model component\n&gt;&gt;&gt; cal.to_geotiff(\"up_down\", \"model_up.tif\", group=\"model_3d\")\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def to_geotiff(\n    self,\n    layer: str,\n    output_path: Path | str,\n    group: str | None = None,\n    compress: str = \"DEFLATE\",\n    **kwargs,\n) -&gt; Path:\n    \"\"\"Export layer to GeoTIFF.\n\n    Parameters\n    ----------\n    layer : str\n        Name of layer to export.\n    output_path : Path or str\n        Output GeoTIFF path.\n    group : str or None, optional\n        Group containing layer. Default is None (main group).\n    compress : str, optional\n        Compression method. Default is \"DEFLATE\".\n    **kwargs\n        Additional rasterio creation options.\n\n    Returns\n    -------\n    Path\n        Path to created GeoTIFF.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Export main calibration\n    &gt;&gt;&gt; cal.to_geotiff(\"calibration\", \"calibration.tif\")\n\n    &gt;&gt;&gt; # Export 3D model component\n    &gt;&gt;&gt; cal.to_geotiff(\"up_down\", \"model_up.tif\", group=\"model_3d\")\n\n    \"\"\"\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    ds = self.open_dataset(group=group)\n\n    if layer not in ds:\n        available = list(ds.data_vars)\n        group_str = f\" in {group} group\" if group else \"\"\n        raise ValueError(\n            f\"Layer '{layer}' not found{group_str}. Available: {available}\"\n        )\n\n    da = ds[layer]\n    data = da.values\n\n    # Extract spatial information\n    if \"spatial_ref\" in ds:\n        transform = self._get_transform(ds)\n        crs = self._get_crs(ds)\n    else:\n        transform = Affine.translation(\n            float(ds.x.values[0]),\n            float(ds.y.values[0]),\n        ) * Affine.scale(\n            float(ds.x.values[1] - ds.x.values[0]),\n            float(ds.y.values[1] - ds.y.values[0]),\n        )\n        crs = None\n\n    # Write GeoTIFF\n    profile = {\n        \"driver\": \"GTiff\",\n        \"height\": data.shape[0],\n        \"width\": data.shape[1],\n        \"count\": 1,\n        \"dtype\": np.float32,\n        \"transform\": transform,\n        \"compress\": compress,\n        \"tiled\": True,\n        \"blockxsize\": 512,\n        \"blockysize\": 512,\n        **kwargs,\n    }\n\n    if crs:\n        profile[\"crs\"] = crs\n\n    with rasterio.open(output_path, \"w\", **profile) as dst:\n        dst.write(data.astype(np.float32), 1)\n        dst.set_band_description(1, layer)\n\n        # Add OPERA metadata tags\n        dst.update_tags(\n            product_type=f\"OPERA_L4_CAL-DISP-{self.sensor}\",\n            sensor=self.sensor,\n            frame_id=self.frame_id,\n            polarization=self.polarization,\n            primary_date=self.primary_date.isoformat(),\n            secondary_date=self.secondary_date.isoformat(),\n            layer=layer,\n            group=group if group else \"main\",\n        )\n\n    return output_path\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct","title":"DispProduct  <code>dataclass</code>","text":"<p>OPERA DISP-S1 displacement product.</p> <p>Represents a Level-3 interferometric displacement product from the OPERA DISP-S1 archive. Products contain displacement measurements between two Sentinel-1 acquisition dates.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the NetCDF product file.</p> required <code>frame_id</code> <code>int</code> <p>OPERA frame identifier (e.g., 8882).</p> required <code>primary_date</code> <code>datetime</code> <p>Earlier acquisition date (reference).</p> required <code>secondary_date</code> <code>datetime</code> <p>Later acquisition date.</p> required <code>polarization</code> <code>str</code> <p>Radar polarization (e.g., \"VV\", \"VH\").</p> required <code>version</code> <code>str</code> <p>Product version string (e.g., \"1.0\").</p> required <code>production_date</code> <code>datetime</code> <p>Date when product was generated.</p> required <code>mode</code> <code>str</code> <p>Acquisition mode (e.g., \"IW\"). Default is \"IW\".</p> <code>'IW'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; path = Path(\n\"OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc\")\n&gt;&gt;&gt; product = DispProduct.from_path(path)\n&gt;&gt;&gt; product.frame_id\n8882\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct--get-reference-point","title":"Get reference point","text":"<pre><code>&gt;&gt;&gt; row, col = product.get_reference_point_index()\n&gt;&gt;&gt; lat, lon = product.get_reference_point_latlon()\n&gt;&gt;&gt; print(f\"Reference at ({row}, {col}): {lat:.4f}\u00b0N, {lon:.4f}\u00b0E\")\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>@dataclass\nclass DispProduct:\n    \"\"\"OPERA DISP-S1 displacement product.\n\n    Represents a Level-3 interferometric displacement product from\n    the OPERA DISP-S1 archive. Products contain displacement measurements\n    between two Sentinel-1 acquisition dates.\n\n    Parameters\n    ----------\n    path : Path\n        Path to the NetCDF product file.\n    frame_id : int\n        OPERA frame identifier (e.g., 8882).\n    primary_date : datetime\n        Earlier acquisition date (reference).\n    secondary_date : datetime\n        Later acquisition date.\n    polarization : str\n        Radar polarization (e.g., \"VV\", \"VH\").\n    version : str\n        Product version string (e.g., \"1.0\").\n    production_date : datetime\n        Date when product was generated.\n    mode : str, optional\n        Acquisition mode (e.g., \"IW\"). Default is \"IW\".\n\n    Examples\n    --------\n    &gt;&gt;&gt; path = Path(\n    \"OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc\")\n    &gt;&gt;&gt; product = DispProduct.from_path(path)\n    &gt;&gt;&gt; product.frame_id\n    8882\n\n    # Get reference point\n    &gt;&gt;&gt; row, col = product.get_reference_point_index()\n    &gt;&gt;&gt; lat, lon = product.get_reference_point_latlon()\n    &gt;&gt;&gt; print(f\"Reference at ({row}, {col}): {lat:.4f}\u00b0N, {lon:.4f}\u00b0E\")\n\n    \"\"\"\n\n    path: Path\n    frame_id: int\n    primary_date: datetime\n    secondary_date: datetime\n    polarization: str\n    version: str\n    production_date: datetime\n    mode: str = \"IW\"\n\n    # Filename pattern for OPERA DISP-S1 products\n    _PATTERN = re.compile(\n        r\"OPERA_L3_DISP-S1_\"\n        r\"(?P&lt;mode&gt;\\w+)_\"\n        r\"F(?P&lt;frame_id&gt;\\d+)_\"\n        r\"(?P&lt;pol&gt;\\w+)_\"\n        r\"(?P&lt;primary&gt;\\d{8}T\\d{6}Z)_\"\n        r\"(?P&lt;secondary&gt;\\d{8}T\\d{6}Z)_\"\n        r\"v(?P&lt;version&gt;[\\d.]+)_\"\n        r\"(?P&lt;production&gt;\\d{8}T\\d{6}Z)\"\n        r\"\\.nc$\"\n    )\n\n    # Main dataset layers\n    DISPLACEMENT_LAYERS = [\n        \"displacement\",\n        \"short_wavelength_displacement\",\n        \"recommended_mask\",\n        \"connected_component_labels\",\n        \"temporal_coherence\",\n        \"estimated_phase_quality\",\n        \"persistent_scatterer_mask\",\n        \"shp_counts\",\n        \"water_mask\",\n        \"phase_similarity\",\n        \"timeseries_inversion_residuals\",\n    ]\n\n    # Corrections group layers\n    CORRECTION_LAYERS = [\n        \"ionospheric_delay\",\n        \"solid_earth_tide\",\n        \"perpendicular_baseline\",\n    ]\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate product after construction.\"\"\"\n        self.path = Path(self.path)\n\n        if self.frame_id &lt;= 0:\n            raise ValueError(f\"frame_id must be positive, got {self.frame_id}\")\n\n        if self.secondary_date &lt;= self.primary_date:\n            raise ValueError(\n                f\"Secondary date ({self.secondary_date}) must be after \"\n                f\"primary date ({self.primary_date})\"\n            )\n\n        if self.polarization not in {\"VV\", \"VH\", \"HH\", \"HV\"}:\n            raise ValueError(f\"Invalid polarization: {self.polarization}\")\n\n    @classmethod\n    def from_path(cls, path: Path | str) -&gt; \"DispProduct\":\n        \"\"\"Parse product metadata from filename.\n\n        Parameters\n        ----------\n        path : Path or str\n            Path to OPERA DISP-S1 NetCDF file.\n\n        Returns\n        -------\n        DispProduct\n            Parsed product instance.\n\n        Raises\n        ------\n        ValueError\n            If filename doesn't match expected OPERA DISP-S1 format.\n\n        \"\"\"\n        path = Path(path)\n        match = cls._PATTERN.match(path.name)\n\n        if not match:\n            raise ValueError(\n                f\"Filename does not match OPERA DISP-S1 pattern: {path.name}\"\n            )\n\n        return cls(\n            path=path,\n            frame_id=int(match.group(\"frame_id\")),\n            primary_date=datetime.strptime(match.group(\"primary\"), \"%Y%m%dT%H%M%SZ\"),\n            secondary_date=datetime.strptime(\n                match.group(\"secondary\"), \"%Y%m%dT%H%M%SZ\"\n            ),\n            polarization=match.group(\"pol\"),\n            version=match.group(\"version\"),\n            production_date=datetime.strptime(\n                match.group(\"production\"), \"%Y%m%dT%H%M%SZ\"\n            ),\n            mode=match.group(\"mode\"),\n        )\n\n    def open_dataset(\n        self, group: Literal[\"main\", \"corrections\"] | None = None\n    ) -&gt; xr.Dataset:\n        \"\"\"Open dataset.\n\n        Parameters\n        ----------\n        group : {\"main\", \"corrections\"} or None, optional\n            Which group to open. If None, opens main group. Default is None.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing displacement and quality layers.\n\n        Raises\n        ------\n        FileNotFoundError\n            If product file does not exist.\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n        if group == \"corrections\":\n            return xr.open_dataset(self.path, group=\"corrections\", engine=\"h5netcdf\")\n\n        return xr.open_dataset(self.path, engine=\"h5netcdf\")\n\n    def open_corrections(self) -&gt; xr.Dataset:\n        \"\"\"Open corrections group dataset.\n\n        Returns\n        -------\n        xr.Dataset\n            Corrections dataset containing ionospheric delay, solid earth tide, etc.\n\n        Raises\n        ------\n        FileNotFoundError\n            If product file does not exist.\n\n        \"\"\"\n        return self.open_dataset(group=\"corrections\")\n\n    def get_epsg(self) -&gt; int | None:\n        \"\"\"Get EPSG code from spatial reference.\n\n        Returns\n        -------\n        int or None\n            EPSG code if found, None otherwise.\n\n        Examples\n        --------\n        &gt;&gt;&gt; product.get_epsg()\n        32615\n\n        \"\"\"\n        ds = self.open_dataset()\n        crs = self._get_crs(ds)\n\n        # Parse EPSG from CRS\n        raster_crs = CRS.from_wkt(crs)\n        return raster_crs.to_epsg()\n\n    def get_bounds(self) -&gt; dict[str, float]:\n        \"\"\"Get bounds in native projection coordinates.\n\n        Returns\n        -------\n        dict[str, float]\n            Dictionary with keys: left, bottom, right, top.\n\n        Examples\n        --------\n        &gt;&gt;&gt; bounds = product.get_bounds()\n        &gt;&gt;&gt; bounds\n        {'left': 71970.0, 'bottom': 3153930.0, 'right': 355890.0, 'top': 3385920.0}\n\n        \"\"\"\n        ds = self.open_dataset()\n\n        x = ds.x.values\n        y = ds.y.values\n\n        return {\n            \"left\": float(x.min()),\n            \"bottom\": float(y.min()),\n            \"right\": float(x.max()),\n            \"top\": float(y.max()),\n        }\n\n    def get_bounds_wgs84(self) -&gt; dict[str, float]:\n        \"\"\"Get bounds transformed to WGS84 (EPSG:4326).\n\n        Returns\n        -------\n        dict[str, float]\n            Dictionary with keys: west, south, east, north in decimal degrees.\n\n        Examples\n        --------\n        &gt;&gt;&gt; bounds = product.get_bounds_wgs84()\n        &gt;&gt;&gt; bounds\n        {'west': -95.567, 'south': 28.486, 'east': -93.212, 'north': 30.845}\n\n        \"\"\"\n        ds = self.open_dataset()\n\n        # Get native bounds\n        x = ds.x.values\n        y = ds.y.values\n        left = float(x.min())\n        bottom = float(y.min())\n        right = float(x.max())\n        top = float(y.max())\n\n        # Get native CRS\n        crs_wkt = self._get_crs(ds)\n        src_crs = CRS.from_wkt(crs_wkt)\n\n        # Transform to WGS84\n        west, south, east, north = transform_bounds(\n            src_crs,\n            CRS.from_epsg(4326),\n            left,\n            bottom,\n            right,\n            top,\n        )\n\n        return {\n            \"west\": west,\n            \"south\": south,\n            \"east\": east,\n            \"north\": north,\n        }\n\n    def get_reference_point_index(self) -&gt; tuple[int, int]:\n        \"\"\"Get reference point pixel indices.\n\n        The reference point is where the phase was set to zero during\n        processing. This is stored in the corrections group.\n\n        Returns\n        -------\n        tuple[int, int]\n            Row and column indices (row, col) of reference point.\n\n        Raises\n        ------\n        ValueError\n            If reference_point variable not found or missing attributes.\n\n        Examples\n        --------\n        &gt;&gt;&gt; row, col = product.get_reference_point_index()\n        &gt;&gt;&gt; print(f\"Reference point at pixel ({row}, {col})\")\n\n        \"\"\"\n        ds = self.open_corrections()\n\n        if \"reference_point\" not in ds:\n            raise ValueError(\"reference_point variable not found in corrections group\")\n\n        ref_attrs = ds.reference_point.attrs\n\n        if \"rows\" not in ref_attrs or \"cols\" not in ref_attrs:\n            raise ValueError(\"reference_point missing 'rows' or 'cols' attributes\")\n\n        row = int(ref_attrs[\"rows\"])\n        col = int(ref_attrs[\"cols\"])\n\n        return (row, col)\n\n    def get_reference_point_latlon(self) -&gt; tuple[float, float]:\n        \"\"\"Get reference point geographic coordinates.\n\n        Returns latitude and longitude of the reference point in WGS84.\n\n        Returns\n        -------\n        tuple[float, float]\n            Latitude and longitude in decimal degrees (lat, lon).\n\n        Raises\n        ------\n        ValueError\n            If reference_point variable not found or missing attributes.\n\n        Examples\n        --------\n        &gt;&gt;&gt; lat, lon = product.get_reference_point_latlon()\n        &gt;&gt;&gt; print(f\"Reference point: {lat:.6f}\u00b0N, {lon:.6f}\u00b0E\")\n\n        \"\"\"\n        ds = self.open_corrections()\n\n        if \"reference_point\" not in ds:\n            raise ValueError(\"reference_point variable not found in corrections group\")\n\n        ref_attrs = ds.reference_point.attrs\n\n        if \"latitudes\" not in ref_attrs or \"longitudes\" not in ref_attrs:\n            raise ValueError(\n                \"reference_point missing 'latitudes' or 'longitudes' attributes\"\n            )\n\n        lat = float(ref_attrs[\"latitudes\"])\n        lon = float(ref_attrs[\"longitudes\"])\n\n        return (lat, lon)\n\n    def to_geotiff(\n        self,\n        layer: str,\n        output_path: Path | str,\n        group: Literal[\"main\", \"corrections\"] = \"main\",\n        compress: str = \"DEFLATE\",\n        **kwargs,\n    ) -&gt; Path:\n        \"\"\"Export layer to optimized GeoTIFF.\n\n        Parameters\n        ----------\n        layer : str\n            Name of layer to export (e.g., \"displacement\", \"ionospheric_delay\").\n        output_path : Path or str\n            Output GeoTIFF path.\n        group : {\"main\", \"corrections\"}, optional\n            Which group to read from. Default is \"main\".\n        compress : str, optional\n            Compression method. Default is \"DEFLATE\".\n        **kwargs\n            Additional rasterio creation options.\n\n        Returns\n        -------\n        Path\n            Path to created GeoTIFF.\n\n        Raises\n        ------\n        ValueError\n            If layer not found in specified group.\n\n        Examples\n        --------\n        &gt;&gt;&gt; product.to_geotiff(\"displacement\", \"disp.tif\")\n        &gt;&gt;&gt; product.to_geotiff(\"ionospheric_delay\", \"iono.tif\", group=\"corrections\")\n\n        \"\"\"\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Open appropriate dataset\n        ds = self.open_dataset(group=group if group == \"corrections\" else None)\n\n        if layer not in ds:\n            available = list(ds.data_vars)\n            raise ValueError(\n                f\"Layer '{layer}' not found in {group} group. \"\n                f\"Available layers: {available}\"\n            )\n\n        # Get data array\n        da = ds[layer]\n\n        # Extract transform from spatial_ref\n        transform = self._get_transform(ds)\n\n        # Get CRS\n        crs = self._get_crs(ds)\n\n        # Prepare data - handle (y, x) or (time, y, x) shapes\n        if da.ndim == 3:\n            # Take first time slice if 3D\n            data = da.isel(time=0).values\n        else:\n            data = da.values\n\n        # Write GeoTIFF\n        profile = {\n            \"driver\": \"GTiff\",\n            \"height\": data.shape[0],\n            \"width\": data.shape[1],\n            \"count\": 1,\n            \"dtype\": data.dtype,\n            \"crs\": crs,\n            \"transform\": transform,\n            \"compress\": compress,\n            \"tiled\": True,\n            \"blockxsize\": 512,\n            \"blockysize\": 512,\n            **kwargs,\n        }\n\n        with rasterio.open(output_path, \"w\", **profile) as dst:\n            dst.write(data, 1)\n            dst.set_band_description(1, layer)\n\n        return output_path\n\n    def _get_transform(self, ds: xr.Dataset) -&gt; Affine:\n        \"\"\"Extract affine transform from dataset.\"\"\"\n        gt = ds.spatial_ref.attrs.get(\"GeoTransform\")\n        if gt is None:\n            raise ValueError(\"No GeoTransform found in spatial_ref\")\n\n        # Parse string like \"71970.0 30.0 0.0 3385920.0 0.0 -30.0\"\n        vals = [float(x) for x in gt.split()]\n        return Affine(vals[1], vals[2], vals[0], vals[4], vals[5], vals[3])\n\n    def _get_crs(self, ds: xr.Dataset) -&gt; str:\n        \"\"\"Extract CRS from dataset.\"\"\"\n        crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n        if crs_wkt is None:\n            raise ValueError(\"No crs_wkt found in spatial_ref\")\n        return crs_wkt\n\n    @property\n    def baseline_days(self) -&gt; int:\n        \"\"\"Temporal baseline in days between acquisitions.\"\"\"\n        return (self.secondary_date - self.primary_date).days\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Product filename.\"\"\"\n        return self.path.name\n\n    @property\n    def exists(self) -&gt; bool:\n        \"\"\"Check if product file exists on disk.\"\"\"\n        return self.path.exists()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Concise string representation.\"\"\"\n        return (\n            f\"DispProduct(frame={self.frame_id}, \"\n            f\"{self.primary_date.date()} \u2192 {self.secondary_date.date()}, \"\n            f\"{self.polarization})\"\n        )\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.baseline_days","title":"baseline_days  <code>property</code>","text":"<pre><code>baseline_days: int\n</code></pre> <p>Temporal baseline in days between acquisitions.</p>"},{"location":"api/#cal_disp.product.DispProduct.exists","title":"exists  <code>property</code>","text":"<pre><code>exists: bool\n</code></pre> <p>Check if product file exists on disk.</p>"},{"location":"api/#cal_disp.product.DispProduct.filename","title":"filename  <code>property</code>","text":"<pre><code>filename: str\n</code></pre> <p>Product filename.</p>"},{"location":"api/#cal_disp.product.DispProduct.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(path: Path | str) -&gt; DispProduct\n</code></pre> <p>Parse product metadata from filename.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path or str</code> <p>Path to OPERA DISP-S1 NetCDF file.</p> required <p>Returns:</p> Type Description <code>DispProduct</code> <p>Parsed product instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If filename doesn't match expected OPERA DISP-S1 format.</p> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>@classmethod\ndef from_path(cls, path: Path | str) -&gt; \"DispProduct\":\n    \"\"\"Parse product metadata from filename.\n\n    Parameters\n    ----------\n    path : Path or str\n        Path to OPERA DISP-S1 NetCDF file.\n\n    Returns\n    -------\n    DispProduct\n        Parsed product instance.\n\n    Raises\n    ------\n    ValueError\n        If filename doesn't match expected OPERA DISP-S1 format.\n\n    \"\"\"\n    path = Path(path)\n    match = cls._PATTERN.match(path.name)\n\n    if not match:\n        raise ValueError(\n            f\"Filename does not match OPERA DISP-S1 pattern: {path.name}\"\n        )\n\n    return cls(\n        path=path,\n        frame_id=int(match.group(\"frame_id\")),\n        primary_date=datetime.strptime(match.group(\"primary\"), \"%Y%m%dT%H%M%SZ\"),\n        secondary_date=datetime.strptime(\n            match.group(\"secondary\"), \"%Y%m%dT%H%M%SZ\"\n        ),\n        polarization=match.group(\"pol\"),\n        version=match.group(\"version\"),\n        production_date=datetime.strptime(\n            match.group(\"production\"), \"%Y%m%dT%H%M%SZ\"\n        ),\n        mode=match.group(\"mode\"),\n    )\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.get_bounds","title":"get_bounds","text":"<pre><code>get_bounds() -&gt; dict[str, float]\n</code></pre> <p>Get bounds in native projection coordinates.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with keys: left, bottom, right, top.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bounds = product.get_bounds()\n&gt;&gt;&gt; bounds\n{'left': 71970.0, 'bottom': 3153930.0, 'right': 355890.0, 'top': 3385920.0}\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def get_bounds(self) -&gt; dict[str, float]:\n    \"\"\"Get bounds in native projection coordinates.\n\n    Returns\n    -------\n    dict[str, float]\n        Dictionary with keys: left, bottom, right, top.\n\n    Examples\n    --------\n    &gt;&gt;&gt; bounds = product.get_bounds()\n    &gt;&gt;&gt; bounds\n    {'left': 71970.0, 'bottom': 3153930.0, 'right': 355890.0, 'top': 3385920.0}\n\n    \"\"\"\n    ds = self.open_dataset()\n\n    x = ds.x.values\n    y = ds.y.values\n\n    return {\n        \"left\": float(x.min()),\n        \"bottom\": float(y.min()),\n        \"right\": float(x.max()),\n        \"top\": float(y.max()),\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.get_bounds_wgs84","title":"get_bounds_wgs84","text":"<pre><code>get_bounds_wgs84() -&gt; dict[str, float]\n</code></pre> <p>Get bounds transformed to WGS84 (EPSG:4326).</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with keys: west, south, east, north in decimal degrees.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bounds = product.get_bounds_wgs84()\n&gt;&gt;&gt; bounds\n{'west': -95.567, 'south': 28.486, 'east': -93.212, 'north': 30.845}\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def get_bounds_wgs84(self) -&gt; dict[str, float]:\n    \"\"\"Get bounds transformed to WGS84 (EPSG:4326).\n\n    Returns\n    -------\n    dict[str, float]\n        Dictionary with keys: west, south, east, north in decimal degrees.\n\n    Examples\n    --------\n    &gt;&gt;&gt; bounds = product.get_bounds_wgs84()\n    &gt;&gt;&gt; bounds\n    {'west': -95.567, 'south': 28.486, 'east': -93.212, 'north': 30.845}\n\n    \"\"\"\n    ds = self.open_dataset()\n\n    # Get native bounds\n    x = ds.x.values\n    y = ds.y.values\n    left = float(x.min())\n    bottom = float(y.min())\n    right = float(x.max())\n    top = float(y.max())\n\n    # Get native CRS\n    crs_wkt = self._get_crs(ds)\n    src_crs = CRS.from_wkt(crs_wkt)\n\n    # Transform to WGS84\n    west, south, east, north = transform_bounds(\n        src_crs,\n        CRS.from_epsg(4326),\n        left,\n        bottom,\n        right,\n        top,\n    )\n\n    return {\n        \"west\": west,\n        \"south\": south,\n        \"east\": east,\n        \"north\": north,\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.get_epsg","title":"get_epsg","text":"<pre><code>get_epsg() -&gt; int | None\n</code></pre> <p>Get EPSG code from spatial reference.</p> <p>Returns:</p> Type Description <code>int or None</code> <p>EPSG code if found, None otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; product.get_epsg()\n32615\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def get_epsg(self) -&gt; int | None:\n    \"\"\"Get EPSG code from spatial reference.\n\n    Returns\n    -------\n    int or None\n        EPSG code if found, None otherwise.\n\n    Examples\n    --------\n    &gt;&gt;&gt; product.get_epsg()\n    32615\n\n    \"\"\"\n    ds = self.open_dataset()\n    crs = self._get_crs(ds)\n\n    # Parse EPSG from CRS\n    raster_crs = CRS.from_wkt(crs)\n    return raster_crs.to_epsg()\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.get_reference_point_index","title":"get_reference_point_index","text":"<pre><code>get_reference_point_index() -&gt; tuple[int, int]\n</code></pre> <p>Get reference point pixel indices.</p> <p>The reference point is where the phase was set to zero during processing. This is stored in the corrections group.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Row and column indices (row, col) of reference point.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If reference_point variable not found or missing attributes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; row, col = product.get_reference_point_index()\n&gt;&gt;&gt; print(f\"Reference point at pixel ({row}, {col})\")\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def get_reference_point_index(self) -&gt; tuple[int, int]:\n    \"\"\"Get reference point pixel indices.\n\n    The reference point is where the phase was set to zero during\n    processing. This is stored in the corrections group.\n\n    Returns\n    -------\n    tuple[int, int]\n        Row and column indices (row, col) of reference point.\n\n    Raises\n    ------\n    ValueError\n        If reference_point variable not found or missing attributes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; row, col = product.get_reference_point_index()\n    &gt;&gt;&gt; print(f\"Reference point at pixel ({row}, {col})\")\n\n    \"\"\"\n    ds = self.open_corrections()\n\n    if \"reference_point\" not in ds:\n        raise ValueError(\"reference_point variable not found in corrections group\")\n\n    ref_attrs = ds.reference_point.attrs\n\n    if \"rows\" not in ref_attrs or \"cols\" not in ref_attrs:\n        raise ValueError(\"reference_point missing 'rows' or 'cols' attributes\")\n\n    row = int(ref_attrs[\"rows\"])\n    col = int(ref_attrs[\"cols\"])\n\n    return (row, col)\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.get_reference_point_latlon","title":"get_reference_point_latlon","text":"<pre><code>get_reference_point_latlon() -&gt; tuple[float, float]\n</code></pre> <p>Get reference point geographic coordinates.</p> <p>Returns latitude and longitude of the reference point in WGS84.</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>Latitude and longitude in decimal degrees (lat, lon).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If reference_point variable not found or missing attributes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lat, lon = product.get_reference_point_latlon()\n&gt;&gt;&gt; print(f\"Reference point: {lat:.6f}\u00b0N, {lon:.6f}\u00b0E\")\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def get_reference_point_latlon(self) -&gt; tuple[float, float]:\n    \"\"\"Get reference point geographic coordinates.\n\n    Returns latitude and longitude of the reference point in WGS84.\n\n    Returns\n    -------\n    tuple[float, float]\n        Latitude and longitude in decimal degrees (lat, lon).\n\n    Raises\n    ------\n    ValueError\n        If reference_point variable not found or missing attributes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; lat, lon = product.get_reference_point_latlon()\n    &gt;&gt;&gt; print(f\"Reference point: {lat:.6f}\u00b0N, {lon:.6f}\u00b0E\")\n\n    \"\"\"\n    ds = self.open_corrections()\n\n    if \"reference_point\" not in ds:\n        raise ValueError(\"reference_point variable not found in corrections group\")\n\n    ref_attrs = ds.reference_point.attrs\n\n    if \"latitudes\" not in ref_attrs or \"longitudes\" not in ref_attrs:\n        raise ValueError(\n            \"reference_point missing 'latitudes' or 'longitudes' attributes\"\n        )\n\n    lat = float(ref_attrs[\"latitudes\"])\n    lon = float(ref_attrs[\"longitudes\"])\n\n    return (lat, lon)\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.open_corrections","title":"open_corrections","text":"<pre><code>open_corrections() -&gt; xr.Dataset\n</code></pre> <p>Open corrections group dataset.</p> <p>Returns:</p> Type Description <code>Dataset</code> <p>Corrections dataset containing ionospheric delay, solid earth tide, etc.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If product file does not exist.</p> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def open_corrections(self) -&gt; xr.Dataset:\n    \"\"\"Open corrections group dataset.\n\n    Returns\n    -------\n    xr.Dataset\n        Corrections dataset containing ionospheric delay, solid earth tide, etc.\n\n    Raises\n    ------\n    FileNotFoundError\n        If product file does not exist.\n\n    \"\"\"\n    return self.open_dataset(group=\"corrections\")\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.open_dataset","title":"open_dataset","text":"<pre><code>open_dataset(group: Literal['main', 'corrections'] | None = None) -&gt; xr.Dataset\n</code></pre> <p>Open dataset.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>(main, corrections)</code> <p>Which group to open. If None, opens main group. Default is None.</p> <code>\"main\"</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing displacement and quality layers.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If product file does not exist.</p> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def open_dataset(\n    self, group: Literal[\"main\", \"corrections\"] | None = None\n) -&gt; xr.Dataset:\n    \"\"\"Open dataset.\n\n    Parameters\n    ----------\n    group : {\"main\", \"corrections\"} or None, optional\n        Which group to open. If None, opens main group. Default is None.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing displacement and quality layers.\n\n    Raises\n    ------\n    FileNotFoundError\n        If product file does not exist.\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n    if group == \"corrections\":\n        return xr.open_dataset(self.path, group=\"corrections\", engine=\"h5netcdf\")\n\n    return xr.open_dataset(self.path, engine=\"h5netcdf\")\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.to_geotiff","title":"to_geotiff","text":"<pre><code>to_geotiff(layer: str, output_path: Path | str, group: Literal['main', 'corrections'] = 'main', compress: str = 'DEFLATE', **kwargs) -&gt; Path\n</code></pre> <p>Export layer to optimized GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>str</code> <p>Name of layer to export (e.g., \"displacement\", \"ionospheric_delay\").</p> required <code>output_path</code> <code>Path or str</code> <p>Output GeoTIFF path.</p> required <code>group</code> <code>(main, corrections)</code> <p>Which group to read from. Default is \"main\".</p> <code>\"main\"</code> <code>compress</code> <code>str</code> <p>Compression method. Default is \"DEFLATE\".</p> <code>'DEFLATE'</code> <code>**kwargs</code> <p>Additional rasterio creation options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to created GeoTIFF.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If layer not found in specified group.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; product.to_geotiff(\"displacement\", \"disp.tif\")\n&gt;&gt;&gt; product.to_geotiff(\"ionospheric_delay\", \"iono.tif\", group=\"corrections\")\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def to_geotiff(\n    self,\n    layer: str,\n    output_path: Path | str,\n    group: Literal[\"main\", \"corrections\"] = \"main\",\n    compress: str = \"DEFLATE\",\n    **kwargs,\n) -&gt; Path:\n    \"\"\"Export layer to optimized GeoTIFF.\n\n    Parameters\n    ----------\n    layer : str\n        Name of layer to export (e.g., \"displacement\", \"ionospheric_delay\").\n    output_path : Path or str\n        Output GeoTIFF path.\n    group : {\"main\", \"corrections\"}, optional\n        Which group to read from. Default is \"main\".\n    compress : str, optional\n        Compression method. Default is \"DEFLATE\".\n    **kwargs\n        Additional rasterio creation options.\n\n    Returns\n    -------\n    Path\n        Path to created GeoTIFF.\n\n    Raises\n    ------\n    ValueError\n        If layer not found in specified group.\n\n    Examples\n    --------\n    &gt;&gt;&gt; product.to_geotiff(\"displacement\", \"disp.tif\")\n    &gt;&gt;&gt; product.to_geotiff(\"ionospheric_delay\", \"iono.tif\", group=\"corrections\")\n\n    \"\"\"\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Open appropriate dataset\n    ds = self.open_dataset(group=group if group == \"corrections\" else None)\n\n    if layer not in ds:\n        available = list(ds.data_vars)\n        raise ValueError(\n            f\"Layer '{layer}' not found in {group} group. \"\n            f\"Available layers: {available}\"\n        )\n\n    # Get data array\n    da = ds[layer]\n\n    # Extract transform from spatial_ref\n    transform = self._get_transform(ds)\n\n    # Get CRS\n    crs = self._get_crs(ds)\n\n    # Prepare data - handle (y, x) or (time, y, x) shapes\n    if da.ndim == 3:\n        # Take first time slice if 3D\n        data = da.isel(time=0).values\n    else:\n        data = da.values\n\n    # Write GeoTIFF\n    profile = {\n        \"driver\": \"GTiff\",\n        \"height\": data.shape[0],\n        \"width\": data.shape[1],\n        \"count\": 1,\n        \"dtype\": data.dtype,\n        \"crs\": crs,\n        \"transform\": transform,\n        \"compress\": compress,\n        \"tiled\": True,\n        \"blockxsize\": 512,\n        \"blockysize\": 512,\n        **kwargs,\n    }\n\n    with rasterio.open(output_path, \"w\", **profile) as dst:\n        dst.write(data, 1)\n        dst.set_band_description(1, layer)\n\n    return output_path\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer","title":"StaticLayer  <code>dataclass</code>","text":"<p>OPERA DISP-S1-STATIC layer.</p> <p>Represents a single static layer (DEM, incidence angle, LOS vectors, etc.) used as input for DISP-S1 processing. These are frame-specific GeoTIFF files that don't change over time.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; path = Path(\"OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_dem.tif\")\n&gt;&gt;&gt; layer = StaticLayer.from_path(path)\n&gt;&gt;&gt; layer.frame_id\n8882\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer--read-los-components","title":"Read LOS components","text":"<pre><code>&gt;&gt;&gt; los_layer = StaticLayer.from_path(\"..._line_of_sight_enu.tif\")\n&gt;&gt;&gt; bands = los_layer.read_bands()\n&gt;&gt;&gt; east, north, up = bands[0], bands[1], bands[2]\n</code></pre> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>@dataclass\nclass StaticLayer:\n    \"\"\"OPERA DISP-S1-STATIC layer.\n\n    Represents a single static layer (DEM, incidence angle, LOS vectors, etc.)\n    used as input for DISP-S1 processing. These are frame-specific GeoTIFF\n    files that don't change over time.\n\n    Examples\n    --------\n    &gt;&gt;&gt; path = Path(\"OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_dem.tif\")\n    &gt;&gt;&gt; layer = StaticLayer.from_path(path)\n    &gt;&gt;&gt; layer.frame_id\n    8882\n\n    # Read LOS components\n    &gt;&gt;&gt; los_layer = StaticLayer.from_path(\"..._line_of_sight_enu.tif\")\n    &gt;&gt;&gt; bands = los_layer.read_bands()\n    &gt;&gt;&gt; east, north, up = bands[0], bands[1], bands[2]\n\n    \"\"\"\n\n    path: Path\n    frame_id: int\n    reference_date: datetime\n    satellite: str\n    version: str\n    layer_type: str\n\n    _PATTERN = re.compile(\n        r\"OPERA_L3_DISP-S1-STATIC_\"\n        r\"F(?P&lt;frame_id&gt;\\d+)_\"\n        r\"(?P&lt;date&gt;\\d{8})_\"\n        r\"(?P&lt;satellite&gt;S1[AB])_\"\n        r\"v(?P&lt;version&gt;[\\d.]+)_\"\n        r\"(?P&lt;layer&gt;[\\w_]+)\"\n        r\"\\.tif$\"\n    )\n\n    LAYER_TYPES = [\n        \"dem\",\n        \"line_of_sight_enu\",\n        \"layover_shadow_mask\",\n    ]\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate layer after construction.\"\"\"\n        self.path = Path(self.path)\n\n        if self.frame_id &lt;= 0:\n            raise ValueError(f\"frame_id must be positive, got {self.frame_id}\")\n\n    @classmethod\n    def from_path(cls, path: Path | str) -&gt; \"StaticLayer\":\n        \"\"\"Parse layer metadata from filename.\"\"\"\n        path = Path(path)\n        match = cls._PATTERN.match(path.name)\n\n        if not match:\n            raise ValueError(\n                f\"Filename does not match OPERA DISP-S1-STATIC pattern: {path.name}\"\n            )\n\n        return cls(\n            path=path,\n            frame_id=int(match.group(\"frame_id\")),\n            reference_date=datetime.strptime(match.group(\"date\"), \"%Y%m%d\"),\n            satellite=match.group(\"satellite\"),\n            version=match.group(\"version\"),\n            layer_type=match.group(\"layer\"),\n        )\n\n    @property\n    def num_bands(self) -&gt; int:\n        \"\"\"Get number of bands in layer.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            return src.count\n\n    def read(self, band: int = 1, masked: bool = True) -&gt; np.ndarray:\n        \"\"\"Read single band data.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            if band &lt; 1 or band &gt; src.count:\n                raise ValueError(\n                    f\"Band {band} out of range. File has {src.count} bands.\"\n                )\n\n            data = src.read(band)\n\n            if masked and src.nodata is not None:\n                data = np.ma.masked_equal(data, src.nodata)\n\n        return data\n\n    def read_bands(self, masked: bool = True) -&gt; list[np.ndarray]:\n        \"\"\"Read all bands.\n\n        Parameters\n        ----------\n        masked : bool, optional\n            If True, return masked arrays with nodata values masked.\n            Default is True.\n\n        Returns\n        -------\n        list[np.ndarray]\n            List of arrays, one per band.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Read DEM (single band)\n        &gt;&gt;&gt; dem_layer = StaticLayer.from_path(\"..._dem.tif\")\n        &gt;&gt;&gt; bands = dem_layer.read_bands()\n        &gt;&gt;&gt; dem = bands[0]\n\n        &gt;&gt;&gt; # Read LOS vectors (three bands)\n        &gt;&gt;&gt; los_layer = StaticLayer.from_path(\"..._line_of_sight_enu.tif\")\n        &gt;&gt;&gt; bands = los_layer.read_bands()\n        &gt;&gt;&gt; east, north, up = bands[0], bands[1], bands[2]\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            bands = []\n            for band_idx in range(1, src.count + 1):\n                data = src.read(band_idx)\n                if masked and src.nodata is not None:\n                    data = np.ma.masked_equal(data, src.nodata)\n                bands.append(data)\n\n        return bands\n\n    def to_dataset(self) -&gt; xr.Dataset:\n        \"\"\"Convert raster to xarray Dataset.\"\"\"\n        # Get shape and transform using existing methods\n        height, width = self.get_shape()\n        transform = self.get_transform()\n\n        # Generate x and y coordinates from transform\n        x_coords = np.arange(width) * transform[0] + transform[2] + transform[0] / 2\n        y_coords = np.arange(height) * transform[4] + transform[5] + transform[4] / 2\n\n        # Create coordinates dict\n        coords = {\n            \"y\": ([\"y\"], y_coords),\n            \"x\": ([\"x\"], x_coords),\n        }\n\n        # Create data variables\n        data_vars = {}\n        nodata = self.get_nodata()\n\n        if self.num_bands == 1:\n            # Single band - use layer_type as variable name\n            data = self.read(band=1, masked=False)\n            if nodata is not None:\n                data = np.where(data == nodata, np.nan, data)\n            data_vars[self.layer_type] = ([\"y\", \"x\"], data)\n\n        elif self.layer_type == \"line_of_sight_enu\" and self.num_bands == 3:\n            # Special case: LOS vectors - create three variables\n            band_names = [\"los_east\", \"los_north\", \"los_up\"]\n            bands = self.read_bands(masked=False)\n\n            for name, data in zip(band_names, bands):\n                if nodata is not None:\n                    data = np.where(data == nodata, np.nan, data)\n                data_vars[name] = ([\"y\", \"x\"], data)\n\n        else:\n            # Generic multi-band: use band dimension\n            bands = self.read_bands(masked=False)\n            all_data = np.stack(bands)\n\n            if nodata is not None:\n                all_data = np.where(all_data == nodata, np.nan, all_data)\n\n            coords[\"band\"] = ([\"band\"], np.arange(1, self.num_bands + 1))\n            data_vars[self.layer_type] = ([\"band\", \"y\", \"x\"], all_data)\n\n        # Create dataset\n        ds = xr.Dataset(data_vars=data_vars, coords=coords)\n\n        # Add attributes\n        ds.attrs[\"frame_id\"] = self.frame_id\n        ds.attrs[\"satellite\"] = self.satellite\n        ds.attrs[\"version\"] = self.version\n        ds.attrs[\"reference_date\"] = self.reference_date.isoformat()\n        ds.attrs[\"layer_type\"] = self.layer_type\n\n        # Add CRS information using existing method\n        crs = self.get_crs()\n        if crs:\n            ds.attrs[\"crs_wkt\"] = crs.to_wkt()\n            epsg = self.get_epsg()\n            if epsg:\n                ds.attrs[\"epsg\"] = epsg\n\n        # Add transform\n        ds.attrs[\"transform\"] = list(transform)\n        ds.attrs[\"nodata\"] = nodata\n\n        # Add coordinate attributes\n        ds[\"x\"].attrs[\"standard_name\"] = \"projection_x_coordinate\"\n        ds[\"x\"].attrs[\"long_name\"] = \"x coordinate of projection\"\n        ds[\"x\"].attrs[\"units\"] = \"m\"\n\n        ds[\"y\"].attrs[\"standard_name\"] = \"projection_y_coordinate\"\n        ds[\"y\"].attrs[\"long_name\"] = \"y coordinate of projection\"\n        ds[\"y\"].attrs[\"units\"] = \"m\"\n\n        # Add variable-specific attributes\n        if self.layer_type == \"dem\":\n            ds[\"dem\"].attrs[\"units\"] = \"m\"\n            ds[\"dem\"].attrs[\"long_name\"] = \"Digital Elevation Model\"\n        elif self.layer_type == \"line_of_sight_enu\":\n            ds[\"los_east\"].attrs[\"long_name\"] = \"LOS unit vector - East component\"\n            ds[\"los_north\"].attrs[\"long_name\"] = \"LOS unit vector - North component\"\n            ds[\"los_up\"].attrs[\"long_name\"] = \"LOS unit vector - Up component\"\n        elif self.layer_type == \"layover_shadow_mask\":\n            ds[\"layover_shadow_mask\"].attrs[\"long_name\"] = \"Layover and Shadow Mask\"\n            ds[\"layover_shadow_mask\"].attrs[\n                \"description\"\n            ] = \"0=valid, 1=layover, 2=shadow\"\n\n        return ds\n\n    def compute_incidence_angle(\n        self,\n        fill_value: float = 0.0,\n        dtype: np.dtype = np.float32,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute incidence angle from LOS up component.\n\n        Only valid for line_of_sight_enu layers.\n\n        Parameters\n        ----------\n        fill_value : float, optional\n            Value to use for masked/invalid pixels. Default is 0.0.\n        dtype : np.dtype, optional\n            Output data type. Default is np.float32.\n\n        Returns\n        -------\n        np.ndarray\n            Incidence angle in degrees (0-90\u00b0).\n\n        Raises\n        ------\n        ValueError\n            If not a line_of_sight_enu layer or wrong number of bands.\n\n        \"\"\"\n        if self.layer_type != \"line_of_sight_enu\":\n            raise ValueError(\n                \"compute_incidence_angle() only valid for line_of_sight_enu layers, \"\n                f\"got {self.layer_type}\"\n            )\n\n        if self.num_bands != 3:\n            raise ValueError(f\"Expected 3 bands for LOS ENU, got {self.num_bands}\")\n\n        # Get LOS up component (band 3)\n        bands = self.read_bands()\n        los_up = bands[2]\n\n        # Handle masked arrays\n        if isinstance(los_up, np.ma.MaskedArray):\n            los_up_data = los_up.data\n            mask = los_up.mask\n        else:\n            los_up_data = los_up\n            mask = None\n\n        # Clip to valid range [-1, 1] to avoid arccos domain errors\n        los_up_clipped = np.clip(los_up_data, -1.0, 1.0)\n\n        # Compute incidence angle in degrees\n        incidence_angle = np.rad2deg(np.arccos(los_up_clipped))\n\n        # Apply mask if present\n        if mask is not None:\n            incidence_angle = np.where(mask, fill_value, incidence_angle)\n\n        return incidence_angle.astype(dtype)\n\n    def export_incidence_angle(\n        self,\n        output_path: Path | str,\n        fill_value: float = 0.0,\n        nodata: float | None = 0.0,\n        compress: str = \"DEFLATE\",\n        **kwargs,\n    ) -&gt; Path:\n        \"\"\"Compute and export incidence angle to GeoTIFF.\"\"\"\n        if self.layer_type != \"line_of_sight_enu\":\n            raise ValueError(\n                \"export_incidence_angle() only valid for line_of_sight_enu layers, \"\n                f\"got {self.layer_type}\"\n            )\n\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Compute incidence angle\n        incidence_angle = self.compute_incidence_angle(fill_value=fill_value)\n\n        # Write GeoTIFF\n        profile = {\n            \"driver\": \"GTiff\",\n            \"height\": incidence_angle.shape[0],\n            \"width\": incidence_angle.shape[1],\n            \"count\": 1,\n            \"dtype\": incidence_angle.dtype,\n            \"crs\": self.get_crs(),\n            \"transform\": self.get_transform(),\n            \"nodata\": nodata,\n            \"compress\": compress,\n            \"tiled\": True,\n            \"blockxsize\": 512,\n            \"blockysize\": 512,\n            **kwargs,\n        }\n\n        with rasterio.open(output_path, \"w\", **profile) as dst:\n            dst.write(incidence_angle, 1)\n            dst.set_band_description(1, \"Incidence angle (degrees)\")\n            dst.update_tags(\n                1,\n                units=\"degrees\",\n                description=\"Incidence angle computed from LOS up component\",\n            )\n\n        return output_path\n\n    def get_profile(self) -&gt; dict:\n        \"\"\"Get rasterio profile.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            return src.profile\n\n    def get_transform(self) -&gt; Affine:\n        \"\"\"Get affine transform.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            return src.transform\n\n    def get_crs(self) -&gt; CRS:\n        \"\"\"Get coordinate reference system.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            return src.crs\n\n    def get_epsg(self) -&gt; int | None:\n        \"\"\"Get EPSG code.\"\"\"\n        crs = self.get_crs()\n        return crs.to_epsg() if crs else None\n\n    def get_bounds(self) -&gt; dict[str, float]:\n        \"\"\"Get bounds in native projection.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            bounds = src.bounds\n            return {\n                \"left\": bounds.left,\n                \"bottom\": bounds.bottom,\n                \"right\": bounds.right,\n                \"top\": bounds.top,\n            }\n\n    def get_bounds_wgs84(self) -&gt; dict[str, float]:\n        \"\"\"Get bounds transformed to WGS84.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            bounds = src.bounds\n            west, south, east, north = transform_bounds(\n                src.crs,\n                CRS.from_epsg(4326),\n                bounds.left,\n                bounds.bottom,\n                bounds.right,\n                bounds.top,\n            )\n\n        return {\n            \"west\": west,\n            \"south\": south,\n            \"east\": east,\n            \"north\": north,\n        }\n\n    def get_shape(self) -&gt; tuple[int, int]:\n        \"\"\"Get array shape.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            return (src.height, src.width)\n\n    def get_nodata(self) -&gt; float | None:\n        \"\"\"Get nodata value.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            return src.nodata\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Layer filename.\"\"\"\n        return self.path.name\n\n    @property\n    def exists(self) -&gt; bool:\n        \"\"\"Check if layer file exists.\"\"\"\n        return self.path.exists()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Concise string representation.\"\"\"\n        band_info = f\", bands={self.num_bands}\" if self.exists else \"\"\n        return f\"StaticLayer(frame={self.frame_id}, layer={self.layer_type}{band_info})\"\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.exists","title":"exists  <code>property</code>","text":"<pre><code>exists: bool\n</code></pre> <p>Check if layer file exists.</p>"},{"location":"api/#cal_disp.product.StaticLayer.filename","title":"filename  <code>property</code>","text":"<pre><code>filename: str\n</code></pre> <p>Layer filename.</p>"},{"location":"api/#cal_disp.product.StaticLayer.num_bands","title":"num_bands  <code>property</code>","text":"<pre><code>num_bands: int\n</code></pre> <p>Get number of bands in layer.</p>"},{"location":"api/#cal_disp.product.StaticLayer.compute_incidence_angle","title":"compute_incidence_angle","text":"<pre><code>compute_incidence_angle(fill_value: float = 0.0, dtype: dtype = np.float32) -&gt; np.ndarray\n</code></pre> <p>Compute incidence angle from LOS up component.</p> <p>Only valid for line_of_sight_enu layers.</p> <p>Parameters:</p> Name Type Description Default <code>fill_value</code> <code>float</code> <p>Value to use for masked/invalid pixels. Default is 0.0.</p> <code>0.0</code> <code>dtype</code> <code>dtype</code> <p>Output data type. Default is np.float32.</p> <code>float32</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Incidence angle in degrees (0-90\u00b0).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If not a line_of_sight_enu layer or wrong number of bands.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def compute_incidence_angle(\n    self,\n    fill_value: float = 0.0,\n    dtype: np.dtype = np.float32,\n) -&gt; np.ndarray:\n    \"\"\"Compute incidence angle from LOS up component.\n\n    Only valid for line_of_sight_enu layers.\n\n    Parameters\n    ----------\n    fill_value : float, optional\n        Value to use for masked/invalid pixels. Default is 0.0.\n    dtype : np.dtype, optional\n        Output data type. Default is np.float32.\n\n    Returns\n    -------\n    np.ndarray\n        Incidence angle in degrees (0-90\u00b0).\n\n    Raises\n    ------\n    ValueError\n        If not a line_of_sight_enu layer or wrong number of bands.\n\n    \"\"\"\n    if self.layer_type != \"line_of_sight_enu\":\n        raise ValueError(\n            \"compute_incidence_angle() only valid for line_of_sight_enu layers, \"\n            f\"got {self.layer_type}\"\n        )\n\n    if self.num_bands != 3:\n        raise ValueError(f\"Expected 3 bands for LOS ENU, got {self.num_bands}\")\n\n    # Get LOS up component (band 3)\n    bands = self.read_bands()\n    los_up = bands[2]\n\n    # Handle masked arrays\n    if isinstance(los_up, np.ma.MaskedArray):\n        los_up_data = los_up.data\n        mask = los_up.mask\n    else:\n        los_up_data = los_up\n        mask = None\n\n    # Clip to valid range [-1, 1] to avoid arccos domain errors\n    los_up_clipped = np.clip(los_up_data, -1.0, 1.0)\n\n    # Compute incidence angle in degrees\n    incidence_angle = np.rad2deg(np.arccos(los_up_clipped))\n\n    # Apply mask if present\n    if mask is not None:\n        incidence_angle = np.where(mask, fill_value, incidence_angle)\n\n    return incidence_angle.astype(dtype)\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.export_incidence_angle","title":"export_incidence_angle","text":"<pre><code>export_incidence_angle(output_path: Path | str, fill_value: float = 0.0, nodata: float | None = 0.0, compress: str = 'DEFLATE', **kwargs) -&gt; Path\n</code></pre> <p>Compute and export incidence angle to GeoTIFF.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def export_incidence_angle(\n    self,\n    output_path: Path | str,\n    fill_value: float = 0.0,\n    nodata: float | None = 0.0,\n    compress: str = \"DEFLATE\",\n    **kwargs,\n) -&gt; Path:\n    \"\"\"Compute and export incidence angle to GeoTIFF.\"\"\"\n    if self.layer_type != \"line_of_sight_enu\":\n        raise ValueError(\n            \"export_incidence_angle() only valid for line_of_sight_enu layers, \"\n            f\"got {self.layer_type}\"\n        )\n\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Compute incidence angle\n    incidence_angle = self.compute_incidence_angle(fill_value=fill_value)\n\n    # Write GeoTIFF\n    profile = {\n        \"driver\": \"GTiff\",\n        \"height\": incidence_angle.shape[0],\n        \"width\": incidence_angle.shape[1],\n        \"count\": 1,\n        \"dtype\": incidence_angle.dtype,\n        \"crs\": self.get_crs(),\n        \"transform\": self.get_transform(),\n        \"nodata\": nodata,\n        \"compress\": compress,\n        \"tiled\": True,\n        \"blockxsize\": 512,\n        \"blockysize\": 512,\n        **kwargs,\n    }\n\n    with rasterio.open(output_path, \"w\", **profile) as dst:\n        dst.write(incidence_angle, 1)\n        dst.set_band_description(1, \"Incidence angle (degrees)\")\n        dst.update_tags(\n            1,\n            units=\"degrees\",\n            description=\"Incidence angle computed from LOS up component\",\n        )\n\n    return output_path\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(path: Path | str) -&gt; StaticLayer\n</code></pre> <p>Parse layer metadata from filename.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>@classmethod\ndef from_path(cls, path: Path | str) -&gt; \"StaticLayer\":\n    \"\"\"Parse layer metadata from filename.\"\"\"\n    path = Path(path)\n    match = cls._PATTERN.match(path.name)\n\n    if not match:\n        raise ValueError(\n            f\"Filename does not match OPERA DISP-S1-STATIC pattern: {path.name}\"\n        )\n\n    return cls(\n        path=path,\n        frame_id=int(match.group(\"frame_id\")),\n        reference_date=datetime.strptime(match.group(\"date\"), \"%Y%m%d\"),\n        satellite=match.group(\"satellite\"),\n        version=match.group(\"version\"),\n        layer_type=match.group(\"layer\"),\n    )\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_bounds","title":"get_bounds","text":"<pre><code>get_bounds() -&gt; dict[str, float]\n</code></pre> <p>Get bounds in native projection.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_bounds(self) -&gt; dict[str, float]:\n    \"\"\"Get bounds in native projection.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        bounds = src.bounds\n        return {\n            \"left\": bounds.left,\n            \"bottom\": bounds.bottom,\n            \"right\": bounds.right,\n            \"top\": bounds.top,\n        }\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_bounds_wgs84","title":"get_bounds_wgs84","text":"<pre><code>get_bounds_wgs84() -&gt; dict[str, float]\n</code></pre> <p>Get bounds transformed to WGS84.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_bounds_wgs84(self) -&gt; dict[str, float]:\n    \"\"\"Get bounds transformed to WGS84.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        bounds = src.bounds\n        west, south, east, north = transform_bounds(\n            src.crs,\n            CRS.from_epsg(4326),\n            bounds.left,\n            bounds.bottom,\n            bounds.right,\n            bounds.top,\n        )\n\n    return {\n        \"west\": west,\n        \"south\": south,\n        \"east\": east,\n        \"north\": north,\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_crs","title":"get_crs","text":"<pre><code>get_crs() -&gt; CRS\n</code></pre> <p>Get coordinate reference system.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_crs(self) -&gt; CRS:\n    \"\"\"Get coordinate reference system.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        return src.crs\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_epsg","title":"get_epsg","text":"<pre><code>get_epsg() -&gt; int | None\n</code></pre> <p>Get EPSG code.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_epsg(self) -&gt; int | None:\n    \"\"\"Get EPSG code.\"\"\"\n    crs = self.get_crs()\n    return crs.to_epsg() if crs else None\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_nodata","title":"get_nodata","text":"<pre><code>get_nodata() -&gt; float | None\n</code></pre> <p>Get nodata value.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_nodata(self) -&gt; float | None:\n    \"\"\"Get nodata value.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        return src.nodata\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_profile","title":"get_profile","text":"<pre><code>get_profile() -&gt; dict\n</code></pre> <p>Get rasterio profile.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_profile(self) -&gt; dict:\n    \"\"\"Get rasterio profile.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        return src.profile\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_shape","title":"get_shape","text":"<pre><code>get_shape() -&gt; tuple[int, int]\n</code></pre> <p>Get array shape.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_shape(self) -&gt; tuple[int, int]:\n    \"\"\"Get array shape.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        return (src.height, src.width)\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_transform","title":"get_transform","text":"<pre><code>get_transform() -&gt; Affine\n</code></pre> <p>Get affine transform.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_transform(self) -&gt; Affine:\n    \"\"\"Get affine transform.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        return src.transform\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.read","title":"read","text":"<pre><code>read(band: int = 1, masked: bool = True) -&gt; np.ndarray\n</code></pre> <p>Read single band data.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def read(self, band: int = 1, masked: bool = True) -&gt; np.ndarray:\n    \"\"\"Read single band data.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        if band &lt; 1 or band &gt; src.count:\n            raise ValueError(\n                f\"Band {band} out of range. File has {src.count} bands.\"\n            )\n\n        data = src.read(band)\n\n        if masked and src.nodata is not None:\n            data = np.ma.masked_equal(data, src.nodata)\n\n    return data\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.read_bands","title":"read_bands","text":"<pre><code>read_bands(masked: bool = True) -&gt; list[np.ndarray]\n</code></pre> <p>Read all bands.</p> <p>Parameters:</p> Name Type Description Default <code>masked</code> <code>bool</code> <p>If True, return masked arrays with nodata values masked. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>List of arrays, one per band.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Read DEM (single band)\n&gt;&gt;&gt; dem_layer = StaticLayer.from_path(\"..._dem.tif\")\n&gt;&gt;&gt; bands = dem_layer.read_bands()\n&gt;&gt;&gt; dem = bands[0]\n</code></pre> <pre><code>&gt;&gt;&gt; # Read LOS vectors (three bands)\n&gt;&gt;&gt; los_layer = StaticLayer.from_path(\"..._line_of_sight_enu.tif\")\n&gt;&gt;&gt; bands = los_layer.read_bands()\n&gt;&gt;&gt; east, north, up = bands[0], bands[1], bands[2]\n</code></pre> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def read_bands(self, masked: bool = True) -&gt; list[np.ndarray]:\n    \"\"\"Read all bands.\n\n    Parameters\n    ----------\n    masked : bool, optional\n        If True, return masked arrays with nodata values masked.\n        Default is True.\n\n    Returns\n    -------\n    list[np.ndarray]\n        List of arrays, one per band.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Read DEM (single band)\n    &gt;&gt;&gt; dem_layer = StaticLayer.from_path(\"..._dem.tif\")\n    &gt;&gt;&gt; bands = dem_layer.read_bands()\n    &gt;&gt;&gt; dem = bands[0]\n\n    &gt;&gt;&gt; # Read LOS vectors (three bands)\n    &gt;&gt;&gt; los_layer = StaticLayer.from_path(\"..._line_of_sight_enu.tif\")\n    &gt;&gt;&gt; bands = los_layer.read_bands()\n    &gt;&gt;&gt; east, north, up = bands[0], bands[1], bands[2]\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        bands = []\n        for band_idx in range(1, src.count + 1):\n            data = src.read(band_idx)\n            if masked and src.nodata is not None:\n                data = np.ma.masked_equal(data, src.nodata)\n            bands.append(data)\n\n    return bands\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.to_dataset","title":"to_dataset","text":"<pre><code>to_dataset() -&gt; xr.Dataset\n</code></pre> <p>Convert raster to xarray Dataset.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def to_dataset(self) -&gt; xr.Dataset:\n    \"\"\"Convert raster to xarray Dataset.\"\"\"\n    # Get shape and transform using existing methods\n    height, width = self.get_shape()\n    transform = self.get_transform()\n\n    # Generate x and y coordinates from transform\n    x_coords = np.arange(width) * transform[0] + transform[2] + transform[0] / 2\n    y_coords = np.arange(height) * transform[4] + transform[5] + transform[4] / 2\n\n    # Create coordinates dict\n    coords = {\n        \"y\": ([\"y\"], y_coords),\n        \"x\": ([\"x\"], x_coords),\n    }\n\n    # Create data variables\n    data_vars = {}\n    nodata = self.get_nodata()\n\n    if self.num_bands == 1:\n        # Single band - use layer_type as variable name\n        data = self.read(band=1, masked=False)\n        if nodata is not None:\n            data = np.where(data == nodata, np.nan, data)\n        data_vars[self.layer_type] = ([\"y\", \"x\"], data)\n\n    elif self.layer_type == \"line_of_sight_enu\" and self.num_bands == 3:\n        # Special case: LOS vectors - create three variables\n        band_names = [\"los_east\", \"los_north\", \"los_up\"]\n        bands = self.read_bands(masked=False)\n\n        for name, data in zip(band_names, bands):\n            if nodata is not None:\n                data = np.where(data == nodata, np.nan, data)\n            data_vars[name] = ([\"y\", \"x\"], data)\n\n    else:\n        # Generic multi-band: use band dimension\n        bands = self.read_bands(masked=False)\n        all_data = np.stack(bands)\n\n        if nodata is not None:\n            all_data = np.where(all_data == nodata, np.nan, all_data)\n\n        coords[\"band\"] = ([\"band\"], np.arange(1, self.num_bands + 1))\n        data_vars[self.layer_type] = ([\"band\", \"y\", \"x\"], all_data)\n\n    # Create dataset\n    ds = xr.Dataset(data_vars=data_vars, coords=coords)\n\n    # Add attributes\n    ds.attrs[\"frame_id\"] = self.frame_id\n    ds.attrs[\"satellite\"] = self.satellite\n    ds.attrs[\"version\"] = self.version\n    ds.attrs[\"reference_date\"] = self.reference_date.isoformat()\n    ds.attrs[\"layer_type\"] = self.layer_type\n\n    # Add CRS information using existing method\n    crs = self.get_crs()\n    if crs:\n        ds.attrs[\"crs_wkt\"] = crs.to_wkt()\n        epsg = self.get_epsg()\n        if epsg:\n            ds.attrs[\"epsg\"] = epsg\n\n    # Add transform\n    ds.attrs[\"transform\"] = list(transform)\n    ds.attrs[\"nodata\"] = nodata\n\n    # Add coordinate attributes\n    ds[\"x\"].attrs[\"standard_name\"] = \"projection_x_coordinate\"\n    ds[\"x\"].attrs[\"long_name\"] = \"x coordinate of projection\"\n    ds[\"x\"].attrs[\"units\"] = \"m\"\n\n    ds[\"y\"].attrs[\"standard_name\"] = \"projection_y_coordinate\"\n    ds[\"y\"].attrs[\"long_name\"] = \"y coordinate of projection\"\n    ds[\"y\"].attrs[\"units\"] = \"m\"\n\n    # Add variable-specific attributes\n    if self.layer_type == \"dem\":\n        ds[\"dem\"].attrs[\"units\"] = \"m\"\n        ds[\"dem\"].attrs[\"long_name\"] = \"Digital Elevation Model\"\n    elif self.layer_type == \"line_of_sight_enu\":\n        ds[\"los_east\"].attrs[\"long_name\"] = \"LOS unit vector - East component\"\n        ds[\"los_north\"].attrs[\"long_name\"] = \"LOS unit vector - North component\"\n        ds[\"los_up\"].attrs[\"long_name\"] = \"LOS unit vector - Up component\"\n    elif self.layer_type == \"layover_shadow_mask\":\n        ds[\"layover_shadow_mask\"].attrs[\"long_name\"] = \"Layover and Shadow Mask\"\n        ds[\"layover_shadow_mask\"].attrs[\n            \"description\"\n        ] = \"0=valid, 1=layover, 2=shadow\"\n\n    return ds\n</code></pre>"},{"location":"api/#cal_disp.product.TropoProduct","title":"TropoProduct  <code>dataclass</code>","text":"<p>OPERA TROPO-ZENITH tropospheric delay product.</p> <p>Minimal class for managing OPERA tropospheric products. Processing functions are standalone for composability.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; product = TropoProduct.from_path(\"tropo.nc\")\n&gt;&gt;&gt; ds = product.open_dataset()\n&gt;&gt;&gt; total = product.get_total_delay()\n</code></pre> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>@dataclass\nclass TropoProduct:\n    \"\"\"OPERA TROPO-ZENITH tropospheric delay product.\n\n    Minimal class for managing OPERA tropospheric products.\n    Processing functions are standalone for composability.\n\n    Examples\n    --------\n    &gt;&gt;&gt; product = TropoProduct.from_path(\"tropo.nc\")\n    &gt;&gt;&gt; ds = product.open_dataset()\n    &gt;&gt;&gt; total = product.get_total_delay()\n\n    \"\"\"\n\n    path: Path\n    date: datetime\n    production_date: datetime\n    model: str\n    version: str\n\n    _PATTERN = re.compile(\n        r\"OPERA_L4_TROPO-ZENITH_\"\n        r\"(?P&lt;date&gt;\\d{8}T\\d{6}Z)_\"\n        r\"(?P&lt;production&gt;\\d{8}T\\d{6}Z)_\"\n        r\"(?P&lt;model&gt;\\w+)_\"\n        r\"v(?P&lt;version&gt;[\\d.]+)\"\n        r\"\\.nc$\"\n    )\n\n    TROPO_LAYERS = [\n        \"wet_delay\",\n        \"hydrostatic_delay\",\n    ]\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate product after construction.\"\"\"\n        self.path = Path(self.path)\n\n        if self.production_date &lt; self.date:\n            raise ValueError(\n                f\"Production date ({self.production_date}) cannot be before \"\n                f\"model date ({self.date})\"\n            )\n\n    @classmethod\n    def from_path(cls, path: Path | str) -&gt; \"TropoProduct\":\n        \"\"\"Parse product metadata from filename.\"\"\"\n        path = Path(path)\n        match = cls._PATTERN.match(path.name)\n\n        if not match:\n            raise ValueError(\n                f\"Filename does not match OPERA TROPO-ZENITH pattern: {path.name}\"\n            )\n\n        return cls(\n            path=path,\n            date=datetime.strptime(match.group(\"date\"), \"%Y%m%dT%H%M%SZ\"),\n            production_date=datetime.strptime(\n                match.group(\"production\"), \"%Y%m%dT%H%M%SZ\"\n            ),\n            model=match.group(\"model\"),\n            version=match.group(\"version\"),\n        )\n\n    def matches_date(self, target_date: datetime, hours: float = 6.0) -&gt; bool:\n        \"\"\"Check if product date is within time window of target date.\"\"\"\n        delta = abs(self.date - target_date)\n        return delta &lt;= timedelta(hours=hours)\n\n    def open_dataset(\n        self,\n        bounds: tuple[float, float, float, float] | None = None,\n        max_height: float | None = None,\n        bounds_crs: str = \"EPSG:4326\",\n        bounds_buffer: float = 0.0,\n    ) -&gt; xr.Dataset:\n        \"\"\"Open tropospheric delay dataset with optional subsetting.\n\n        Parameters\n        ----------\n        bounds : tuple[float, float, float, float] or None, optional\n            Spatial bounds as (west, south, east, north). Default is None.\n        max_height : float or None, optional\n            Maximum height in meters. Default is None.\n        bounds_crs : str, optional\n            CRS of bounds. Default is \"EPSG:4326\".\n        bounds_buffer : float, optional\n            Buffer to add to bounds in degrees (for lat/lon) or meters\n            (for projected CRS). Default is 0.0. Useful value: 0.2 for lat/lon.\n\n        Returns\n        -------\n        xr.Dataset\n            Tropospheric delay dataset.\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n        ds = xr.open_dataset(self.path, engine=\"h5netcdf\")\n\n        if bounds is not None:\n            # Apply buffer if requested\n            if bounds_buffer &gt; 0:\n                west, south, east, north = bounds\n                bounds = (\n                    west - bounds_buffer,\n                    south - bounds_buffer,\n                    east + bounds_buffer,\n                    north + bounds_buffer,\n                )\n            ds = self._subset_spatial(ds, bounds, bounds_crs)\n\n        if max_height is not None:\n            ds = self._subset_height(ds, max_height)\n\n        return ds\n\n    def get_total_delay(\n        self,\n        time_idx: int = 0,\n        bounds: tuple[float, float, float, float] | None = None,\n        max_height: float | None = None,\n        bounds_crs: str = \"EPSG:4326\",\n        bounds_buffer: float = 0.0,\n    ) -&gt; xr.DataArray:\n        \"\"\"Get total zenith delay (wet + hydrostatic).\n\n        Computes total delay as sum of wet and hydrostatic components.\n\n        Parameters\n        ----------\n        time_idx : int, optional\n            Time index to extract. Default is 0.\n        bounds : tuple[float, float, float, float] or None, optional\n            Spatial bounds for subsetting. Default is None.\n        max_height : float or None, optional\n            Maximum height for subsetting. Default is None.\n        bounds_crs : str, optional\n            CRS of bounds. Default is \"EPSG:4326\".\n        bounds_buffer : float, optional\n            Buffer to add to bounds. Default is 0.0.\n\n        Returns\n        -------\n        xr.DataArray\n            Total zenith delay with dimensions (height, latitude, longitude).\n\n        \"\"\"\n        ds = self.open_dataset(\n            bounds=bounds,\n            max_height=max_height,\n            bounds_crs=bounds_crs,\n            bounds_buffer=bounds_buffer,\n        )\n\n        # Compute total delay from wet + hydrostatic\n        if \"wet_delay\" not in ds:\n            raise ValueError(\"wet_delay not found in dataset\")\n        if \"hydrostatic_delay\" not in ds:\n            raise ValueError(\"hydrostatic_delay not found in dataset\")\n\n        da = ds[\"wet_delay\"] + ds[\"hydrostatic_delay\"]\n        da.name = \"zenith_total_delay\"\n        da.attrs.update(\n            {\n                \"long_name\": \"Total zenith tropospheric delay\",\n                \"units\": \"meters\",\n                \"description\": \"Sum of wet and hydrostatic components\",\n            }\n        )\n\n        # Preserve spatial reference\n        if \"spatial_ref\" in ds:\n            da = da.assign_coords({\"spatial_ref\": ds[\"spatial_ref\"]})\n\n        # Handle time dimension\n        if \"time\" in da.dims:\n            n_times = len(da.time)\n            if abs(time_idx) &gt;= n_times:\n                raise ValueError(\n                    f\"time_idx {time_idx} out of range for {n_times} timesteps\"\n                )\n            da = da.isel(time=time_idx)\n\n        return da\n\n    def _subset_spatial(\n        self,\n        ds: xr.Dataset,\n        bounds: tuple[float, float, float, float],\n        bounds_crs: str,\n    ) -&gt; xr.Dataset:\n        \"\"\"Subset dataset to spatial bounds.\"\"\"\n        west, south, east, north = bounds\n\n        ds_crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n        if ds_crs_wkt is None:\n            raise ValueError(\"Dataset missing CRS information\")\n\n        ds_crs = CRS.from_wkt(ds_crs_wkt)\n\n        # Transform bounds to dataset CRS if needed\n        if bounds_crs != ds_crs.to_string():\n            left, bottom, right, top = rio_transform_bounds(\n                CRS.from_string(bounds_crs),\n                ds_crs,\n                west,\n                south,\n                east,\n                north,\n            )\n        else:\n            left, bottom, right, top = west, south, east, north\n\n        # Use latitude/longitude for subsetting\n        ds_subset = ds.sel(\n            longitude=slice(left, right),\n            latitude=slice(top, bottom),\n        )\n\n        return ds_subset\n\n    def _subset_height(self, ds: xr.Dataset, max_height: float) -&gt; xr.Dataset:\n        \"\"\"Subset dataset by maximum height.\"\"\"\n        if \"height\" not in ds.dims:\n            return ds\n\n        return ds.where(ds[\"height\"] &lt;= max_height, drop=True)\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Product filename.\"\"\"\n        return self.path.name\n\n    @property\n    def exists(self) -&gt; bool:\n        \"\"\"Check if product file exists.\"\"\"\n        return self.path.exists()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation.\"\"\"\n        return f\"TropoProduct(date={self.date.isoformat()}, model={self.model})\"\n</code></pre>"},{"location":"api/#cal_disp.product.TropoProduct.exists","title":"exists  <code>property</code>","text":"<pre><code>exists: bool\n</code></pre> <p>Check if product file exists.</p>"},{"location":"api/#cal_disp.product.TropoProduct.filename","title":"filename  <code>property</code>","text":"<pre><code>filename: str\n</code></pre> <p>Product filename.</p>"},{"location":"api/#cal_disp.product.TropoProduct.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(path: Path | str) -&gt; TropoProduct\n</code></pre> <p>Parse product metadata from filename.</p> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>@classmethod\ndef from_path(cls, path: Path | str) -&gt; \"TropoProduct\":\n    \"\"\"Parse product metadata from filename.\"\"\"\n    path = Path(path)\n    match = cls._PATTERN.match(path.name)\n\n    if not match:\n        raise ValueError(\n            f\"Filename does not match OPERA TROPO-ZENITH pattern: {path.name}\"\n        )\n\n    return cls(\n        path=path,\n        date=datetime.strptime(match.group(\"date\"), \"%Y%m%dT%H%M%SZ\"),\n        production_date=datetime.strptime(\n            match.group(\"production\"), \"%Y%m%dT%H%M%SZ\"\n        ),\n        model=match.group(\"model\"),\n        version=match.group(\"version\"),\n    )\n</code></pre>"},{"location":"api/#cal_disp.product.TropoProduct.get_total_delay","title":"get_total_delay","text":"<pre><code>get_total_delay(time_idx: int = 0, bounds: tuple[float, float, float, float] | None = None, max_height: float | None = None, bounds_crs: str = 'EPSG:4326', bounds_buffer: float = 0.0) -&gt; xr.DataArray\n</code></pre> <p>Get total zenith delay (wet + hydrostatic).</p> <p>Computes total delay as sum of wet and hydrostatic components.</p> <p>Parameters:</p> Name Type Description Default <code>time_idx</code> <code>int</code> <p>Time index to extract. Default is 0.</p> <code>0</code> <code>bounds</code> <code>tuple[float, float, float, float] or None</code> <p>Spatial bounds for subsetting. Default is None.</p> <code>None</code> <code>max_height</code> <code>float or None</code> <p>Maximum height for subsetting. Default is None.</p> <code>None</code> <code>bounds_crs</code> <code>str</code> <p>CRS of bounds. Default is \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>bounds_buffer</code> <code>float</code> <p>Buffer to add to bounds. Default is 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>Total zenith delay with dimensions (height, latitude, longitude).</p> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>def get_total_delay(\n    self,\n    time_idx: int = 0,\n    bounds: tuple[float, float, float, float] | None = None,\n    max_height: float | None = None,\n    bounds_crs: str = \"EPSG:4326\",\n    bounds_buffer: float = 0.0,\n) -&gt; xr.DataArray:\n    \"\"\"Get total zenith delay (wet + hydrostatic).\n\n    Computes total delay as sum of wet and hydrostatic components.\n\n    Parameters\n    ----------\n    time_idx : int, optional\n        Time index to extract. Default is 0.\n    bounds : tuple[float, float, float, float] or None, optional\n        Spatial bounds for subsetting. Default is None.\n    max_height : float or None, optional\n        Maximum height for subsetting. Default is None.\n    bounds_crs : str, optional\n        CRS of bounds. Default is \"EPSG:4326\".\n    bounds_buffer : float, optional\n        Buffer to add to bounds. Default is 0.0.\n\n    Returns\n    -------\n    xr.DataArray\n        Total zenith delay with dimensions (height, latitude, longitude).\n\n    \"\"\"\n    ds = self.open_dataset(\n        bounds=bounds,\n        max_height=max_height,\n        bounds_crs=bounds_crs,\n        bounds_buffer=bounds_buffer,\n    )\n\n    # Compute total delay from wet + hydrostatic\n    if \"wet_delay\" not in ds:\n        raise ValueError(\"wet_delay not found in dataset\")\n    if \"hydrostatic_delay\" not in ds:\n        raise ValueError(\"hydrostatic_delay not found in dataset\")\n\n    da = ds[\"wet_delay\"] + ds[\"hydrostatic_delay\"]\n    da.name = \"zenith_total_delay\"\n    da.attrs.update(\n        {\n            \"long_name\": \"Total zenith tropospheric delay\",\n            \"units\": \"meters\",\n            \"description\": \"Sum of wet and hydrostatic components\",\n        }\n    )\n\n    # Preserve spatial reference\n    if \"spatial_ref\" in ds:\n        da = da.assign_coords({\"spatial_ref\": ds[\"spatial_ref\"]})\n\n    # Handle time dimension\n    if \"time\" in da.dims:\n        n_times = len(da.time)\n        if abs(time_idx) &gt;= n_times:\n            raise ValueError(\n                f\"time_idx {time_idx} out of range for {n_times} timesteps\"\n            )\n        da = da.isel(time=time_idx)\n\n    return da\n</code></pre>"},{"location":"api/#cal_disp.product.TropoProduct.matches_date","title":"matches_date","text":"<pre><code>matches_date(target_date: datetime, hours: float = 6.0) -&gt; bool\n</code></pre> <p>Check if product date is within time window of target date.</p> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>def matches_date(self, target_date: datetime, hours: float = 6.0) -&gt; bool:\n    \"\"\"Check if product date is within time window of target date.\"\"\"\n    delta = abs(self.date - target_date)\n    return delta &lt;= timedelta(hours=hours)\n</code></pre>"},{"location":"api/#cal_disp.product.TropoProduct.open_dataset","title":"open_dataset","text":"<pre><code>open_dataset(bounds: tuple[float, float, float, float] | None = None, max_height: float | None = None, bounds_crs: str = 'EPSG:4326', bounds_buffer: float = 0.0) -&gt; xr.Dataset\n</code></pre> <p>Open tropospheric delay dataset with optional subsetting.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>tuple[float, float, float, float] or None</code> <p>Spatial bounds as (west, south, east, north). Default is None.</p> <code>None</code> <code>max_height</code> <code>float or None</code> <p>Maximum height in meters. Default is None.</p> <code>None</code> <code>bounds_crs</code> <code>str</code> <p>CRS of bounds. Default is \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>bounds_buffer</code> <code>float</code> <p>Buffer to add to bounds in degrees (for lat/lon) or meters (for projected CRS). Default is 0.0. Useful value: 0.2 for lat/lon.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Tropospheric delay dataset.</p> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>def open_dataset(\n    self,\n    bounds: tuple[float, float, float, float] | None = None,\n    max_height: float | None = None,\n    bounds_crs: str = \"EPSG:4326\",\n    bounds_buffer: float = 0.0,\n) -&gt; xr.Dataset:\n    \"\"\"Open tropospheric delay dataset with optional subsetting.\n\n    Parameters\n    ----------\n    bounds : tuple[float, float, float, float] or None, optional\n        Spatial bounds as (west, south, east, north). Default is None.\n    max_height : float or None, optional\n        Maximum height in meters. Default is None.\n    bounds_crs : str, optional\n        CRS of bounds. Default is \"EPSG:4326\".\n    bounds_buffer : float, optional\n        Buffer to add to bounds in degrees (for lat/lon) or meters\n        (for projected CRS). Default is 0.0. Useful value: 0.2 for lat/lon.\n\n    Returns\n    -------\n    xr.Dataset\n        Tropospheric delay dataset.\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n    ds = xr.open_dataset(self.path, engine=\"h5netcdf\")\n\n    if bounds is not None:\n        # Apply buffer if requested\n        if bounds_buffer &gt; 0:\n            west, south, east, north = bounds\n            bounds = (\n                west - bounds_buffer,\n                south - bounds_buffer,\n                east + bounds_buffer,\n                north + bounds_buffer,\n            )\n        ds = self._subset_spatial(ds, bounds, bounds_crs)\n\n    if max_height is not None:\n        ds = self._subset_height(ds, max_height)\n\n    return ds\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid","title":"UnrGrid  <code>dataclass</code>","text":"<p>UNR GNSS grid data.</p> <p>Represents gridded GNSS velocity data from University of Nevada Reno. Data is stored as parquet with point geometries and metadata.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load from path (frame_id parsed if in filename)\n&gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n&gt;&gt;&gt; grid.frame_id\n8882\n</code></pre> <pre><code>&gt;&gt;&gt; # Load GeoDataFrame\n&gt;&gt;&gt; gdf = grid.load()\n&gt;&gt;&gt; gdf.columns\n['lon', 'lat', 'east', 'north', 'up', 'geometry', ...]\n</code></pre> <pre><code>&gt;&gt;&gt; # Get metadata\n&gt;&gt;&gt; meta = grid.get_metadata()\n&gt;&gt;&gt; meta['source']\n'UNR'\n</code></pre> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>@dataclass\nclass UnrGrid:\n    \"\"\"UNR GNSS grid data.\n\n    Represents gridded GNSS velocity data from University of Nevada Reno.\n    Data is stored as parquet with point geometries and metadata.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Load from path (frame_id parsed if in filename)\n    &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n    &gt;&gt;&gt; grid.frame_id\n    8882\n\n    &gt;&gt;&gt; # Load GeoDataFrame\n    &gt;&gt;&gt; gdf = grid.load()\n    &gt;&gt;&gt; gdf.columns\n    ['lon', 'lat', 'east', 'north', 'up', 'geometry', ...]\n\n    &gt;&gt;&gt; # Get metadata\n    &gt;&gt;&gt; meta = grid.get_metadata()\n    &gt;&gt;&gt; meta['source']\n    'UNR'\n\n    \"\"\"\n\n    path: Path\n    frame_id: int | None = None\n\n    # Optional pattern to extract frame_id from filename\n    _PATTERN = re.compile(r\"frame[\\s_-]?(\\d+)\", re.IGNORECASE)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate grid after construction.\"\"\"\n        self.path = Path(self.path)\n\n    @classmethod\n    def from_path(cls, path: Path | str, frame_id: int | None = None) -&gt; \"UnrGrid\":\n        \"\"\"Create UnrGrid from parquet file path.\n\n        Parameters\n        ----------\n        path : Path or str\n            Path to UNR parquet file.\n        frame_id : int or None, optional\n            Frame ID. If None, attempts to parse from filename.\n            Default is None.\n\n        Returns\n        -------\n        UnrGrid\n            Grid instance.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Frame ID from filename\n        &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n        &gt;&gt;&gt; grid.frame_id\n        8882\n\n        &gt;&gt;&gt; # Explicit frame ID\n        &gt;&gt;&gt; grid = UnrGrid.from_path(\"custom_unr_data.parquet\", frame_id=8882)\n        &gt;&gt;&gt; grid.frame_id\n        8882\n\n        &gt;&gt;&gt; # No frame ID\n        &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_data.parquet\")\n        &gt;&gt;&gt; grid.frame_id is None\n        True\n\n        \"\"\"\n        path = Path(path)\n\n        # Try to parse frame_id from filename if not provided\n        if frame_id is None:\n            match = cls._PATTERN.search(path.name)\n            if match:\n                frame_id = int(match.group(1))\n\n        return cls(path=path, frame_id=frame_id)\n\n    def load(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Load UNR grid as GeoDataFrame.\n\n        Returns\n        -------\n        gpd.GeoDataFrame\n            GeoDataFrame with point geometries and velocity data.\n\n        Raises\n        ------\n        FileNotFoundError\n            If parquet file does not exist.\n\n        Examples\n        --------\n        &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n        &gt;&gt;&gt; gdf = grid.load()\n        &gt;&gt;&gt; gdf.crs\n        'EPSG:4326'\n        &gt;&gt;&gt; gdf[['lon', 'lat', 'east', 'north', 'up']].head()\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"UNR grid file not found: {self.path}\")\n\n        # Load parquet as DataFrame\n        df = pd.read_parquet(self.path)\n\n        # Create GeoDataFrame with point geometries\n        gdf = gpd.GeoDataFrame(\n            df,\n            geometry=gpd.points_from_xy(x=df.lon, y=df.lat),\n            crs=\"EPSG:4326\",\n        )\n\n        return gdf\n\n    def get_metadata(self) -&gt; dict[str, str]:\n        \"\"\"Extract metadata from parquet file.\n\n        Returns\n        -------\n        dict[str, str]\n            Metadata dictionary.\n\n        Examples\n        --------\n        &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n        &gt;&gt;&gt; meta = grid.get_metadata()\n        &gt;&gt;&gt; meta.keys()\n        dict_keys(['source', 'date_created', 'frame_id', ...])\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"UNR grid file not found: {self.path}\")\n\n        meta = pq.read_metadata(self.path).metadata\n\n        if meta is None:\n            return {}\n\n        metadata_dict = {k.decode(): v.decode() for k, v in meta.items()}\n\n        return metadata_dict\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Load as regular DataFrame without geometry.\n\n        Returns\n        -------\n        pd.DataFrame\n            DataFrame with lon, lat, and velocity columns.\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"UNR grid file not found: {self.path}\")\n\n        return pd.read_parquet(self.path)\n\n    def get_bounds(self) -&gt; dict[str, float]:\n        \"\"\"Get spatial bounds of grid.\n\n        Returns\n        -------\n        dict[str, float]\n            Dictionary with keys: west, south, east, north.\n\n        \"\"\"\n        gdf = self.load()\n        bounds = gdf.total_bounds  # (minx, miny, maxx, maxy)\n\n        return {\n            \"west\": bounds[0],\n            \"south\": bounds[1],\n            \"east\": bounds[2],\n            \"north\": bounds[3],\n        }\n\n    def get_station_count(self) -&gt; int:\n        \"\"\"Get number of GNSS stations in grid.\n\n        Returns\n        -------\n        int\n            Number of stations.\n\n        \"\"\"\n        df = self.to_dataframe()\n        return len(df)\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Grid filename.\"\"\"\n        return self.path.name\n\n    @property\n    def exists(self) -&gt; bool:\n        \"\"\"Check if grid file exists.\"\"\"\n        return self.path.exists()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation.\"\"\"\n        frame_str = f\"frame={self.frame_id}\" if self.frame_id else \"frame=None\"\n        return (\n            f\"UnrGrid({frame_str},\"\n            f\" stations={self.get_station_count() if self.exists else '?'})\"\n        )\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid.exists","title":"exists  <code>property</code>","text":"<pre><code>exists: bool\n</code></pre> <p>Check if grid file exists.</p>"},{"location":"api/#cal_disp.product.UnrGrid.filename","title":"filename  <code>property</code>","text":"<pre><code>filename: str\n</code></pre> <p>Grid filename.</p>"},{"location":"api/#cal_disp.product.UnrGrid.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(path: Path | str, frame_id: int | None = None) -&gt; UnrGrid\n</code></pre> <p>Create UnrGrid from parquet file path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path or str</code> <p>Path to UNR parquet file.</p> required <code>frame_id</code> <code>int or None</code> <p>Frame ID. If None, attempts to parse from filename. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>UnrGrid</code> <p>Grid instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Frame ID from filename\n&gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n&gt;&gt;&gt; grid.frame_id\n8882\n</code></pre> <pre><code>&gt;&gt;&gt; # Explicit frame ID\n&gt;&gt;&gt; grid = UnrGrid.from_path(\"custom_unr_data.parquet\", frame_id=8882)\n&gt;&gt;&gt; grid.frame_id\n8882\n</code></pre> <pre><code>&gt;&gt;&gt; # No frame ID\n&gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_data.parquet\")\n&gt;&gt;&gt; grid.frame_id is None\nTrue\n</code></pre> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>@classmethod\ndef from_path(cls, path: Path | str, frame_id: int | None = None) -&gt; \"UnrGrid\":\n    \"\"\"Create UnrGrid from parquet file path.\n\n    Parameters\n    ----------\n    path : Path or str\n        Path to UNR parquet file.\n    frame_id : int or None, optional\n        Frame ID. If None, attempts to parse from filename.\n        Default is None.\n\n    Returns\n    -------\n    UnrGrid\n        Grid instance.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Frame ID from filename\n    &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n    &gt;&gt;&gt; grid.frame_id\n    8882\n\n    &gt;&gt;&gt; # Explicit frame ID\n    &gt;&gt;&gt; grid = UnrGrid.from_path(\"custom_unr_data.parquet\", frame_id=8882)\n    &gt;&gt;&gt; grid.frame_id\n    8882\n\n    &gt;&gt;&gt; # No frame ID\n    &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_data.parquet\")\n    &gt;&gt;&gt; grid.frame_id is None\n    True\n\n    \"\"\"\n    path = Path(path)\n\n    # Try to parse frame_id from filename if not provided\n    if frame_id is None:\n        match = cls._PATTERN.search(path.name)\n        if match:\n            frame_id = int(match.group(1))\n\n    return cls(path=path, frame_id=frame_id)\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid.get_bounds","title":"get_bounds","text":"<pre><code>get_bounds() -&gt; dict[str, float]\n</code></pre> <p>Get spatial bounds of grid.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with keys: west, south, east, north.</p> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>def get_bounds(self) -&gt; dict[str, float]:\n    \"\"\"Get spatial bounds of grid.\n\n    Returns\n    -------\n    dict[str, float]\n        Dictionary with keys: west, south, east, north.\n\n    \"\"\"\n    gdf = self.load()\n    bounds = gdf.total_bounds  # (minx, miny, maxx, maxy)\n\n    return {\n        \"west\": bounds[0],\n        \"south\": bounds[1],\n        \"east\": bounds[2],\n        \"north\": bounds[3],\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata() -&gt; dict[str, str]\n</code></pre> <p>Extract metadata from parquet file.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Metadata dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n&gt;&gt;&gt; meta = grid.get_metadata()\n&gt;&gt;&gt; meta.keys()\ndict_keys(['source', 'date_created', 'frame_id', ...])\n</code></pre> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>def get_metadata(self) -&gt; dict[str, str]:\n    \"\"\"Extract metadata from parquet file.\n\n    Returns\n    -------\n    dict[str, str]\n        Metadata dictionary.\n\n    Examples\n    --------\n    &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n    &gt;&gt;&gt; meta = grid.get_metadata()\n    &gt;&gt;&gt; meta.keys()\n    dict_keys(['source', 'date_created', 'frame_id', ...])\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"UNR grid file not found: {self.path}\")\n\n    meta = pq.read_metadata(self.path).metadata\n\n    if meta is None:\n        return {}\n\n    metadata_dict = {k.decode(): v.decode() for k, v in meta.items()}\n\n    return metadata_dict\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid.get_station_count","title":"get_station_count","text":"<pre><code>get_station_count() -&gt; int\n</code></pre> <p>Get number of GNSS stations in grid.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of stations.</p> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>def get_station_count(self) -&gt; int:\n    \"\"\"Get number of GNSS stations in grid.\n\n    Returns\n    -------\n    int\n        Number of stations.\n\n    \"\"\"\n    df = self.to_dataframe()\n    return len(df)\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid.load","title":"load","text":"<pre><code>load() -&gt; gpd.GeoDataFrame\n</code></pre> <p>Load UNR grid as GeoDataFrame.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with point geometries and velocity data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If parquet file does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n&gt;&gt;&gt; gdf = grid.load()\n&gt;&gt;&gt; gdf.crs\n'EPSG:4326'\n&gt;&gt;&gt; gdf[['lon', 'lat', 'east', 'north', 'up']].head()\n</code></pre> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>def load(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Load UNR grid as GeoDataFrame.\n\n    Returns\n    -------\n    gpd.GeoDataFrame\n        GeoDataFrame with point geometries and velocity data.\n\n    Raises\n    ------\n    FileNotFoundError\n        If parquet file does not exist.\n\n    Examples\n    --------\n    &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n    &gt;&gt;&gt; gdf = grid.load()\n    &gt;&gt;&gt; gdf.crs\n    'EPSG:4326'\n    &gt;&gt;&gt; gdf[['lon', 'lat', 'east', 'north', 'up']].head()\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"UNR grid file not found: {self.path}\")\n\n    # Load parquet as DataFrame\n    df = pd.read_parquet(self.path)\n\n    # Create GeoDataFrame with point geometries\n    gdf = gpd.GeoDataFrame(\n        df,\n        geometry=gpd.points_from_xy(x=df.lon, y=df.lat),\n        crs=\"EPSG:4326\",\n    )\n\n    return gdf\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe() -&gt; pd.DataFrame\n</code></pre> <p>Load as regular DataFrame without geometry.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with lon, lat, and velocity columns.</p> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Load as regular DataFrame without geometry.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with lon, lat, and velocity columns.\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"UNR grid file not found: {self.path}\")\n\n    return pd.read_parquet(self.path)\n</code></pre>"},{"location":"api/#cal_disp.product.bounds_contains","title":"bounds_contains","text":"<pre><code>bounds_contains(outer_bounds: tuple[float, float, float, float] | dict[str, float], inner_bounds: tuple[float, float, float, float] | dict[str, float], buffer: float = 0.0) -&gt; bool\n</code></pre> <p>Check if outer bounds completely contain inner bounds.</p> <p>Parameters:</p> Name Type Description Default <code>outer_bounds</code> <code>tuple or dict</code> <p>Outer bounds as (west, south, east, north) or dict with those keys.</p> required <code>inner_bounds</code> <code>tuple or dict</code> <p>Inner bounds as (west, south, east, north) or dict with those keys.</p> required <code>buffer</code> <code>float</code> <p>Buffer distance to require around inner bounds (in same units). Default is 0.0 (exact containment).</p> <code>0.0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if outer bounds completely contain inner bounds (with buffer).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Check if UNR grid covers DISP frame\n&gt;&gt;&gt; unr_bounds = (-97, 28, -93, 32)\n&gt;&gt;&gt; disp_bounds = (-96, 29, -94, 31)\n&gt;&gt;&gt; bounds_contains(unr_bounds, disp_bounds)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # With buffer requirement\n&gt;&gt;&gt; bounds_contains(unr_bounds, disp_bounds, buffer=0.5)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Dict format\n&gt;&gt;&gt; unr_bounds = {\"west\": -97, \"south\": 28, \"east\": -93, \"north\": 32}\n&gt;&gt;&gt; disp_bounds = {\"west\": -96, \"south\": 29, \"east\": -94, \"north\": 31}\n&gt;&gt;&gt; bounds_contains(unr_bounds, disp_bounds)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Does not contain\n&gt;&gt;&gt; small_bounds = (-95, 29, -94, 30)\n&gt;&gt;&gt; large_bounds = (-96, 28, -93, 31)\n&gt;&gt;&gt; bounds_contains(small_bounds, large_bounds)\nFalse\n</code></pre> Source code in <code>src/cal_disp/product/_utils.py</code> <pre><code>def bounds_contains(\n    outer_bounds: tuple[float, float, float, float] | dict[str, float],\n    inner_bounds: tuple[float, float, float, float] | dict[str, float],\n    buffer: float = 0.0,\n) -&gt; bool:\n    \"\"\"Check if outer bounds completely contain inner bounds.\n\n    Parameters\n    ----------\n    outer_bounds : tuple or dict\n        Outer bounds as (west, south, east, north) or dict with those keys.\n    inner_bounds : tuple or dict\n        Inner bounds as (west, south, east, north) or dict with those keys.\n    buffer : float, optional\n        Buffer distance to require around inner bounds (in same units).\n        Default is 0.0 (exact containment).\n\n    Returns\n    -------\n    bool\n        True if outer bounds completely contain inner bounds (with buffer).\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Check if UNR grid covers DISP frame\n    &gt;&gt;&gt; unr_bounds = (-97, 28, -93, 32)\n    &gt;&gt;&gt; disp_bounds = (-96, 29, -94, 31)\n    &gt;&gt;&gt; bounds_contains(unr_bounds, disp_bounds)\n    True\n\n    &gt;&gt;&gt; # With buffer requirement\n    &gt;&gt;&gt; bounds_contains(unr_bounds, disp_bounds, buffer=0.5)\n    True\n\n    &gt;&gt;&gt; # Dict format\n    &gt;&gt;&gt; unr_bounds = {\"west\": -97, \"south\": 28, \"east\": -93, \"north\": 32}\n    &gt;&gt;&gt; disp_bounds = {\"west\": -96, \"south\": 29, \"east\": -94, \"north\": 31}\n    &gt;&gt;&gt; bounds_contains(unr_bounds, disp_bounds)\n    True\n\n    &gt;&gt;&gt; # Does not contain\n    &gt;&gt;&gt; small_bounds = (-95, 29, -94, 30)\n    &gt;&gt;&gt; large_bounds = (-96, 28, -93, 31)\n    &gt;&gt;&gt; bounds_contains(small_bounds, large_bounds)\n    False\n\n    \"\"\"\n    # Parse outer bounds\n    if isinstance(outer_bounds, dict):\n        o_west = outer_bounds[\"west\"]\n        o_south = outer_bounds[\"south\"]\n        o_east = outer_bounds[\"east\"]\n        o_north = outer_bounds[\"north\"]\n    else:\n        o_west, o_south, o_east, o_north = outer_bounds\n\n    # Parse inner bounds\n    if isinstance(inner_bounds, dict):\n        i_west = inner_bounds[\"west\"]\n        i_south = inner_bounds[\"south\"]\n        i_east = inner_bounds[\"east\"]\n        i_north = inner_bounds[\"north\"]\n    else:\n        i_west, i_south, i_east, i_north = inner_bounds\n\n    # Check containment with buffer\n    return (\n        o_west &lt;= i_west - buffer\n        and o_south &lt;= i_south - buffer\n        and o_east &gt;= i_east + buffer\n        and o_north &gt;= i_north + buffer\n    )\n</code></pre>"},{"location":"api/#cal_disp.product.check_bounds_coverage","title":"check_bounds_coverage","text":"<pre><code>check_bounds_coverage(outer_bounds: tuple[float, float, float, float] | dict[str, float], inner_bounds: tuple[float, float, float, float] | dict[str, float], buffer: float = 0.0) -&gt; dict[str, bool | dict[str, float]]\n</code></pre> <p>Check bounds coverage with detailed gap information.</p> <p>Parameters:</p> Name Type Description Default <code>outer_bounds</code> <code>tuple or dict</code> <p>Outer bounds as (west, south, east, north) or dict.</p> required <code>inner_bounds</code> <code>tuple or dict</code> <p>Inner bounds as (west, south, east, north) or dict.</p> required <code>buffer</code> <code>float</code> <p>Buffer distance required. Default is 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with: - \"contains\": bool, True if outer contains inner (with buffer) - \"gaps\": dict, Gap distances by direction (negative = covered)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; unr_bounds = (-97, 28, -93, 32)\n&gt;&gt;&gt; disp_bounds = (-96, 29, -94, 31)\n&gt;&gt;&gt; result = check_bounds_coverage(unr_bounds, disp_bounds, buffer=0.5)\n&gt;&gt;&gt; result[\"contains\"]\nTrue\n&gt;&gt;&gt; result[\"gaps\"]\n{'west': -0.5, 'south': -0.5, 'east': -0.5, 'north': -0.5}\n</code></pre> <pre><code>&gt;&gt;&gt; # Insufficient coverage\n&gt;&gt;&gt; small_bounds = (-95.5, 29, -94, 30)\n&gt;&gt;&gt; result = check_bounds_coverage(small_bounds, disp_bounds)\n&gt;&gt;&gt; result[\"contains\"]\nFalse\n&gt;&gt;&gt; result[\"gaps\"]\n{'west': 0.5, 'south': 0.0, 'east': 0.0, 'north': 1.0}\n</code></pre> Source code in <code>src/cal_disp/product/_utils.py</code> <pre><code>def check_bounds_coverage(\n    outer_bounds: tuple[float, float, float, float] | dict[str, float],\n    inner_bounds: tuple[float, float, float, float] | dict[str, float],\n    buffer: float = 0.0,\n) -&gt; dict[str, bool | dict[str, float]]:\n    \"\"\"Check bounds coverage with detailed gap information.\n\n    Parameters\n    ----------\n    outer_bounds : tuple or dict\n        Outer bounds as (west, south, east, north) or dict.\n    inner_bounds : tuple or dict\n        Inner bounds as (west, south, east, north) or dict.\n    buffer : float, optional\n        Buffer distance required. Default is 0.0.\n\n    Returns\n    -------\n    dict\n        Dictionary with:\n        - \"contains\": bool, True if outer contains inner (with buffer)\n        - \"gaps\": dict, Gap distances by direction (negative = covered)\n\n    Examples\n    --------\n    &gt;&gt;&gt; unr_bounds = (-97, 28, -93, 32)\n    &gt;&gt;&gt; disp_bounds = (-96, 29, -94, 31)\n    &gt;&gt;&gt; result = check_bounds_coverage(unr_bounds, disp_bounds, buffer=0.5)\n    &gt;&gt;&gt; result[\"contains\"]\n    True\n    &gt;&gt;&gt; result[\"gaps\"]\n    {'west': -0.5, 'south': -0.5, 'east': -0.5, 'north': -0.5}\n\n    &gt;&gt;&gt; # Insufficient coverage\n    &gt;&gt;&gt; small_bounds = (-95.5, 29, -94, 30)\n    &gt;&gt;&gt; result = check_bounds_coverage(small_bounds, disp_bounds)\n    &gt;&gt;&gt; result[\"contains\"]\n    False\n    &gt;&gt;&gt; result[\"gaps\"]\n    {'west': 0.5, 'south': 0.0, 'east': 0.0, 'north': 1.0}\n\n    \"\"\"\n    # Parse bounds\n    if isinstance(outer_bounds, dict):\n        o_west = outer_bounds[\"west\"]\n        o_south = outer_bounds[\"south\"]\n        o_east = outer_bounds[\"east\"]\n        o_north = outer_bounds[\"north\"]\n    else:\n        o_west, o_south, o_east, o_north = outer_bounds\n\n    if isinstance(inner_bounds, dict):\n        i_west = inner_bounds[\"west\"]\n        i_south = inner_bounds[\"south\"]\n        i_east = inner_bounds[\"east\"]\n        i_north = inner_bounds[\"north\"]\n    else:\n        i_west, i_south, i_east, i_north = inner_bounds\n\n    # Calculate gaps (positive = missing coverage, negative = extra coverage)\n    gaps = {\n        \"west\": (i_west - buffer) - o_west,\n        \"south\": (i_south - buffer) - o_south,\n        \"east\": o_east - (i_east + buffer),\n        \"north\": o_north - (i_north + buffer),\n    }\n\n    # Check if all gaps are negative or zero (fully contained)\n    contains = all(gap &lt;= 0 for gap in gaps.values())\n\n    return {\n        \"contains\": contains,\n        \"gaps\": gaps,\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.compute_los_correction","title":"compute_los_correction","text":"<pre><code>compute_los_correction(zenith_delay_2d: DataArray, los_up: DataArray, reference_correction: DataArray | None = None, target_crs: str | None = None, output_path: Path | str | None = None, output_format: str = 'geotiff') -&gt; xr.DataArray\n</code></pre> <p>Convert zenith delay to line-of-sight correction.</p> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>def compute_los_correction(\n    zenith_delay_2d: xr.DataArray,\n    los_up: xr.DataArray,\n    reference_correction: xr.DataArray | None = None,\n    target_crs: str | None = None,\n    output_path: Path | str | None = None,\n    output_format: str = \"geotiff\",\n) -&gt; xr.DataArray:\n    \"\"\"Convert zenith delay to line-of-sight correction.\"\"\"\n    # Ensure same grid\n    if los_up.shape != zenith_delay_2d.shape:\n        if hasattr(los_up, \"rio\") and hasattr(zenith_delay_2d, \"rio\"):\n            los_up = los_up.rio.reproject_match(zenith_delay_2d)\n        else:\n            raise ValueError(\n                f\"Shape mismatch: los_up {los_up.shape} vs \"\n                f\"zenith_delay {zenith_delay_2d.shape}\"\n            )\n\n    # Convert zenith to LOS\n    # Note: -1 matches DISP convention (positive = apparent uplift)\n    los_correction = -1 * (zenith_delay_2d / los_up)\n\n    # Subtract reference if provided\n    if reference_correction is not None:\n        if reference_correction.shape != los_correction.shape:\n            if hasattr(reference_correction, \"rio\"):\n                reference_correction = reference_correction.rio.reproject_match(\n                    los_correction\n                )\n\n        los_correction = (los_correction - reference_correction).astype(np.float32)\n\n    # Reproject to target CRS if requested\n    if target_crs is not None:\n        if hasattr(los_correction, \"rio\"):\n            los_correction = los_correction.rio.reproject(target_crs)\n        else:\n            raise ValueError(\"Cannot reproject: DataArray missing CRS information\")\n\n    # Add metadata\n    los_correction.name = \"los_correction\"\n    los_correction.attrs.update(\n        {\n            \"units\": \"meters\",\n            \"long_name\": \"Line-of-sight atmospheric correction\",\n            \"line_of_sight_convention\": (\n                \"Positive means decrease in delay (apparent uplift towards satellite)\"\n            ),\n        }\n    )\n\n    if reference_correction is not None:\n        los_correction.attrs[\"reference_subtracted\"] = \"yes\"\n\n    if target_crs is not None:\n        los_correction.attrs[\"target_crs\"] = target_crs\n\n    # Save if requested\n    if output_path is not None:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        if output_format == \"netcdf\":\n            los_correction.to_netcdf(output_path, engine=\"h5netcdf\")\n        elif output_format == \"geotiff\":\n            los_correction.rio.to_raster(\n                output_path,\n                compress=\"deflate\",\n                tiled=True,\n                dtype=\"float32\",\n            )\n        else:\n            raise ValueError(f\"Unknown format: {output_format}\")\n\n    return los_correction\n</code></pre>"},{"location":"api/#cal_disp.product.interpolate_in_time","title":"interpolate_in_time","text":"<pre><code>interpolate_in_time(tropo_early: TropoProduct, tropo_late: TropoProduct, target_datetime: datetime, bounds: tuple[float, float, float, float] | None = None, max_height: float = 11000.0, bounds_buffer: float = 0.2, output_path: Path | str | None = None) -&gt; xr.DataArray\n</code></pre> <p>Interpolate tropospheric delay between two products in time.</p> <p>Parameters:</p> Name Type Description Default <code>tropo_early</code> <code>TropoProduct</code> <p>Earlier tropospheric product.</p> required <code>tropo_late</code> <code>TropoProduct</code> <p>Later tropospheric product.</p> required <code>target_datetime</code> <code>datetime</code> <p>Target datetime for interpolation.</p> required <code>bounds</code> <code>tuple[float, float, float, float] or None</code> <p>Spatial bounds as (west, south, east, north). Default is None.</p> <code>None</code> <code>max_height</code> <code>float</code> <p>Maximum height in meters. Default is 11000 m.</p> <code>11000.0</code> <code>bounds_buffer</code> <code>float</code> <p>Buffer to add to bounds in degrees. Default is 0.2.</p> <code>0.2</code> <code>output_path</code> <code>Path, str, or None</code> <p>If provided, save result to NetCDF. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>Interpolated tropospheric delay at target datetime.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; from product import TropoProduct\n&gt;&gt;&gt; from product._tropo import interpolate_in_time\n&gt;&gt;&gt;\n&gt;&gt;&gt; early = TropoProduct.from_path(\"tropo_00Z.nc\")\n&gt;&gt;&gt; late = TropoProduct.from_path(\"tropo_06Z.nc\")\n&gt;&gt;&gt; target = datetime(2022, 1, 11, 3, 0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic interpolation\n&gt;&gt;&gt; delay = interpolate_in_time(early, late, target)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With spatial subsetting\n&gt;&gt;&gt; bounds = (-96, 29, -94, 31)\n&gt;&gt;&gt; delay = interpolate_in_time(\n...     early, late, target,\n...     bounds=bounds,\n...     max_height=11000,\n...     bounds_buffer=0.2,\n... )\n</code></pre> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>def interpolate_in_time(\n    tropo_early: TropoProduct,\n    tropo_late: TropoProduct,\n    target_datetime: datetime,\n    bounds: tuple[float, float, float, float] | None = None,\n    max_height: float = 11e3,\n    bounds_buffer: float = 0.2,\n    output_path: Path | str | None = None,\n) -&gt; xr.DataArray:\n    \"\"\"Interpolate tropospheric delay between two products in time.\n\n    Parameters\n    ----------\n    tropo_early : TropoProduct\n        Earlier tropospheric product.\n    tropo_late : TropoProduct\n        Later tropospheric product.\n    target_datetime : datetime\n        Target datetime for interpolation.\n    bounds : tuple[float, float, float, float] or None, optional\n        Spatial bounds as (west, south, east, north). Default is None.\n    max_height : float, optional\n        Maximum height in meters. Default is 11000 m.\n    bounds_buffer : float, optional\n        Buffer to add to bounds in degrees. Default is 0.2.\n    output_path : Path, str, or None, optional\n        If provided, save result to NetCDF. Default is None.\n\n    Returns\n    -------\n    xr.DataArray\n        Interpolated tropospheric delay at target datetime.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from datetime import datetime\n    &gt;&gt;&gt; from product import TropoProduct\n    &gt;&gt;&gt; from product._tropo import interpolate_in_time\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; early = TropoProduct.from_path(\"tropo_00Z.nc\")\n    &gt;&gt;&gt; late = TropoProduct.from_path(\"tropo_06Z.nc\")\n    &gt;&gt;&gt; target = datetime(2022, 1, 11, 3, 0)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Basic interpolation\n    &gt;&gt;&gt; delay = interpolate_in_time(early, late, target)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # With spatial subsetting\n    &gt;&gt;&gt; bounds = (-96, 29, -94, 31)\n    &gt;&gt;&gt; delay = interpolate_in_time(\n    ...     early, late, target,\n    ...     bounds=bounds,\n    ...     max_height=11000,\n    ...     bounds_buffer=0.2,\n    ... )\n\n    \"\"\"\n    if tropo_early.date &gt; tropo_late.date:\n        raise ValueError(\n            f\"Early product date ({tropo_early.date}) must be before \"\n            f\"late product date ({tropo_late.date})\"\n        )\n\n    if target_datetime &lt; tropo_early.date or target_datetime &gt; tropo_late.date:\n        raise ValueError(\n            f\"Target datetime ({target_datetime}) must be between \"\n            f\"early ({tropo_early.date}) and late ({tropo_late.date}) dates\"\n        )\n\n    # Get total delay with consistent kwargs\n    da_early = tropo_early.get_total_delay(\n        bounds=bounds,\n        max_height=max_height,\n        bounds_buffer=bounds_buffer,\n    )\n\n    da_late = tropo_late.get_total_delay(\n        bounds=bounds,\n        max_height=max_height,\n        bounds_buffer=bounds_buffer,\n    )\n\n    # Compute interpolation weight\n    delta_total = (tropo_late.date - tropo_early.date).total_seconds()\n    delta_target = (target_datetime - tropo_early.date).total_seconds()\n    weight = delta_target / delta_total\n\n    # Linear interpolation\n    da_interp = (1 - weight) * da_early + weight * da_late\n\n    # Add metadata\n    da_interp.name = \"zenith_total_delay\"\n    da_interp.attrs.update(\n        {\n            \"interpolation_method\": \"linear\",\n            \"early_product\": tropo_early.filename,\n            \"late_product\": tropo_late.filename,\n            \"early_date\": tropo_early.date.isoformat(),\n            \"late_date\": tropo_late.date.isoformat(),\n            \"target_date\": target_datetime.isoformat(),\n            \"interpolation_weight\": float(weight),\n            \"long_name\": \"Total zenith tropospheric delay\",\n            \"units\": \"meters\",\n        }\n    )\n\n    # Save if requested\n    if output_path is not None:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        da_interp.to_netcdf(output_path, engine=\"h5netcdf\")\n\n    return da_interp\n</code></pre>"},{"location":"api/#cal_disp.product.interpolate_to_dem_surface","title":"interpolate_to_dem_surface","text":"<pre><code>interpolate_to_dem_surface(da_tropo_cube: DataArray, dem: DataArray, method: str = 'linear', output_path: Path | str | None = None, output_format: str = 'netcdf') -&gt; xr.DataArray\n</code></pre> <p>Interpolate 3D tropospheric delay to DEM surface heights.</p> <p>Parameters:</p> Name Type Description Default <code>da_tropo_cube</code> <code>DataArray</code> <p>3D tropospheric delay with dims (height, y, x). Assumed to be in EPSG:4326 (WGS84) if CRS not specified.</p> required <code>dem</code> <code>DataArray</code> <p>DEM with surface heights in meters. Must have CRS information.</p> required <code>method</code> <code>str</code> <p>Interpolation method (\"linear\" or \"nearest\"). Default is \"linear\".</p> <code>'linear'</code> <code>output_path</code> <code>Path, str, or None</code> <p>If provided, save result. Default is None.</p> <code>None</code> <code>output_format</code> <code>str</code> <p>Output format (\"netcdf\" or \"geotiff\"). Default is \"netcdf\".</p> <code>'netcdf'</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>2D tropospheric delay at DEM surface.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If DEM is missing CRS information.</p> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>def interpolate_to_dem_surface(\n    da_tropo_cube: xr.DataArray,\n    dem: xr.DataArray,\n    method: str = \"linear\",\n    output_path: Path | str | None = None,\n    output_format: str = \"netcdf\",\n) -&gt; xr.DataArray:\n    \"\"\"Interpolate 3D tropospheric delay to DEM surface heights.\n\n    Parameters\n    ----------\n    da_tropo_cube : xr.DataArray\n        3D tropospheric delay with dims (height, y, x).\n        Assumed to be in EPSG:4326 (WGS84) if CRS not specified.\n    dem : xr.DataArray\n        DEM with surface heights in meters. Must have CRS information.\n    method : str, optional\n        Interpolation method (\"linear\" or \"nearest\"). Default is \"linear\".\n    output_path : Path, str, or None, optional\n        If provided, save result. Default is None.\n    output_format : str, optional\n        Output format (\"netcdf\" or \"geotiff\"). Default is \"netcdf\".\n\n    Returns\n    -------\n    xr.DataArray\n        2D tropospheric delay at DEM surface.\n\n    Raises\n    ------\n    ValueError\n        If DEM is missing CRS information.\n\n    \"\"\"\n    # Ensure consistent coordinate naming\n    if \"latitude\" in da_tropo_cube.dims:\n        da_tropo_cube = da_tropo_cube.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n\n    # Check DEM has CRS\n    if not hasattr(dem, \"rio\") or dem.rio.crs is None:\n        raise ValueError(\n            \"DEM is missing CRS information. \"\n            \"Use dem.rio.write_crs() to set the CRS before calling this function.\"\n        )\n\n    dem_crs = dem.rio.crs\n\n    # Write CRS to tropo if missing (assume EPSG:4326)\n    if not hasattr(da_tropo_cube, \"rio\") or da_tropo_cube.rio.crs is None:\n        da_tropo_cube = da_tropo_cube.rio.write_crs(\"EPSG:4326\")\n\n    # Reproject if different CRS\n    if da_tropo_cube.rio.crs != dem_crs:\n        td_utm = da_tropo_cube.rio.reproject(\n            dem_crs,\n            resampling=Resampling.cubic,\n        )\n    else:\n        td_utm = da_tropo_cube\n\n    # Find height dimension\n    if \"height\" not in td_utm.dims:\n        raise ValueError(\n            f\"No height dimension found. Available dims: {list(td_utm.dims)}\"\n        )\n\n    # Build interpolator\n    rgi = RegularGridInterpolator(\n        (td_utm[\"height\"].values, td_utm.y.values, td_utm.x.values),\n        np.nan_to_num(td_utm.values),\n        method=method,\n        bounds_error=False,\n        fill_value=np.nan,\n    )\n\n    # Create coordinate meshgrid\n    yy, xx = np.meshgrid(dem.y.values, dem.x.values, indexing=\"ij\")\n    pts = np.column_stack([dem.values.ravel(), yy.ravel(), xx.ravel()])\n\n    # Interpolate\n    vals = rgi(pts)\n\n    # Create output DataArray\n    out = dem.copy()\n    out.values[:] = vals.reshape(dem.shape).astype(np.float32)\n    out.name = da_tropo_cube.name or \"tropospheric_delay\"\n\n    # Update attributes\n    out.attrs.update(\n        {\n            \"interpolation_method\": method,\n            \"interpolated_from\": \"3D tropospheric model\",\n            \"units\": \"meters\",\n            \"long_name\": \"Tropospheric delay at DEM surface\",\n        }\n    )\n\n    # Add time if present in input\n    if \"time\" in td_utm.coords:\n        out.attrs[\"time\"] = str(td_utm.time.values)\n\n    # Preserve any existing tropo metadata\n    if \"target_date\" in da_tropo_cube.attrs:\n        out.attrs[\"target_date\"] = da_tropo_cube.attrs[\"target_date\"]\n    if \"interpolation_weight\" in da_tropo_cube.attrs:\n        out.attrs[\"interpolation_weight\"] = da_tropo_cube.attrs[\"interpolation_weight\"]\n\n    # Save if requested\n    if output_path is not None:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        if output_format == \"netcdf\":\n            out.to_netcdf(output_path, engine=\"h5netcdf\")\n        elif output_format == \"geotiff\":\n            out.rio.to_raster(\n                output_path,\n                compress=\"deflate\",\n                tiled=True,\n                dtype=\"float32\",\n            )\n        else:\n            raise ValueError(f\"Unknown format: {output_format}\")\n\n    return out\n</code></pre>"},{"location":"api/#config","title":"Config","text":""},{"location":"api/#cal_disp.config","title":"cal_disp.config","text":""},{"location":"api/#cal_disp.config.DynamicAncillaryFileGroup","title":"DynamicAncillaryFileGroup","text":"<p>               Bases: <code>YamlModel</code></p> <p>Dynamic ancillary files for the SAS.</p> <p>Attributes:</p> Name Type Description <code>algorithm_parameters_file</code> <code>Path</code> <p>Path to file containing SAS algorithm parameters.</p> <code>geometry_file</code> <code>Path</code> <p>Path to the DISP static_layer file with line-of-sight unit vectors.</p> <code>mask_file</code> <code>Optional[Path]</code> <p>Optional byte mask file to ignore low correlation/bad data.</p> <code>troposphere_files</code> <code>List[Path]</code> <p>Paths to TROPO files for atmospheric correction.</p> <code>ionosphere_files</code> <code>List[Path]</code> <p>Paths to IONO files for ionospheric correction.</p> <code>tiles_files</code> <code>Optional[List[Path]]</code> <p>Paths to calibration tile bounds files.</p> Source code in <code>src/cal_disp/config/_base.py</code> <pre><code>class DynamicAncillaryFileGroup(YamlModel):\n    \"\"\"Dynamic ancillary files for the SAS.\n\n    Attributes\n    ----------\n    algorithm_parameters_file : Path\n        Path to file containing SAS algorithm parameters.\n    geometry_file : Path\n        Path to the DISP static_layer file with line-of-sight unit vectors.\n    mask_file : Optional[Path]\n        Optional byte mask file to ignore low correlation/bad data.\n    troposphere_files : List[Path]\n        Paths to TROPO files for atmospheric correction.\n    ionosphere_files : List[Path]\n        Paths to IONO files for ionospheric correction.\n    tiles_files : Optional[List[Path]]\n        Paths to calibration tile bounds files.\n\n    \"\"\"\n\n    algorithm_parameters_file: RequiredPath = Field(\n        ...,\n        description=\"Path to file containing SAS algorithm parameters.\",\n    )\n\n    los_file: RequiredPath = Field(\n        ...,\n        alias=\"static_los_file\",\n        description=(\n            \"Path to the DISP static los layer file (1 per frame) with line-of-sight\"\n            \" unit vectors.\"\n        ),\n    )\n\n    dem_file: RequiredPath = Field(\n        ...,\n        alias=\"static_dem_file\",\n        description=(\n            \"Path to the DISP static dem layer file (1 per frame) with line-of-sight\"\n            \" unit vectors.\"\n        ),\n    )\n    # NOTE should I add also shadow_layover static file as input\n\n    mask_file: OptionalPath = Field(\n        default=None,\n        description=(\n            \"Optional Byte mask file used to ignore low correlation/bad data (e.g water\"\n            \" mask). Convention is 0 for no data/invalid, and 1 for good data. Dtype\"\n            \" must be uint8.\"\n        ),\n    )\n\n    reference_tropo_files: Optional[List[Path]] = Field(\n        default=None,\n        alias=\"ref_tropo_files\",\n        description=(\n            \"Path to the TROPO file for the reference date.\"\n            \" If not provided, tropospheric correction for reference is skipped.\"\n        ),\n    )\n\n    secondary_tropo_files: Optional[List[Path]] = Field(\n        default=None,\n        alias=\"sec_tropo_files\",\n        description=(\n            \"Path to the TROPO file for the secondary date.\"\n            \" If not provided, tropospheric correction for secondary is skipped.\"\n        ),\n    )\n\n    iono_files: Optional[List[Path]] = Field(\n        default=None,\n        alias=\"iono_files\",\n        description=(\n            \"Path to the IONO files\"\n            \" If not provided, ionosphere correction for reference is skipped.\"\n        ),\n    )\n\n    tiles_files: Optional[List[Path]] = Field(\n        default=None,\n        description=(\n            \"Paths to the calibration tile bounds files (e.g. S1 burst bounds) covering\"\n            \" full frame. If none provided, calibration per tile is skipped.\"\n        ),\n    )\n\n    @field_validator(\n        \"reference_tropo_files\",\n        \"secondary_tropo_files\",\n        \"iono_files\",\n        \"tiles_files\",\n        mode=\"before\",\n    )\n    @classmethod\n    def _validate_file_lists(cls, v):\n        \"\"\"Validate and process file lists or glob patterns.\"\"\"\n        return _read_file_list_or_glob(cls, v)\n\n    def get_all_files(self) -&gt; Dict[str, Path | list[Path]]:\n        return self.get_all_file_paths(flatten_lists=True)\n\n    model_config = STRICT_CONFIG_WITH_ALIASES\n</code></pre>"},{"location":"api/#cal_disp.config.InputFileGroup","title":"InputFileGroup","text":"<p>               Bases: <code>YamlModel</code></p> <p>Input file group for the SAS.</p> <p>Attributes:</p> Name Type Description <code>disp_file</code> <code>Path</code> <p>Path to DISP file.</p> <code>calibration_reference_grid</code> <code>Path</code> <p>Path to UNR calibration reference file (parquet format).</p> Source code in <code>src/cal_disp/config/_base.py</code> <pre><code>class InputFileGroup(YamlModel):\n    \"\"\"Input file group for the SAS.\n\n    Attributes\n    ----------\n    disp_file : Path\n        Path to DISP file.\n    calibration_reference_grid : Path\n        Path to UNR calibration reference file (parquet format).\n\n    \"\"\"\n\n    disp_file: RequiredPath = Field(\n        ...,\n        description=\"Path to DISP file.\",\n    )\n\n    calibration_reference_grid: RequiredPath = Field(\n        ...,\n        description=\"Path to UNR calibration reference file [parquet].\",\n    )\n\n    frame_id: int = Field(\n        ...,\n        description=\"Frame ID of the DISP frame.\",\n    )\n\n    model_config = ConfigDict(\n        extra=\"forbid\",\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.StaticAncillaryFileGroup","title":"StaticAncillaryFileGroup","text":"<p>               Bases: <code>YamlModel</code></p> <p>Static ancillary files for the SAS.</p> <p>These files contain configuration and reference data that don't change between processing runs for a given frame.</p> <p>Attributes:</p> Name Type Description <code>algorithm_parameters_overrides_json</code> <code>Optional[Path]</code> <p>JSON file with frame-specific algorithm parameter overrides.</p> <code>deformation_area_database_json</code> <code>Optional[Path]</code> <p>GeoJSON file with deforming areas to exclude from calibration.</p> <code>event_database_json</code> <code>Optional[Path]</code> <p>GeoJSON file with earthquake/volcanic activity events for each frame.</p> Source code in <code>src/cal_disp/config/_base.py</code> <pre><code>class StaticAncillaryFileGroup(YamlModel):\n    \"\"\"Static ancillary files for the SAS.\n\n    These files contain configuration and reference data that don't change\n    between processing runs for a given frame.\n\n    Attributes\n    ----------\n    algorithm_parameters_overrides_json : Optional[Path]\n        JSON file with frame-specific algorithm parameter overrides.\n    deformation_area_database_json : Optional[Path]\n        GeoJSON file with deforming areas to exclude from calibration.\n    event_database_json : Optional[Path]\n        GeoJSON file with earthquake/volcanic activity events for each frame.\n\n    \"\"\"\n\n    algorithm_parameters_overrides_json: OptionalPath = Field(\n        default=None,\n        description=(\n            \"JSON file containing frame-specific algorithm parameters to override the\"\n            \" defaults passed in the `algorithm_parameters.yaml`.\"\n        ),\n    )\n\n    deformation_area_database_json: OptionalPath = Field(\n        default=None,\n        alias=\"defo_area_db_json\",\n        description=(\n            \"GeoJSON file containing list of deforming areas to exclude from\"\n            \" calibration (e.g. Central Valley subsidence).\"\n        ),\n    )\n\n    event_database_json: OptionalPath = Field(\n        default=None,\n        alias=\"event_db_json\",\n        description=(\n            \"GeoJSON file containing list of events (earthquakes, volcanic activity)\"\n            \" for each frame.\"\n        ),\n    )\n\n    def has_algorithm_overrides(self) -&gt; bool:\n        \"\"\"Check if algorithm parameter overrides are provided.\"\"\"\n        return self.algorithm_parameters_overrides_json is not None\n\n    def has_deformation_database(self) -&gt; bool:\n        \"\"\"Check if deformation area database is provided.\"\"\"\n        return self.deformation_area_database_json is not None\n\n    def has_event_database(self) -&gt; bool:\n        \"\"\"Check if event database is provided.\"\"\"\n        return self.event_database_json is not None\n\n    def get_all_files(self) -&gt; Dict[str, Path | list[Path]]:\n        return self.get_all_file_paths(flatten_lists=True)\n\n    model_config = STRICT_CONFIG_WITH_ALIASES\n</code></pre>"},{"location":"api/#cal_disp.config.StaticAncillaryFileGroup.has_algorithm_overrides","title":"has_algorithm_overrides","text":"<pre><code>has_algorithm_overrides() -&gt; bool\n</code></pre> <p>Check if algorithm parameter overrides are provided.</p> Source code in <code>src/cal_disp/config/_base.py</code> <pre><code>def has_algorithm_overrides(self) -&gt; bool:\n    \"\"\"Check if algorithm parameter overrides are provided.\"\"\"\n    return self.algorithm_parameters_overrides_json is not None\n</code></pre>"},{"location":"api/#cal_disp.config.StaticAncillaryFileGroup.has_deformation_database","title":"has_deformation_database","text":"<pre><code>has_deformation_database() -&gt; bool\n</code></pre> <p>Check if deformation area database is provided.</p> Source code in <code>src/cal_disp/config/_base.py</code> <pre><code>def has_deformation_database(self) -&gt; bool:\n    \"\"\"Check if deformation area database is provided.\"\"\"\n    return self.deformation_area_database_json is not None\n</code></pre>"},{"location":"api/#cal_disp.config.StaticAncillaryFileGroup.has_event_database","title":"has_event_database","text":"<pre><code>has_event_database() -&gt; bool\n</code></pre> <p>Check if event database is provided.</p> Source code in <code>src/cal_disp/config/_base.py</code> <pre><code>def has_event_database(self) -&gt; bool:\n    \"\"\"Check if event database is provided.\"\"\"\n    return self.event_database_json is not None\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings","title":"WorkerSettings","text":"<p>               Bases: <code>YamlModel</code></p> <p>Settings for controlling CPU/threading and parallelism.</p> <p>This class configures Dask distributed computing settings including worker count, threads per worker, and data block sizes for chunked processing.</p> <p>Attributes:</p> Name Type Description <code>n_workers</code> <code>int</code> <p>Number of Dask workers to spawn. Default is 4.</p> <code>threads_per_worker</code> <code>int</code> <p>Number of threads per worker. Default is 2.</p> <code>block_shape</code> <code>Tuple[int, int]</code> <p>Block size (rows, columns) for chunked data loading.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Default settings\n&gt;&gt;&gt; settings = WorkerSettings()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Custom configuration\n&gt;&gt;&gt; settings = WorkerSettings(\n...     n_workers=8,\n...     threads_per_worker=4,\n...     block_shape=(256, 256)\n... )\n&gt;&gt;&gt; print(f\"Total threads: {settings.total_threads}\")\n</code></pre> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>class WorkerSettings(YamlModel):\n    \"\"\"Settings for controlling CPU/threading and parallelism.\n\n    This class configures Dask distributed computing settings including\n    worker count, threads per worker, and data block sizes for chunked\n    processing.\n\n    Attributes\n    ----------\n    n_workers : int\n        Number of Dask workers to spawn. Default is 4.\n    threads_per_worker : int\n        Number of threads per worker. Default is 2.\n    block_shape : Tuple[int, int]\n        Block size (rows, columns) for chunked data loading.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Default settings\n    &gt;&gt;&gt; settings = WorkerSettings()\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Custom configuration\n    &gt;&gt;&gt; settings = WorkerSettings(\n    ...     n_workers=8,\n    ...     threads_per_worker=4,\n    ...     block_shape=(256, 256)\n    ... )\n    &gt;&gt;&gt; print(f\"Total threads: {settings.total_threads}\")\n\n    \"\"\"\n\n    n_workers: int = Field(\n        default=4,\n        ge=1,\n        le=256,  # Reasonable upper limit\n        description=(\n            \"Number of workers to use in dask.Client. Typically set to the number \"\n            \"of physical CPU cores or less. Default: 4\"\n        ),\n    )\n\n    threads_per_worker: int = Field(\n        default=2,\n        ge=1,\n        le=32,  # Reasonable upper limit\n        description=(\n            \"Number of threads per worker in dask.Client. Controls parallelism \"\n            \"within each worker. Default: 2\"\n        ),\n    )\n\n    block_shape: Tuple[int, int] = Field(\n        default=(128, 128),\n        description=(\n            \"Size (rows, columns) of blocks of data to load at a time. \"\n            \"Larger blocks use more memory but may be more efficient. \"\n            \"Must be positive integers. Default: (128, 128)\"\n        ),\n    )\n\n    @field_validator(\"block_shape\", mode=\"before\")\n    @classmethod\n    def _validate_block_shape(cls, v) -&gt; Tuple[int, int]:\n        \"\"\"Validate block shape has positive dimensions.\n\n        Parameters\n        ----------\n        v : tuple | list\n            Block shape to validate.\n\n        Returns\n        -------\n        Tuple[int, int]\n            Validated block shape.\n\n        Raises\n        ------\n        ValueError\n            If block dimensions are not positive or not exactly 2 values.\n\n        \"\"\"\n        # Convert to tuple if list\n        if isinstance(v, list):\n            v = tuple(v)\n\n        # Check it's a tuple/list with 2 elements\n        if not isinstance(v, tuple) or len(v) != 2:\n            raise ValueError(\n                f\"block_shape must have exactly 2 dimensions (rows, cols), got {v}\"\n            )\n\n        # Check both are integers\n        if not all(isinstance(x, int) for x in v):\n            raise ValueError(f\"block_shape dimensions must be integers, got {v}\")\n\n        # Check both are positive\n        if not all(x &gt; 0 for x in v):\n            raise ValueError(f\"block_shape dimensions must be positive, got {v}\")\n\n        return v\n\n    # CHANGED: Use @property instead of @computed_field to avoid mypy issues\n    @property\n    def total_threads(self) -&gt; int:\n        \"\"\"Total number of threads across all workers.\n\n        Returns\n        -------\n        int\n            n_workers * threads_per_worker\n\n        \"\"\"\n        return self.n_workers * self.threads_per_worker\n\n    @property\n    def block_size(self) -&gt; int:\n        \"\"\"Total number of elements per block.\n\n        Returns\n        -------\n        int\n            Product of block_shape dimensions (rows * columns).\n\n        \"\"\"\n        return self.block_shape[0] * self.block_shape[1]\n\n    def estimate_memory_per_block(self, dtype_size: int = 8, n_bands: int = 1) -&gt; float:\n        \"\"\"Estimate memory usage per block in MB.\n\n        Parameters\n        ----------\n        dtype_size : int, default=8\n            Size of data type in bytes (e.g., 8 for float64, 4 for float32).\n        n_bands : int, default=1\n            Number of bands/layers in the data.\n\n        Returns\n        -------\n        float\n            Estimated memory in megabytes.\n\n        Examples\n        --------\n        &gt;&gt;&gt; settings = WorkerSettings(block_shape=(256, 256))\n        &gt;&gt;&gt; mem_mb = settings.estimate_memory_per_block(dtype_size=8, n_bands=2)\n        &gt;&gt;&gt; print(f\"Estimated memory: {mem_mb:.2f} MB\")\n\n        \"\"\"\n        bytes_per_block = self.block_size * dtype_size * n_bands\n        return bytes_per_block / (1024 * 1024)  # Convert to MB\n\n    def estimate_total_memory(\n        self, dtype_size: int = 8, n_bands: int = 1, overhead_factor: float = 1.5\n    ) -&gt; float:\n        \"\"\"Estimate total memory usage across all workers in GB.\n\n        Parameters\n        ----------\n        dtype_size : int, default=8\n            Size of data type in bytes.\n        n_bands : int, default=1\n            Number of bands/layers in the data.\n        overhead_factor : float, default=1.5\n            Multiplier for overhead (copies, intermediate results).\n\n        Returns\n        -------\n        float\n            Estimated total memory in gigabytes.\n\n        \"\"\"\n        mb_per_block = self.estimate_memory_per_block(dtype_size, n_bands)\n        # Assume each worker might hold multiple blocks\n        total_mb = mb_per_block * self.n_workers * overhead_factor\n        return total_mb / 1024  # Convert to GB\n\n    def summary(self) -&gt; str:\n        \"\"\"Generate a human-readable summary of settings.\n\n        Returns\n        -------\n        str\n            Multi-line summary string.\n\n        \"\"\"\n        lines = [\n            \"WorkerSettings Configuration:\",\n            \"=\" * 50,\n            f\"Workers:              {self.n_workers}\",\n            f\"Threads per worker:   {self.threads_per_worker}\",\n            f\"Total threads:        {self.total_threads}\",\n            f\"Block shape:          {self.block_shape[0]} x {self.block_shape[1]}\",\n            f\"Block size:           {self.block_size:,} elements\",\n            f\"Estimated mem/block:  {self.estimate_memory_per_block():.2f} MB\",\n            f\"Estimated total mem:  {self.estimate_total_memory():.2f} GB\",\n            \"=\" * 50,\n        ]\n        return \"\\n\".join(lines)\n\n    @classmethod\n    def create_lightweight(cls) -&gt; \"WorkerSettings\":\n        \"\"\"Create lightweight settings for small datasets or testing.\n\n        Returns\n        -------\n        WorkerSettings\n            Configuration with 2 workers, 1 thread each, small blocks.\n\n        \"\"\"\n        return cls(n_workers=2, threads_per_worker=1, block_shape=(64, 64))\n\n    @classmethod\n    def create_standard(cls) -&gt; \"WorkerSettings\":\n        \"\"\"Create standard settings for typical workloads.\n\n        Returns\n        -------\n        WorkerSettings\n            Configuration with 4 workers, 2 threads each, medium blocks.\n\n        \"\"\"\n        return cls(n_workers=4, threads_per_worker=2, block_shape=(128, 128))\n\n    @classmethod\n    def create_heavy(cls) -&gt; \"WorkerSettings\":\n        \"\"\"Create heavy-duty settings for large datasets.\n\n        Returns\n        -------\n        WorkerSettings\n            Configuration with 8 workers, 4 threads each, large blocks.\n\n        \"\"\"\n        return cls(n_workers=8, threads_per_worker=4, block_shape=(256, 256))\n\n    @classmethod\n    def create_from_cpu_count(\n        cls,\n        use_fraction: float = 0.75,\n        threads_per_worker: int = 2,\n        block_shape: Tuple[int, int] = (128, 128),\n    ) -&gt; \"WorkerSettings\":\n        \"\"\"Create settings based on available CPU count.\n\n        Parameters\n        ----------\n        use_fraction : float, default=0.75\n            Fraction of available CPUs to use (0.0 to 1.0).\n        threads_per_worker : int, default=2\n            Threads per worker.\n        block_shape : Tuple[int, int], default=(128, 128)\n            Block shape for data loading.\n\n        Returns\n        -------\n        WorkerSettings\n            Configuration tuned to system CPU count.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Use 75% of available CPUs\n        &gt;&gt;&gt; settings = WorkerSettings.create_from_cpu_count(use_fraction=0.75)\n\n        \"\"\"\n        cpu_count = multiprocessing.cpu_count()\n        n_workers = max(1, int(cpu_count * use_fraction / threads_per_worker))\n\n        return cls(\n            n_workers=n_workers,\n            threads_per_worker=threads_per_worker,\n            block_shape=block_shape,\n        )\n\n    def validate_against_system(self) -&gt; Dict[str, Any]:\n        \"\"\"Validate settings against system resources.\n\n        Returns\n        -------\n        dict\n            Dictionary with validation results and warnings.\n\n        Examples\n        --------\n        &gt;&gt;&gt; settings = WorkerSettings(n_workers=16, threads_per_worker=8)\n        &gt;&gt;&gt; validation = settings.validate_against_system()\n        &gt;&gt;&gt; if validation['warnings']:\n        ...     for warning in validation['warnings']:\n        ...         print(f\"Warning: {warning}\")\n\n        \"\"\"\n        cpu_count = multiprocessing.cpu_count()\n        warnings = []\n\n        # Check if total threads exceed CPU count\n        if self.total_threads &gt; cpu_count:\n            warnings.append(\n                f\"Total threads ({self.total_threads}) exceeds available \"\n                f\"CPU cores ({cpu_count}). May cause oversubscription.\"\n            )\n\n        # Check if block size is very large\n        if self.block_size &gt; 1_000_000:\n            warnings.append(\n                f\"Block size ({self.block_size:,}) is very large. \"\n                \"May cause high memory usage.\"\n            )\n\n        # Check if block size is very small\n        if self.block_size &lt; 1_000:\n            warnings.append(\n                f\"Block size ({self.block_size:,}) is very small. \"\n                \"May cause poor performance due to overhead.\"\n            )\n\n        return {\n            \"valid\": len(warnings) == 0,\n            \"warnings\": warnings,\n            \"system_cpu_count\": cpu_count,\n            \"configured_total_threads\": self.total_threads,\n            \"cpu_utilization\": self.total_threads / cpu_count if cpu_count &gt; 0 else 0,\n        }\n\n    model_config = ConfigDict(\n        extra=\"forbid\",\n        validate_assignment=True,\n        json_schema_extra={\n            \"examples\": [\n                {\"n_workers\": 4, \"threads_per_worker\": 2, \"block_shape\": [128, 128]},\n                {\"n_workers\": 8, \"threads_per_worker\": 4, \"block_shape\": [256, 256]},\n            ]\n        },\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.block_size","title":"block_size  <code>property</code>","text":"<pre><code>block_size: int\n</code></pre> <p>Total number of elements per block.</p> <p>Returns:</p> Type Description <code>int</code> <p>Product of block_shape dimensions (rows * columns).</p>"},{"location":"api/#cal_disp.config.WorkerSettings.total_threads","title":"total_threads  <code>property</code>","text":"<pre><code>total_threads: int\n</code></pre> <p>Total number of threads across all workers.</p> <p>Returns:</p> Type Description <code>int</code> <p>n_workers * threads_per_worker</p>"},{"location":"api/#cal_disp.config.WorkerSettings.create_from_cpu_count","title":"create_from_cpu_count  <code>classmethod</code>","text":"<pre><code>create_from_cpu_count(use_fraction: float = 0.75, threads_per_worker: int = 2, block_shape: Tuple[int, int] = (128, 128)) -&gt; WorkerSettings\n</code></pre> <p>Create settings based on available CPU count.</p> <p>Parameters:</p> Name Type Description Default <code>use_fraction</code> <code>float</code> <p>Fraction of available CPUs to use (0.0 to 1.0).</p> <code>0.75</code> <code>threads_per_worker</code> <code>int</code> <p>Threads per worker.</p> <code>2</code> <code>block_shape</code> <code>Tuple[int, int]</code> <p>Block shape for data loading.</p> <code>(128, 128)</code> <p>Returns:</p> Type Description <code>WorkerSettings</code> <p>Configuration tuned to system CPU count.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Use 75% of available CPUs\n&gt;&gt;&gt; settings = WorkerSettings.create_from_cpu_count(use_fraction=0.75)\n</code></pre> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>@classmethod\ndef create_from_cpu_count(\n    cls,\n    use_fraction: float = 0.75,\n    threads_per_worker: int = 2,\n    block_shape: Tuple[int, int] = (128, 128),\n) -&gt; \"WorkerSettings\":\n    \"\"\"Create settings based on available CPU count.\n\n    Parameters\n    ----------\n    use_fraction : float, default=0.75\n        Fraction of available CPUs to use (0.0 to 1.0).\n    threads_per_worker : int, default=2\n        Threads per worker.\n    block_shape : Tuple[int, int], default=(128, 128)\n        Block shape for data loading.\n\n    Returns\n    -------\n    WorkerSettings\n        Configuration tuned to system CPU count.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Use 75% of available CPUs\n    &gt;&gt;&gt; settings = WorkerSettings.create_from_cpu_count(use_fraction=0.75)\n\n    \"\"\"\n    cpu_count = multiprocessing.cpu_count()\n    n_workers = max(1, int(cpu_count * use_fraction / threads_per_worker))\n\n    return cls(\n        n_workers=n_workers,\n        threads_per_worker=threads_per_worker,\n        block_shape=block_shape,\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.create_heavy","title":"create_heavy  <code>classmethod</code>","text":"<pre><code>create_heavy() -&gt; WorkerSettings\n</code></pre> <p>Create heavy-duty settings for large datasets.</p> <p>Returns:</p> Type Description <code>WorkerSettings</code> <p>Configuration with 8 workers, 4 threads each, large blocks.</p> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>@classmethod\ndef create_heavy(cls) -&gt; \"WorkerSettings\":\n    \"\"\"Create heavy-duty settings for large datasets.\n\n    Returns\n    -------\n    WorkerSettings\n        Configuration with 8 workers, 4 threads each, large blocks.\n\n    \"\"\"\n    return cls(n_workers=8, threads_per_worker=4, block_shape=(256, 256))\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.create_lightweight","title":"create_lightweight  <code>classmethod</code>","text":"<pre><code>create_lightweight() -&gt; WorkerSettings\n</code></pre> <p>Create lightweight settings for small datasets or testing.</p> <p>Returns:</p> Type Description <code>WorkerSettings</code> <p>Configuration with 2 workers, 1 thread each, small blocks.</p> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>@classmethod\ndef create_lightweight(cls) -&gt; \"WorkerSettings\":\n    \"\"\"Create lightweight settings for small datasets or testing.\n\n    Returns\n    -------\n    WorkerSettings\n        Configuration with 2 workers, 1 thread each, small blocks.\n\n    \"\"\"\n    return cls(n_workers=2, threads_per_worker=1, block_shape=(64, 64))\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.create_standard","title":"create_standard  <code>classmethod</code>","text":"<pre><code>create_standard() -&gt; WorkerSettings\n</code></pre> <p>Create standard settings for typical workloads.</p> <p>Returns:</p> Type Description <code>WorkerSettings</code> <p>Configuration with 4 workers, 2 threads each, medium blocks.</p> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>@classmethod\ndef create_standard(cls) -&gt; \"WorkerSettings\":\n    \"\"\"Create standard settings for typical workloads.\n\n    Returns\n    -------\n    WorkerSettings\n        Configuration with 4 workers, 2 threads each, medium blocks.\n\n    \"\"\"\n    return cls(n_workers=4, threads_per_worker=2, block_shape=(128, 128))\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.estimate_memory_per_block","title":"estimate_memory_per_block","text":"<pre><code>estimate_memory_per_block(dtype_size: int = 8, n_bands: int = 1) -&gt; float\n</code></pre> <p>Estimate memory usage per block in MB.</p> <p>Parameters:</p> Name Type Description Default <code>dtype_size</code> <code>int</code> <p>Size of data type in bytes (e.g., 8 for float64, 4 for float32).</p> <code>8</code> <code>n_bands</code> <code>int</code> <p>Number of bands/layers in the data.</p> <code>1</code> <p>Returns:</p> Type Description <code>float</code> <p>Estimated memory in megabytes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; settings = WorkerSettings(block_shape=(256, 256))\n&gt;&gt;&gt; mem_mb = settings.estimate_memory_per_block(dtype_size=8, n_bands=2)\n&gt;&gt;&gt; print(f\"Estimated memory: {mem_mb:.2f} MB\")\n</code></pre> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>def estimate_memory_per_block(self, dtype_size: int = 8, n_bands: int = 1) -&gt; float:\n    \"\"\"Estimate memory usage per block in MB.\n\n    Parameters\n    ----------\n    dtype_size : int, default=8\n        Size of data type in bytes (e.g., 8 for float64, 4 for float32).\n    n_bands : int, default=1\n        Number of bands/layers in the data.\n\n    Returns\n    -------\n    float\n        Estimated memory in megabytes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; settings = WorkerSettings(block_shape=(256, 256))\n    &gt;&gt;&gt; mem_mb = settings.estimate_memory_per_block(dtype_size=8, n_bands=2)\n    &gt;&gt;&gt; print(f\"Estimated memory: {mem_mb:.2f} MB\")\n\n    \"\"\"\n    bytes_per_block = self.block_size * dtype_size * n_bands\n    return bytes_per_block / (1024 * 1024)  # Convert to MB\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.estimate_total_memory","title":"estimate_total_memory","text":"<pre><code>estimate_total_memory(dtype_size: int = 8, n_bands: int = 1, overhead_factor: float = 1.5) -&gt; float\n</code></pre> <p>Estimate total memory usage across all workers in GB.</p> <p>Parameters:</p> Name Type Description Default <code>dtype_size</code> <code>int</code> <p>Size of data type in bytes.</p> <code>8</code> <code>n_bands</code> <code>int</code> <p>Number of bands/layers in the data.</p> <code>1</code> <code>overhead_factor</code> <code>float</code> <p>Multiplier for overhead (copies, intermediate results).</p> <code>1.5</code> <p>Returns:</p> Type Description <code>float</code> <p>Estimated total memory in gigabytes.</p> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>def estimate_total_memory(\n    self, dtype_size: int = 8, n_bands: int = 1, overhead_factor: float = 1.5\n) -&gt; float:\n    \"\"\"Estimate total memory usage across all workers in GB.\n\n    Parameters\n    ----------\n    dtype_size : int, default=8\n        Size of data type in bytes.\n    n_bands : int, default=1\n        Number of bands/layers in the data.\n    overhead_factor : float, default=1.5\n        Multiplier for overhead (copies, intermediate results).\n\n    Returns\n    -------\n    float\n        Estimated total memory in gigabytes.\n\n    \"\"\"\n    mb_per_block = self.estimate_memory_per_block(dtype_size, n_bands)\n    # Assume each worker might hold multiple blocks\n    total_mb = mb_per_block * self.n_workers * overhead_factor\n    return total_mb / 1024  # Convert to GB\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.summary","title":"summary","text":"<pre><code>summary() -&gt; str\n</code></pre> <p>Generate a human-readable summary of settings.</p> <p>Returns:</p> Type Description <code>str</code> <p>Multi-line summary string.</p> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>def summary(self) -&gt; str:\n    \"\"\"Generate a human-readable summary of settings.\n\n    Returns\n    -------\n    str\n        Multi-line summary string.\n\n    \"\"\"\n    lines = [\n        \"WorkerSettings Configuration:\",\n        \"=\" * 50,\n        f\"Workers:              {self.n_workers}\",\n        f\"Threads per worker:   {self.threads_per_worker}\",\n        f\"Total threads:        {self.total_threads}\",\n        f\"Block shape:          {self.block_shape[0]} x {self.block_shape[1]}\",\n        f\"Block size:           {self.block_size:,} elements\",\n        f\"Estimated mem/block:  {self.estimate_memory_per_block():.2f} MB\",\n        f\"Estimated total mem:  {self.estimate_total_memory():.2f} GB\",\n        \"=\" * 50,\n    ]\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.validate_against_system","title":"validate_against_system","text":"<pre><code>validate_against_system() -&gt; Dict[str, Any]\n</code></pre> <p>Validate settings against system resources.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with validation results and warnings.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; settings = WorkerSettings(n_workers=16, threads_per_worker=8)\n&gt;&gt;&gt; validation = settings.validate_against_system()\n&gt;&gt;&gt; if validation['warnings']:\n...     for warning in validation['warnings']:\n...         print(f\"Warning: {warning}\")\n</code></pre> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>def validate_against_system(self) -&gt; Dict[str, Any]:\n    \"\"\"Validate settings against system resources.\n\n    Returns\n    -------\n    dict\n        Dictionary with validation results and warnings.\n\n    Examples\n    --------\n    &gt;&gt;&gt; settings = WorkerSettings(n_workers=16, threads_per_worker=8)\n    &gt;&gt;&gt; validation = settings.validate_against_system()\n    &gt;&gt;&gt; if validation['warnings']:\n    ...     for warning in validation['warnings']:\n    ...         print(f\"Warning: {warning}\")\n\n    \"\"\"\n    cpu_count = multiprocessing.cpu_count()\n    warnings = []\n\n    # Check if total threads exceed CPU count\n    if self.total_threads &gt; cpu_count:\n        warnings.append(\n            f\"Total threads ({self.total_threads}) exceeds available \"\n            f\"CPU cores ({cpu_count}). May cause oversubscription.\"\n        )\n\n    # Check if block size is very large\n    if self.block_size &gt; 1_000_000:\n        warnings.append(\n            f\"Block size ({self.block_size:,}) is very large. \"\n            \"May cause high memory usage.\"\n        )\n\n    # Check if block size is very small\n    if self.block_size &lt; 1_000:\n        warnings.append(\n            f\"Block size ({self.block_size:,}) is very small. \"\n            \"May cause poor performance due to overhead.\"\n        )\n\n    return {\n        \"valid\": len(warnings) == 0,\n        \"warnings\": warnings,\n        \"system_cpu_count\": cpu_count,\n        \"configured_total_threads\": self.total_threads,\n        \"cpu_utilization\": self.total_threads / cpu_count if cpu_count &gt; 0 else 0,\n    }\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel","title":"YamlModel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model that can be exported to yaml.</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>class YamlModel(BaseModel):\n    \"\"\"Pydantic model that can be exported to yaml.\"\"\"\n\n    model_config = STRICT_CONFIG\n\n    def to_yaml(\n        self,\n        output_path: Union[Filename, TextIO],\n        with_comments: bool = True,\n        by_alias: bool = True,\n        indent_per_level: int = 2,\n    ):\n        \"\"\"Save configuration as a yaml file.\n\n        Used to record the default-filled version of a supplied yaml.\n\n        Parameters\n        ----------\n        output_path : Pathlike\n            Path to the yaml file to save.\n        with_comments : bool, default = False.\n            Whether to add comments containing the type/descriptions to all fields.\n        by_alias : bool, default = False.\n            Whether to use the alias names for the fields.\n            Passed to pydantic's ``to_json`` method.\n            https://docs.pydantic.dev/usage/exporting_models/#modeljson\n        indent_per_level : int, default = 2\n            Number of spaces to indent per level.\n\n        \"\"\"\n        yaml_obj = self._to_yaml_obj(by_alias=by_alias)\n\n        if with_comments:\n            _add_comments(\n                yaml_obj,\n                self.model_json_schema(by_alias=by_alias),\n                indent_per_level=indent_per_level,\n            )\n\n        y = YAML()\n        # https://yaml.readthedocs.io/en/latest/detail.html#indentation-of-block-sequences\n        y.indent(\n            offset=indent_per_level,\n            mapping=indent_per_level,\n            # It is best to always have sequence &gt;= offset + 2 but this is not enforced\n            # not following this advice might lead to invalid output.\n            sequence=indent_per_level + 2,\n        )\n        if hasattr(output_path, \"write\"):\n            y.dump(yaml_obj, output_path)\n        else:\n            with open(output_path, \"w\") as f:\n                y.dump(yaml_obj, f)\n\n    @classmethod\n    def from_yaml(cls, yaml_path: Filename):\n        \"\"\"Load a configuration from a yaml file.\n\n        Parameters\n        ----------\n        yaml_path : Pathlike\n            Path to the yaml file to load.\n\n        Returns\n        -------\n        Config\n            Workflow configuration\n\n        \"\"\"\n        y = YAML(typ=\"safe\")\n        with open(yaml_path) as f:\n            data = y.load(f)\n\n        return cls(**data)\n\n    @classmethod\n    def print_yaml_schema(\n        cls,\n        output_path: Union[Filename, TextIO] = sys.stdout,\n        indent_per_level: int = 2,\n    ):\n        \"\"\"Print/save an empty configuration with defaults filled in.\n\n        Ignores the required `input_file_list` input, so a user can\n        inspect all fields.\n\n        Parameters\n        ----------\n        output_path : Pathlike\n            Path or stream to save to the yaml file to.\n            By default, prints to stdout.\n        indent_per_level : int, default = 2\n            Number of spaces to indent per level.\n\n        \"\"\"\n        cls.model_construct().to_yaml(\n            output_path, with_comments=True, indent_per_level=indent_per_level\n        )\n\n    def _to_yaml_obj(self, by_alias: bool = True) -&gt; CommentedMap:\n        # Make the YAML object to add comments to\n        # We can't just do `dumps` for some reason, need a stream\n        y = YAML()\n        ss = StringIO()\n        y.dump(json.loads(self.model_dump_json(by_alias=by_alias)), ss)\n        return y.load(ss.getvalue())\n\n    def get_all_file_paths(\n        self, include_none: bool = False, flatten_lists: bool = True\n    ) -&gt; Dict[str, Union[Path, List[Path]]]:\n        \"\"\"Get all Path fields from the model.\n\n        Parameters\n        ----------\n        include_none : bool, default=False\n            Include fields with None values.\n        flatten_lists : bool, default=True\n            Flatten list fields to individual entries with indices.\n\n        Returns\n        -------\n        dict\n            Mapping of field names to Path objects.\n\n        \"\"\"\n        files: Dict[str, Path | list[Path]] = {}\n\n        for field_name, field_info in self.model_fields.items():\n            value = getattr(self, field_name)\n\n            # Skip None values if requested\n            if value is None and not include_none:\n                continue\n\n            # Check if field is Path or Optional[Path]\n            if self._is_path_field(field_info):\n                if value is not None:\n                    files[field_name] = value\n\n            # Check if field is List[Path]\n            elif self._is_path_list_field(field_info):\n                if value:\n                    if flatten_lists:\n                        for i, path in enumerate(value):\n                            files[f\"{field_name}[{i}]\"] = path\n                    else:\n                        files[field_name] = value\n\n        return files\n\n    @staticmethod\n    def _is_path_field(field_info) -&gt; bool:\n        \"\"\"Check if field is a Path type.\"\"\"\n        from pathlib import Path\n        from typing import Union, get_args, get_origin\n\n        annotation = field_info.annotation\n\n        # Handle Annotated[Path, ...]\n        if get_origin(annotation) is not None:\n            # Check if it's Annotated\n            if hasattr(annotation, \"__metadata__\"):\n                # Get the actual type from Annotated\n                args = get_args(annotation)\n                if args:\n                    annotation = args[0]\n\n        # Direct Path type\n        if annotation is Path:\n            return True\n\n        # Optional[Path] or Union[Path, None]\n        origin = get_origin(annotation)\n        if origin is Union:\n            args = get_args(annotation)\n            return Path in args or any(arg is Path for arg in args)\n\n        return False\n\n    @staticmethod\n    def _is_path_list_field(field_info) -&gt; bool:\n        \"\"\"Check if field is a List[Path] type.\"\"\"\n        from pathlib import Path\n        from typing import Union, get_args, get_origin\n\n        annotation = field_info.annotation\n        origin = get_origin(annotation)\n\n        # Check if it's Optional[List[Path]]\n        if origin is Union:\n            args = get_args(annotation)\n            for arg in args:\n                if arg is type(None):\n                    continue\n                if get_origin(arg) in (list, List):\n                    list_args = get_args(arg)\n                    if list_args:\n                        first_arg = list_args[0]\n                        if first_arg is Path:\n                            return True\n\n        # Check if it's List[Path]\n        if origin in (list, List):\n            args = get_args(annotation)\n            if not args:\n                return False\n            first_arg = args[0]\n            is_path_type: bool = first_arg is Path\n            return is_path_type\n\n        return False\n\n    def validate_files_exist(\n        self, raise_on_missing: bool = False\n    ) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Validate all file paths exist on disk.\n\n        Parameters\n        ----------\n        raise_on_missing : bool, default=False\n            If True, raise FileNotFoundError on first missing file.\n\n        Returns\n        -------\n        dict\n            Detailed status for each file including existence, size, etc.\n\n        Raises\n        ------\n        FileNotFoundError\n            If raise_on_missing=True and any file is missing.\n\n        \"\"\"\n        results: Dict[str, Dict[str, Any]] = {}\n\n        # flatten_lists=True guarantees Dict[str, Path]\n        file_paths: Dict[str, Path] = cast(\n            Dict[str, Path], self.get_all_file_paths(flatten_lists=True)\n        )\n\n        for field_name, path in file_paths.items():\n            exists = path.exists()\n\n            if not exists and raise_on_missing:\n                raise FileNotFoundError(\n                    f\"Required file not found: {field_name} = {path}\"\n                )\n\n            results[field_name] = {\n                \"exists\": exists,\n                \"is_file\": path.is_file() if exists else None,\n                \"is_dir\": path.is_dir() if exists else None,\n                \"size_bytes\": path.stat().st_size if exists else None,\n                \"absolute_path\": str(path.absolute()),\n            }\n\n        return results\n\n    def validate_ready_to_run(self) -&gt; ValidationResult:\n        \"\"\"Check if configuration is ready to run.\"\"\"\n        return ValidationResult(ready=True, errors=[], warnings=[])\n\n    def get_missing_files(self) -&gt; List[str]:\n        \"\"\"Get list of missing file paths.\"\"\"\n        return [\n            f\"{name}: {info['absolute_path']}\"\n            for name, info in self.validate_files_exist().items()\n            if not info[\"exists\"]\n        ]\n\n    def all_files_exist(self) -&gt; bool:\n        \"\"\"Check if all files exist.\"\"\"\n        return len(self.get_missing_files()) == 0\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.all_files_exist","title":"all_files_exist","text":"<pre><code>all_files_exist() -&gt; bool\n</code></pre> <p>Check if all files exist.</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>def all_files_exist(self) -&gt; bool:\n    \"\"\"Check if all files exist.\"\"\"\n    return len(self.get_missing_files()) == 0\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(yaml_path: Filename)\n</code></pre> <p>Load a configuration from a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>Pathlike</code> <p>Path to the yaml file to load.</p> required <p>Returns:</p> Type Description <code>Config</code> <p>Workflow configuration</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>@classmethod\ndef from_yaml(cls, yaml_path: Filename):\n    \"\"\"Load a configuration from a yaml file.\n\n    Parameters\n    ----------\n    yaml_path : Pathlike\n        Path to the yaml file to load.\n\n    Returns\n    -------\n    Config\n        Workflow configuration\n\n    \"\"\"\n    y = YAML(typ=\"safe\")\n    with open(yaml_path) as f:\n        data = y.load(f)\n\n    return cls(**data)\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.get_all_file_paths","title":"get_all_file_paths","text":"<pre><code>get_all_file_paths(include_none: bool = False, flatten_lists: bool = True) -&gt; Dict[str, Union[Path, List[Path]]]\n</code></pre> <p>Get all Path fields from the model.</p> <p>Parameters:</p> Name Type Description Default <code>include_none</code> <code>bool</code> <p>Include fields with None values.</p> <code>False</code> <code>flatten_lists</code> <code>bool</code> <p>Flatten list fields to individual entries with indices.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Mapping of field names to Path objects.</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>def get_all_file_paths(\n    self, include_none: bool = False, flatten_lists: bool = True\n) -&gt; Dict[str, Union[Path, List[Path]]]:\n    \"\"\"Get all Path fields from the model.\n\n    Parameters\n    ----------\n    include_none : bool, default=False\n        Include fields with None values.\n    flatten_lists : bool, default=True\n        Flatten list fields to individual entries with indices.\n\n    Returns\n    -------\n    dict\n        Mapping of field names to Path objects.\n\n    \"\"\"\n    files: Dict[str, Path | list[Path]] = {}\n\n    for field_name, field_info in self.model_fields.items():\n        value = getattr(self, field_name)\n\n        # Skip None values if requested\n        if value is None and not include_none:\n            continue\n\n        # Check if field is Path or Optional[Path]\n        if self._is_path_field(field_info):\n            if value is not None:\n                files[field_name] = value\n\n        # Check if field is List[Path]\n        elif self._is_path_list_field(field_info):\n            if value:\n                if flatten_lists:\n                    for i, path in enumerate(value):\n                        files[f\"{field_name}[{i}]\"] = path\n                else:\n                    files[field_name] = value\n\n    return files\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.get_missing_files","title":"get_missing_files","text":"<pre><code>get_missing_files() -&gt; List[str]\n</code></pre> <p>Get list of missing file paths.</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>def get_missing_files(self) -&gt; List[str]:\n    \"\"\"Get list of missing file paths.\"\"\"\n    return [\n        f\"{name}: {info['absolute_path']}\"\n        for name, info in self.validate_files_exist().items()\n        if not info[\"exists\"]\n    ]\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.print_yaml_schema","title":"print_yaml_schema  <code>classmethod</code>","text":"<pre><code>print_yaml_schema(output_path: Union[Filename, TextIO] = sys.stdout, indent_per_level: int = 2)\n</code></pre> <p>Print/save an empty configuration with defaults filled in.</p> <p>Ignores the required <code>input_file_list</code> input, so a user can inspect all fields.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Pathlike</code> <p>Path or stream to save to the yaml file to. By default, prints to stdout.</p> <code>stdout</code> <code>indent_per_level</code> <code>int</code> <p>Number of spaces to indent per level.</p> <code>= 2</code> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>@classmethod\ndef print_yaml_schema(\n    cls,\n    output_path: Union[Filename, TextIO] = sys.stdout,\n    indent_per_level: int = 2,\n):\n    \"\"\"Print/save an empty configuration with defaults filled in.\n\n    Ignores the required `input_file_list` input, so a user can\n    inspect all fields.\n\n    Parameters\n    ----------\n    output_path : Pathlike\n        Path or stream to save to the yaml file to.\n        By default, prints to stdout.\n    indent_per_level : int, default = 2\n        Number of spaces to indent per level.\n\n    \"\"\"\n    cls.model_construct().to_yaml(\n        output_path, with_comments=True, indent_per_level=indent_per_level\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.to_yaml","title":"to_yaml","text":"<pre><code>to_yaml(output_path: Union[Filename, TextIO], with_comments: bool = True, by_alias: bool = True, indent_per_level: int = 2)\n</code></pre> <p>Save configuration as a yaml file.</p> <p>Used to record the default-filled version of a supplied yaml.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Pathlike</code> <p>Path to the yaml file to save.</p> required <code>with_comments</code> <code>bool</code> <p>Whether to add comments containing the type/descriptions to all fields.</p> <code>= False.</code> <code>by_alias</code> <code>bool</code> <p>Whether to use the alias names for the fields. Passed to pydantic's <code>to_json</code> method. https://docs.pydantic.dev/usage/exporting_models/#modeljson</p> <code>= False.</code> <code>indent_per_level</code> <code>int</code> <p>Number of spaces to indent per level.</p> <code>= 2</code> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>def to_yaml(\n    self,\n    output_path: Union[Filename, TextIO],\n    with_comments: bool = True,\n    by_alias: bool = True,\n    indent_per_level: int = 2,\n):\n    \"\"\"Save configuration as a yaml file.\n\n    Used to record the default-filled version of a supplied yaml.\n\n    Parameters\n    ----------\n    output_path : Pathlike\n        Path to the yaml file to save.\n    with_comments : bool, default = False.\n        Whether to add comments containing the type/descriptions to all fields.\n    by_alias : bool, default = False.\n        Whether to use the alias names for the fields.\n        Passed to pydantic's ``to_json`` method.\n        https://docs.pydantic.dev/usage/exporting_models/#modeljson\n    indent_per_level : int, default = 2\n        Number of spaces to indent per level.\n\n    \"\"\"\n    yaml_obj = self._to_yaml_obj(by_alias=by_alias)\n\n    if with_comments:\n        _add_comments(\n            yaml_obj,\n            self.model_json_schema(by_alias=by_alias),\n            indent_per_level=indent_per_level,\n        )\n\n    y = YAML()\n    # https://yaml.readthedocs.io/en/latest/detail.html#indentation-of-block-sequences\n    y.indent(\n        offset=indent_per_level,\n        mapping=indent_per_level,\n        # It is best to always have sequence &gt;= offset + 2 but this is not enforced\n        # not following this advice might lead to invalid output.\n        sequence=indent_per_level + 2,\n    )\n    if hasattr(output_path, \"write\"):\n        y.dump(yaml_obj, output_path)\n    else:\n        with open(output_path, \"w\") as f:\n            y.dump(yaml_obj, f)\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.validate_files_exist","title":"validate_files_exist","text":"<pre><code>validate_files_exist(raise_on_missing: bool = False) -&gt; Dict[str, Dict[str, Any]]\n</code></pre> <p>Validate all file paths exist on disk.</p> <p>Parameters:</p> Name Type Description Default <code>raise_on_missing</code> <code>bool</code> <p>If True, raise FileNotFoundError on first missing file.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Detailed status for each file including existence, size, etc.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If raise_on_missing=True and any file is missing.</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>def validate_files_exist(\n    self, raise_on_missing: bool = False\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Validate all file paths exist on disk.\n\n    Parameters\n    ----------\n    raise_on_missing : bool, default=False\n        If True, raise FileNotFoundError on first missing file.\n\n    Returns\n    -------\n    dict\n        Detailed status for each file including existence, size, etc.\n\n    Raises\n    ------\n    FileNotFoundError\n        If raise_on_missing=True and any file is missing.\n\n    \"\"\"\n    results: Dict[str, Dict[str, Any]] = {}\n\n    # flatten_lists=True guarantees Dict[str, Path]\n    file_paths: Dict[str, Path] = cast(\n        Dict[str, Path], self.get_all_file_paths(flatten_lists=True)\n    )\n\n    for field_name, path in file_paths.items():\n        exists = path.exists()\n\n        if not exists and raise_on_missing:\n            raise FileNotFoundError(\n                f\"Required file not found: {field_name} = {path}\"\n            )\n\n        results[field_name] = {\n            \"exists\": exists,\n            \"is_file\": path.is_file() if exists else None,\n            \"is_dir\": path.is_dir() if exists else None,\n            \"size_bytes\": path.stat().st_size if exists else None,\n            \"absolute_path\": str(path.absolute()),\n        }\n\n    return results\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.validate_ready_to_run","title":"validate_ready_to_run","text":"<pre><code>validate_ready_to_run() -&gt; ValidationResult\n</code></pre> <p>Check if configuration is ready to run.</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>def validate_ready_to_run(self) -&gt; ValidationResult:\n    \"\"\"Check if configuration is ready to run.\"\"\"\n    return ValidationResult(ready=True, errors=[], warnings=[])\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig","title":"pge_runconfig","text":""},{"location":"api/#cal_disp.config.pge_runconfig.OutputOptions","title":"OutputOptions","text":"<p>               Bases: <code>YamlModel</code></p> <p>Output configuration options.</p> <p>Attributes:</p> Name Type Description <code>product_version</code> <code>str</code> <p>Version of the product in . format. <code>output_format</code> <code>str</code> <p>Format for output files (e.g., 'netcdf', 'hdf5').</p> <code>compression</code> <code>bool</code> <p>Whether to compress output files.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>class OutputOptions(YamlModel):\n    \"\"\"Output configuration options.\n\n    Attributes\n    ----------\n    product_version : str\n        Version of the product in &lt;major&gt;.&lt;minor&gt; format.\n    output_format : str\n        Format for output files (e.g., 'netcdf', 'hdf5').\n    compression : bool\n        Whether to compress output files.\n\n    \"\"\"\n\n    product_version: str = Field(\n        default=\"1.0\",\n        description=\"Version of the product, in &lt;major&gt;.&lt;minor&gt; format.\",\n    )\n\n    output_format: str = Field(\n        default=\"netcdf\",\n        description=\"Output file format.\",\n    )\n\n    compression: bool = Field(\n        default=True,\n        description=\"Whether to compress output files.\",\n    )\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.PrimaryExecutable","title":"PrimaryExecutable","text":"<p>               Bases: <code>YamlModel</code></p> <p>Group describing the primary executable.</p> <p>Attributes:</p> Name Type Description <code>product_type</code> <code>str</code> <p>Product type identifier for the PGE.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>class PrimaryExecutable(YamlModel):\n    \"\"\"Group describing the primary executable.\n\n    Attributes\n    ----------\n    product_type : str\n        Product type identifier for the PGE.\n\n    \"\"\"\n\n    product_type: str = Field(\n        default=\"CAL_DISP\",\n        description=\"Product type of the PGE.\",\n    )\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.ProductPathGroup","title":"ProductPathGroup","text":"<p>               Bases: <code>YamlModel</code></p> <p>Group describing the product paths.</p> <p>Attributes:</p> Name Type Description <code>product_path</code> <code>Path</code> <p>Directory where PGE will place results.</p> <code>scratch_path</code> <code>Path</code> <p>Path to the scratch directory for intermediate files.</p> <code>output_path</code> <code>Path</code> <p>Path to the SAS output directory.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>class ProductPathGroup(YamlModel):\n    \"\"\"Group describing the product paths.\n\n    Attributes\n    ----------\n    product_path : Path\n        Directory where PGE will place results.\n    scratch_path : Path\n        Path to the scratch directory for intermediate files.\n    output_path : Path\n        Path to the SAS output directory.\n\n    \"\"\"\n\n    product_path: DirectoryPath = Field(\n        default=Path(),\n        description=\"Directory where PGE will place results.\",\n    )\n\n    scratch_path: DirectoryPath = Field(\n        default=Path(\"./scratch\"),\n        description=\"Path to the scratch directory for intermediate files.\",\n    )\n\n    output_path: DirectoryPath = Field(\n        default=Path(\"./output\"),\n        description=\"Path to the SAS output directory.\",\n        alias=\"sas_output_path\",\n    )\n\n    model_config = ConfigDict(extra=\"forbid\", populate_by_name=True)\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig","title":"RunConfig","text":"<p>               Bases: <code>YamlModel</code></p> <p>A PGE (Product Generation Executive) run configuration.</p> <p>This class represents the top-level configuration for running the calibration workflow as a PGE. It includes input files, output options, paths, and worker settings.</p> <p>Attributes:</p> Name Type Description <code>input_file_group</code> <code>InputFileGroup</code> <p>Configuration for input files.</p> <code>dynamic_ancillary_file_group</code> <code>Optional[DynamicAncillaryFileGroup]</code> <p>Dynamic ancillary files configuration.</p> <code>static_ancillary_file_group</code> <code>Optional[StaticAncillaryFileGroup]</code> <p>Static ancillary files configuration.</p> <code>output_options</code> <code>OutputOptions</code> <p>Output configuration options.</p> <code>primary_executable</code> <code>PrimaryExecutable</code> <p>Primary executable configuration.</p> <code>product_path_group</code> <code>ProductPathGroup</code> <p>Product path configuration.</p> <code>worker_settings</code> <code>WorkerSettings</code> <p>Dask worker and parallelism configuration.</p> <code>log_file</code> <code>Optional[Path]</code> <p>Path to the output log file.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>class RunConfig(YamlModel):\n    \"\"\"A PGE (Product Generation Executive) run configuration.\n\n    This class represents the top-level configuration for running the\n    calibration workflow as a PGE. It includes input files, output options,\n    paths, and worker settings.\n\n    Attributes\n    ----------\n    input_file_group : InputFileGroup\n        Configuration for input files.\n    dynamic_ancillary_file_group : Optional[DynamicAncillaryFileGroup]\n        Dynamic ancillary files configuration.\n    static_ancillary_file_group : Optional[StaticAncillaryFileGroup]\n        Static ancillary files configuration.\n    output_options : OutputOptions\n        Output configuration options.\n    primary_executable : PrimaryExecutable\n        Primary executable configuration.\n    product_path_group : ProductPathGroup\n        Product path configuration.\n    worker_settings : WorkerSettings\n        Dask worker and parallelism configuration.\n    log_file : Optional[Path]\n        Path to the output log file.\n\n    \"\"\"\n\n    # Used for the top-level key in YAML\n    name: ClassVar[str] = \"cal_disp_workflow\"\n\n    # Input configuration\n    input_file_group: InputFileGroup = Field(\n        ...,\n        description=\"Configuration for required input files.\",\n    )\n\n    dynamic_ancillary_file_group: Optional[DynamicAncillaryFileGroup] = Field(\n        default=None,\n        description=\"Dynamic ancillary files configuration.\",\n    )\n\n    static_ancillary_file_group: Optional[StaticAncillaryFileGroup] = Field(\n        default=None,\n        description=\"Static ancillary files configuration.\",\n    )\n\n    # Output and execution configuration\n    output_options: OutputOptions = Field(\n        default_factory=OutputOptions,\n        description=\"Output configuration options.\",\n    )\n\n    primary_executable: PrimaryExecutable = Field(\n        default_factory=PrimaryExecutable,\n        description=\"Primary executable configuration.\",\n    )\n\n    product_path_group: ProductPathGroup = Field(\n        default_factory=ProductPathGroup,\n        description=\"Product path configuration.\",\n    )\n\n    # Worker settings\n    worker_settings: WorkerSettings = Field(\n        default_factory=WorkerSettings,\n        description=\"Dask worker and parallelism configuration.\",\n    )\n\n    # Logging\n    log_file: Optional[Path] = Field(\n        default=None,\n        description=(\n            \"Path to the output log file in addition to logging to stderr. \"\n            \"If None, will be set based on output path.\"\n        ),\n    )\n\n    def to_workflow(self) -&gt; CalibrationWorkflow:\n        \"\"\"Convert PGE RunConfig to a CalibrationWorkflow object.\n\n        This method translates the PGE-style configuration into the format\n        expected by CalibrationWorkflow.\n\n        Returns\n        -------\n        CalibrationWorkflow\n            Converted workflow configuration.\n\n        Examples\n        --------\n        &gt;&gt;&gt; run_config = RunConfig.from_yaml_file(\"pge_config.yaml\")\n        &gt;&gt;&gt; workflow = run_config.to_workflow()\n        &gt;&gt;&gt; workflow.create_directories()\n        &gt;&gt;&gt; workflow.run()\n\n        \"\"\"\n        # Set up directories\n        scratch_directory = self.product_path_group.scratch_path\n        output_directory = self.product_path_group.output_path\n\n        # Set up log file\n        log_file = self.log_file\n        if log_file is None:\n            log_file = output_directory / \"cal_disp_workflow.log\"\n\n        # Create the workflow\n        workflow = CalibrationWorkflow(\n            # Input files\n            input_options=self.input_file_group,\n            # Ancillary files\n            dynamic_ancillary_file_options=self.dynamic_ancillary_file_group,\n            static_ancillary_file_options=self.static_ancillary_file_group,\n            # Directories\n            work_directory=scratch_directory,\n            output_directory=output_directory,\n            # Settings\n            worker_settings=self.worker_settings,\n            log_file=log_file,\n            # Don't resolve paths yet - let the workflow handle it\n            keep_paths_relative=False,\n        )\n\n        return workflow\n\n    def create_directories(self, exist_ok: bool = True) -&gt; None:\n        \"\"\"Create all necessary directories for the PGE run.\n\n        Parameters\n        ----------\n        exist_ok : bool, default=True\n            If True, don't raise error if directories already exist.\n\n        \"\"\"\n        self.product_path_group.product_path.mkdir(parents=True, exist_ok=exist_ok)\n        self.product_path_group.scratch_path.mkdir(parents=True, exist_ok=exist_ok)\n        self.product_path_group.output_path.mkdir(parents=True, exist_ok=exist_ok)\n\n        # Create tmp directory in scratch\n        tmp_dir = self.product_path_group.scratch_path / \"tmp\"\n        tmp_dir.mkdir(parents=True, exist_ok=exist_ok)\n\n    def validate_ready_to_run(self) -&gt; ValidationResult:\n        \"\"\"Check if run configuration is ready for execution.\"\"\"\n        errors: List[str] = []\n        warnings: List[str] = []\n\n        # Check if None instead\n        if self.input_file_group.disp_file is None:\n            errors.append(\"disp_file must be provided\")\n\n        if self.input_file_group.calibration_reference_grid is None:\n            errors.append(\"calibration_reference_grid must be provided\")\n\n        # Check dynamic ancillary files if provided\n        if self.dynamic_ancillary_file_group:\n            if self.dynamic_ancillary_file_group.algorithm_parameters_file is None:\n                warnings.append(\"Missing algorithm_parameters_file\")\n            if self.dynamic_ancillary_file_group.geometry_file is None:\n                warnings.append(\"Missing geometry_file\")\n\n        return {\n            \"ready\": len(errors) == 0,\n            \"errors\": errors,\n            \"warnings\": warnings,\n        }\n\n    def summary(self) -&gt; str:\n        \"\"\"Generate a human-readable summary of the run configuration.\n\n        Returns\n        -------\n        str\n            Multi-line summary string.\n\n        \"\"\"\n        lines = [\n            \"PGE Run Configuration\",\n            \"=\" * 70,\n            \"\",\n            \"Product Information:\",\n            f\"  Product type:     {self.primary_executable.product_type}\",\n            f\"  Product version:  {self.output_options.product_version}\",\n            f\"  Output format:    {self.output_options.output_format}\",\n            \"\",\n            \"Paths:\",\n            f\"  Product path:     {self.product_path_group.product_path}\",\n            f\"  Scratch path:     {self.product_path_group.scratch_path}\",\n            f\"  Output path:      {self.product_path_group.output_path}\",\n            f\"  Log file:         {self.log_file or 'Default'}\",\n            \"\",\n            \"Input Files:\",\n            f\"  DISP file:        {self.input_file_group.disp_file}\",\n            f\"  Calibration grid: {self.input_file_group.calibration_reference_grid}\",\n            \"\",\n            \"Worker Settings:\",\n            f\"  Workers:          {self.worker_settings.n_workers}\",\n            f\"  Threads/worker:   {self.worker_settings.threads_per_worker}\",\n            f\"  Total threads:    {self.worker_settings.total_threads}\",\n        ]\n\n        # Dynamic ancillary files\n        if self.dynamic_ancillary_file_group:\n            lines.extend(\n                [\n                    \"\",\n                    \"Dynamic Ancillary Files:\",\n                ]\n            )\n            dynamic_files = self.dynamic_ancillary_file_group.get_all_files()\n            for name, path in list(dynamic_files.items())[:5]:\n                lines.append(f\"  {name}: {path}\")\n            if len(dynamic_files) &gt; 5:\n                lines.append(f\"  ... and {len(dynamic_files) - 5} more\")\n\n        # Static ancillary files\n        if self.static_ancillary_file_group:\n            static_files = self.static_ancillary_file_group.get_all_files()\n            if static_files:\n                lines.extend(\n                    [\n                        \"\",\n                        \"Static Ancillary Files:\",\n                    ]\n                )\n                for name, path in static_files.items():\n                    lines.append(f\"  {name}: {path}\")\n\n        # Validation status\n        status = self.validate_ready_to_run()\n        lines.append(\"\")\n        if not status[\"ready\"]:\n            lines.append(\"\u26a0\ufe0f  Status: NOT READY\")\n            lines.append(\"Errors:\")\n            for error in status[\"errors\"]:\n                lines.append(f\"  - {error}\")\n        else:\n            lines.append(\"\u2713 Status: READY\")\n\n        if status[\"warnings\"]:\n            lines.append(\"\")\n            lines.append(\"Warnings:\")\n            for warning in status[\"warnings\"]:\n                lines.append(f\"  \u26a0\ufe0f  {warning}\")\n\n        lines.append(\"=\" * 70)\n\n        return \"\\n\".join(lines)\n\n    @classmethod\n    def create_example(cls) -&gt; \"RunConfig\":\n        \"\"\"Create an example PGE run configuration.\n\n        Returns\n        -------\n        RunConfig\n            Example configuration with placeholder values.\n\n        \"\"\"\n        return cls(\n            input_file_group=InputFileGroup(\n                disp_file=Path(\"input/disp.h5\"),\n                calibration_reference_grid=Path(\"input/cal_grid.parquet\"),\n                frame_id=1,\n            ),\n            dynamic_ancillary_file_group=DynamicAncillaryFileGroup(\n                algorithm_parameters_file=Path(\"config/algorithm_params.yaml\"),\n                static_layers_file=Path(\"input/geometry.h5\"),\n            ),\n            product_path_group=ProductPathGroup(\n                scratch_path=Path(\"./scratch\"), output_path=Path(\"./output\")\n            ),\n            worker_settings=WorkerSettings.create_standard(),\n        )\n\n    @classmethod\n    def from_yaml_file(cls, yaml_path: Path) -&gt; \"RunConfig\":\n        \"\"\"Load run configuration from YAML file.\n\n        Handles optional name wrapper key.\n\n        Parameters\n        ----------\n        yaml_path : Path\n            Path to YAML configuration file.\n\n        Returns\n        -------\n        RunConfig\n            Loaded and validated run configuration.\n\n        \"\"\"\n        data = cls._load_yaml_data(yaml_path)\n\n        # Handle optional wrapper key\n        if cls.name in data:\n            data = data[cls.name]\n\n        return cls.model_validate(data)\n\n    @staticmethod\n    def _load_yaml_data(yaml_path: Path) -&gt; dict:\n        \"\"\"Load YAML data from file.\n\n        Parameters\n        ----------\n        yaml_path : Path\n            Path to YAML file.\n\n        Returns\n        -------\n        dict\n            Loaded YAML data.\n\n        \"\"\"\n        from ruamel.yaml import YAML\n\n        y = YAML(typ=\"safe\")\n        with open(yaml_path) as f:\n            return y.load(f)\n\n    def to_yaml(\n        self,\n        output_path: Union[str, PathLike, TextIO],\n        with_comments: bool = True,  # noqa: ARG002\n        by_alias: bool = True,\n        indent_per_level: int = 2,\n    ) -&gt; None:  # Note: return type can be None or Any, both work\n        \"\"\"Save configuration to YAML file with name wrapper.\n\n        Parameters\n        ----------\n        output_path : str | PathLike | TextIO\n            Path where YAML should be saved.\n        with_comments : bool, default=True\n            Whether to include field descriptions as comments.\n        by_alias : bool, default=True\n            Whether to use field aliases in output.\n        indent_per_level : int, default=2\n            Indentation spacing.\n\n        Notes\n        -----\n        This method always wraps output in {cal_disp_workflow: ...} structure.\n        The with_comments parameter is accepted for signature compatibility but\n        not currently used in the wrapper output.\n\n        \"\"\"\n        from ruamel.yaml import YAML\n\n        # Handle file-like objects\n        if hasattr(output_path, \"write\"):\n            raise ValueError(\n                \"RunConfig.to_yaml doesn't support file-like objects. \"\n                \"Save to a file path instead.\"\n            )\n\n        # Convert to Path\n        output_path_obj = Path(output_path)\n\n        # Get dict and convert paths\n        data = self.model_dump(mode=\"python\", by_alias=by_alias)\n        data = convert_paths_to_strings(data)\n        wrapped = {self.name: data}\n\n        # Write with proper formatting\n        output_path_obj.parent.mkdir(parents=True, exist_ok=True)\n        y = YAML()\n        y.indent(\n            mapping=indent_per_level,\n            sequence=indent_per_level + 2,\n            offset=indent_per_level,\n        )\n        with open(output_path_obj, \"w\") as f:\n            y.dump(wrapped, f)\n\n    model_config = STRICT_CONFIG_WITH_ALIASES\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.create_directories","title":"create_directories","text":"<pre><code>create_directories(exist_ok: bool = True) -&gt; None\n</code></pre> <p>Create all necessary directories for the PGE run.</p> <p>Parameters:</p> Name Type Description Default <code>exist_ok</code> <code>bool</code> <p>If True, don't raise error if directories already exist.</p> <code>True</code> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>def create_directories(self, exist_ok: bool = True) -&gt; None:\n    \"\"\"Create all necessary directories for the PGE run.\n\n    Parameters\n    ----------\n    exist_ok : bool, default=True\n        If True, don't raise error if directories already exist.\n\n    \"\"\"\n    self.product_path_group.product_path.mkdir(parents=True, exist_ok=exist_ok)\n    self.product_path_group.scratch_path.mkdir(parents=True, exist_ok=exist_ok)\n    self.product_path_group.output_path.mkdir(parents=True, exist_ok=exist_ok)\n\n    # Create tmp directory in scratch\n    tmp_dir = self.product_path_group.scratch_path / \"tmp\"\n    tmp_dir.mkdir(parents=True, exist_ok=exist_ok)\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.create_example","title":"create_example  <code>classmethod</code>","text":"<pre><code>create_example() -&gt; 'RunConfig'\n</code></pre> <p>Create an example PGE run configuration.</p> <p>Returns:</p> Type Description <code>RunConfig</code> <p>Example configuration with placeholder values.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>@classmethod\ndef create_example(cls) -&gt; \"RunConfig\":\n    \"\"\"Create an example PGE run configuration.\n\n    Returns\n    -------\n    RunConfig\n        Example configuration with placeholder values.\n\n    \"\"\"\n    return cls(\n        input_file_group=InputFileGroup(\n            disp_file=Path(\"input/disp.h5\"),\n            calibration_reference_grid=Path(\"input/cal_grid.parquet\"),\n            frame_id=1,\n        ),\n        dynamic_ancillary_file_group=DynamicAncillaryFileGroup(\n            algorithm_parameters_file=Path(\"config/algorithm_params.yaml\"),\n            static_layers_file=Path(\"input/geometry.h5\"),\n        ),\n        product_path_group=ProductPathGroup(\n            scratch_path=Path(\"./scratch\"), output_path=Path(\"./output\")\n        ),\n        worker_settings=WorkerSettings.create_standard(),\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.from_yaml_file","title":"from_yaml_file  <code>classmethod</code>","text":"<pre><code>from_yaml_file(yaml_path: Path) -&gt; 'RunConfig'\n</code></pre> <p>Load run configuration from YAML file.</p> <p>Handles optional name wrapper key.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>Path</code> <p>Path to YAML configuration file.</p> required <p>Returns:</p> Type Description <code>RunConfig</code> <p>Loaded and validated run configuration.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>@classmethod\ndef from_yaml_file(cls, yaml_path: Path) -&gt; \"RunConfig\":\n    \"\"\"Load run configuration from YAML file.\n\n    Handles optional name wrapper key.\n\n    Parameters\n    ----------\n    yaml_path : Path\n        Path to YAML configuration file.\n\n    Returns\n    -------\n    RunConfig\n        Loaded and validated run configuration.\n\n    \"\"\"\n    data = cls._load_yaml_data(yaml_path)\n\n    # Handle optional wrapper key\n    if cls.name in data:\n        data = data[cls.name]\n\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.summary","title":"summary","text":"<pre><code>summary() -&gt; str\n</code></pre> <p>Generate a human-readable summary of the run configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Multi-line summary string.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>def summary(self) -&gt; str:\n    \"\"\"Generate a human-readable summary of the run configuration.\n\n    Returns\n    -------\n    str\n        Multi-line summary string.\n\n    \"\"\"\n    lines = [\n        \"PGE Run Configuration\",\n        \"=\" * 70,\n        \"\",\n        \"Product Information:\",\n        f\"  Product type:     {self.primary_executable.product_type}\",\n        f\"  Product version:  {self.output_options.product_version}\",\n        f\"  Output format:    {self.output_options.output_format}\",\n        \"\",\n        \"Paths:\",\n        f\"  Product path:     {self.product_path_group.product_path}\",\n        f\"  Scratch path:     {self.product_path_group.scratch_path}\",\n        f\"  Output path:      {self.product_path_group.output_path}\",\n        f\"  Log file:         {self.log_file or 'Default'}\",\n        \"\",\n        \"Input Files:\",\n        f\"  DISP file:        {self.input_file_group.disp_file}\",\n        f\"  Calibration grid: {self.input_file_group.calibration_reference_grid}\",\n        \"\",\n        \"Worker Settings:\",\n        f\"  Workers:          {self.worker_settings.n_workers}\",\n        f\"  Threads/worker:   {self.worker_settings.threads_per_worker}\",\n        f\"  Total threads:    {self.worker_settings.total_threads}\",\n    ]\n\n    # Dynamic ancillary files\n    if self.dynamic_ancillary_file_group:\n        lines.extend(\n            [\n                \"\",\n                \"Dynamic Ancillary Files:\",\n            ]\n        )\n        dynamic_files = self.dynamic_ancillary_file_group.get_all_files()\n        for name, path in list(dynamic_files.items())[:5]:\n            lines.append(f\"  {name}: {path}\")\n        if len(dynamic_files) &gt; 5:\n            lines.append(f\"  ... and {len(dynamic_files) - 5} more\")\n\n    # Static ancillary files\n    if self.static_ancillary_file_group:\n        static_files = self.static_ancillary_file_group.get_all_files()\n        if static_files:\n            lines.extend(\n                [\n                    \"\",\n                    \"Static Ancillary Files:\",\n                ]\n            )\n            for name, path in static_files.items():\n                lines.append(f\"  {name}: {path}\")\n\n    # Validation status\n    status = self.validate_ready_to_run()\n    lines.append(\"\")\n    if not status[\"ready\"]:\n        lines.append(\"\u26a0\ufe0f  Status: NOT READY\")\n        lines.append(\"Errors:\")\n        for error in status[\"errors\"]:\n            lines.append(f\"  - {error}\")\n    else:\n        lines.append(\"\u2713 Status: READY\")\n\n    if status[\"warnings\"]:\n        lines.append(\"\")\n        lines.append(\"Warnings:\")\n        for warning in status[\"warnings\"]:\n            lines.append(f\"  \u26a0\ufe0f  {warning}\")\n\n    lines.append(\"=\" * 70)\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.to_workflow","title":"to_workflow","text":"<pre><code>to_workflow() -&gt; CalibrationWorkflow\n</code></pre> <p>Convert PGE RunConfig to a CalibrationWorkflow object.</p> <p>This method translates the PGE-style configuration into the format expected by CalibrationWorkflow.</p> <p>Returns:</p> Type Description <code>CalibrationWorkflow</code> <p>Converted workflow configuration.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; run_config = RunConfig.from_yaml_file(\"pge_config.yaml\")\n&gt;&gt;&gt; workflow = run_config.to_workflow()\n&gt;&gt;&gt; workflow.create_directories()\n&gt;&gt;&gt; workflow.run()\n</code></pre> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>def to_workflow(self) -&gt; CalibrationWorkflow:\n    \"\"\"Convert PGE RunConfig to a CalibrationWorkflow object.\n\n    This method translates the PGE-style configuration into the format\n    expected by CalibrationWorkflow.\n\n    Returns\n    -------\n    CalibrationWorkflow\n        Converted workflow configuration.\n\n    Examples\n    --------\n    &gt;&gt;&gt; run_config = RunConfig.from_yaml_file(\"pge_config.yaml\")\n    &gt;&gt;&gt; workflow = run_config.to_workflow()\n    &gt;&gt;&gt; workflow.create_directories()\n    &gt;&gt;&gt; workflow.run()\n\n    \"\"\"\n    # Set up directories\n    scratch_directory = self.product_path_group.scratch_path\n    output_directory = self.product_path_group.output_path\n\n    # Set up log file\n    log_file = self.log_file\n    if log_file is None:\n        log_file = output_directory / \"cal_disp_workflow.log\"\n\n    # Create the workflow\n    workflow = CalibrationWorkflow(\n        # Input files\n        input_options=self.input_file_group,\n        # Ancillary files\n        dynamic_ancillary_file_options=self.dynamic_ancillary_file_group,\n        static_ancillary_file_options=self.static_ancillary_file_group,\n        # Directories\n        work_directory=scratch_directory,\n        output_directory=output_directory,\n        # Settings\n        worker_settings=self.worker_settings,\n        log_file=log_file,\n        # Don't resolve paths yet - let the workflow handle it\n        keep_paths_relative=False,\n    )\n\n    return workflow\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.to_yaml","title":"to_yaml","text":"<pre><code>to_yaml(output_path: Union[str, PathLike, TextIO], with_comments: bool = True, by_alias: bool = True, indent_per_level: int = 2) -&gt; None\n</code></pre> <p>Save configuration to YAML file with name wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str | PathLike | TextIO</code> <p>Path where YAML should be saved.</p> required <code>with_comments</code> <code>bool</code> <p>Whether to include field descriptions as comments.</p> <code>True</code> <code>by_alias</code> <code>bool</code> <p>Whether to use field aliases in output.</p> <code>True</code> <code>indent_per_level</code> <code>int</code> <p>Indentation spacing.</p> <code>2</code> Notes <p>This method always wraps output in {cal_disp_workflow: ...} structure. The with_comments parameter is accepted for signature compatibility but not currently used in the wrapper output.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>def to_yaml(\n    self,\n    output_path: Union[str, PathLike, TextIO],\n    with_comments: bool = True,  # noqa: ARG002\n    by_alias: bool = True,\n    indent_per_level: int = 2,\n) -&gt; None:  # Note: return type can be None or Any, both work\n    \"\"\"Save configuration to YAML file with name wrapper.\n\n    Parameters\n    ----------\n    output_path : str | PathLike | TextIO\n        Path where YAML should be saved.\n    with_comments : bool, default=True\n        Whether to include field descriptions as comments.\n    by_alias : bool, default=True\n        Whether to use field aliases in output.\n    indent_per_level : int, default=2\n        Indentation spacing.\n\n    Notes\n    -----\n    This method always wraps output in {cal_disp_workflow: ...} structure.\n    The with_comments parameter is accepted for signature compatibility but\n    not currently used in the wrapper output.\n\n    \"\"\"\n    from ruamel.yaml import YAML\n\n    # Handle file-like objects\n    if hasattr(output_path, \"write\"):\n        raise ValueError(\n            \"RunConfig.to_yaml doesn't support file-like objects. \"\n            \"Save to a file path instead.\"\n        )\n\n    # Convert to Path\n    output_path_obj = Path(output_path)\n\n    # Get dict and convert paths\n    data = self.model_dump(mode=\"python\", by_alias=by_alias)\n    data = convert_paths_to_strings(data)\n    wrapped = {self.name: data}\n\n    # Write with proper formatting\n    output_path_obj.parent.mkdir(parents=True, exist_ok=True)\n    y = YAML()\n    y.indent(\n        mapping=indent_per_level,\n        sequence=indent_per_level + 2,\n        offset=indent_per_level,\n    )\n    with open(output_path_obj, \"w\") as f:\n        y.dump(wrapped, f)\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.validate_ready_to_run","title":"validate_ready_to_run","text":"<pre><code>validate_ready_to_run() -&gt; ValidationResult\n</code></pre> <p>Check if run configuration is ready for execution.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>def validate_ready_to_run(self) -&gt; ValidationResult:\n    \"\"\"Check if run configuration is ready for execution.\"\"\"\n    errors: List[str] = []\n    warnings: List[str] = []\n\n    # Check if None instead\n    if self.input_file_group.disp_file is None:\n        errors.append(\"disp_file must be provided\")\n\n    if self.input_file_group.calibration_reference_grid is None:\n        errors.append(\"calibration_reference_grid must be provided\")\n\n    # Check dynamic ancillary files if provided\n    if self.dynamic_ancillary_file_group:\n        if self.dynamic_ancillary_file_group.algorithm_parameters_file is None:\n            warnings.append(\"Missing algorithm_parameters_file\")\n        if self.dynamic_ancillary_file_group.geometry_file is None:\n            warnings.append(\"Missing geometry_file\")\n\n    return {\n        \"ready\": len(errors) == 0,\n        \"errors\": errors,\n        \"warnings\": warnings,\n    }\n</code></pre>"},{"location":"api/#cal_disp.config.workflow","title":"workflow","text":""},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow","title":"CalibrationWorkflow","text":"<p>               Bases: <code>YamlModel</code></p> <p>Calibration workflow configuration.</p> <p>This class manages the complete configuration for the displacement calibration workflow, including input files, output directories, worker settings, and logging configuration.</p> <p>Attributes:</p> Name Type Description <code>input_options</code> <code>Optional[InputFileGroup]</code> <p>Configuration for required input files.</p> <code>work_directory</code> <code>Path</code> <p>Directory for intermediate processing files.</p> <code>output_directory</code> <code>Path</code> <p>Directory for final output files.</p> <code>keep_paths_relative</code> <code>bool</code> <p>If False, resolve all paths to absolute paths.</p> <code>dynamic_ancillary_file_options</code> <code>Optional[DynamicAncillaryFileGroup]</code> <p>Optional dynamic ancillary files for processing.</p> <code>static_ancillary_file_options</code> <code>Optional[StaticAncillaryFileGroup]</code> <p>Optional static ancillary files for processing.</p> <code>worker_settings</code> <code>WorkerSettings</code> <p>Dask worker and threading configuration.</p> <code>log_file</code> <code>Optional[Path]</code> <p>Custom log file path.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>class CalibrationWorkflow(YamlModel):\n    \"\"\"Calibration workflow configuration.\n\n    This class manages the complete configuration for the displacement calibration\n    workflow, including input files, output directories, worker settings, and\n    logging configuration.\n\n    Attributes\n    ----------\n    input_options : Optional[InputFileGroup]\n        Configuration for required input files.\n    work_directory : Path\n        Directory for intermediate processing files.\n    output_directory : Path\n        Directory for final output files.\n    keep_paths_relative : bool\n        If False, resolve all paths to absolute paths.\n    dynamic_ancillary_file_options : Optional[DynamicAncillaryFileGroup]\n        Optional dynamic ancillary files for processing.\n    static_ancillary_file_options : Optional[StaticAncillaryFileGroup]\n        Optional static ancillary files for processing.\n    worker_settings : WorkerSettings\n        Dask worker and threading configuration.\n    log_file : Optional[Path]\n        Custom log file path.\n\n    \"\"\"\n\n    # Input/output file configuration\n    input_options: Optional[InputFileGroup] = Field(\n        default=None,\n        description=(\n            \"Configuration for required input files. Must be provided before running\"\n            \" workflow.\"\n        ),\n    )\n\n    work_directory: DirectoryPath = Field(\n        default=Path(),\n        description=(\n            \"Directory for intermediate processing files. Created if it doesn't exist.\"\n        ),\n    )\n\n    output_directory: DirectoryPath = Field(\n        default=Path(),\n        description=\"Directory for final output files. Created if it doesn't exist.\",\n    )\n\n    keep_paths_relative: bool = Field(\n        default=False,\n        description=(\n            \"If False, resolve all relative paths to absolute paths. \"\n            \"If True, keep paths as provided.\"\n        ),\n    )\n\n    # Optional ancillary file groups\n    dynamic_ancillary_file_options: Optional[DynamicAncillaryFileGroup] = Field(\n        default=None,\n        description=\"Dynamic ancillary files (dem, los, masks, troposphere, etc.).\",\n    )\n\n    static_ancillary_file_options: Optional[StaticAncillaryFileGroup] = Field(\n        default=None,\n        description=\"Static ancillary files (algorithm overrides, databases, etc.).\",\n    )\n\n    # Worker and logging configuration\n    worker_settings: WorkerSettings = Field(\n        default_factory=WorkerSettings,\n        description=\"Worker and parallelism configuration.\",\n    )\n\n    log_file: Optional[Path] = Field(\n        default=None,\n        description=(\n            \"Path to output log file (in addition to logging to stderr). \"\n            \"If None, defaults to 'cal_disp.log' in work_directory.\"\n        ),\n    )\n\n    @model_validator(mode=\"after\")\n    def _resolve_paths(self) -&gt; \"CalibrationWorkflow\":\n        \"\"\"Resolve all paths to absolute if keep_paths_relative is False.\n\n        Uses object.__setattr__() to bypass validate_assignment and avoid recursion.\n        \"\"\"\n        if not self.keep_paths_relative:\n            # Use object.__setattr__() to bypass validation and avoid recursion\n            object.__setattr__(self, \"work_directory\", self.work_directory.resolve())\n            object.__setattr__(\n                self, \"output_directory\", self.output_directory.resolve()\n            )\n\n            # Resolve log file path if provided\n            if self.log_file is not None:\n                object.__setattr__(self, \"log_file\", self.log_file.resolve())\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def _set_default_log_file(self) -&gt; \"CalibrationWorkflow\":\n        \"\"\"Set default log file path if not provided.\"\"\"\n        if self.log_file is None:\n            # Use object.__setattr__() to bypass validation\n            object.__setattr__(self, \"log_file\", self.work_directory / \"cal_disp.log\")\n\n        return self\n\n    def validate_ready_to_run(self) -&gt; ValidationResult:\n        \"\"\"Check if workflow is ready to run.\"\"\"\n        errors = []\n        warnings = []\n\n        if self.input_options is None:\n            errors.append(\"input_options must be provided\")\n        else:\n            if self.input_options.disp_file is None:\n                errors.append(\"disp_file must be provided in input_options\")\n            if self.input_options.calibration_reference_grid is None:\n                errors.append(\n                    \"calibration_reference_grid must be provided in input_options\"\n                )\n\n            missing = self.get_missing_files()\n            if missing:\n                warnings.append(f\"Missing files: {', '.join(missing)}\")\n\n        return {\n            \"ready\": len(errors) == 0,\n            \"errors\": errors,\n            \"warnings\": warnings,\n        }\n\n    def validate_input_files_exist(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Check if all input files exist.\"\"\"\n        results = {}\n\n        if self.input_options:\n            results.update(self.input_options.validate_files_exist())\n\n        if self.dynamic_ancillary_file_options:\n            results.update(self.dynamic_ancillary_file_options.validate_files_exist())\n\n        if self.static_ancillary_file_options:\n            results.update(self.static_ancillary_file_options.validate_files_exist())\n\n        return results\n\n    def get_missing_files(self) -&gt; List[str]:\n        \"\"\"Get list of missing required input files.\"\"\"\n        status = self.validate_input_files_exist()\n        return [name for name, info in status.items() if not info[\"exists\"]]\n\n    def create_directories(self, exist_ok: bool = True) -&gt; None:\n        \"\"\"Create work and output directories if they don't exist.\n\n        Parameters\n        ----------\n        exist_ok : bool, default=True\n            If True, don't raise error if directories already exist.\n\n        \"\"\"\n        self.work_directory.mkdir(parents=True, exist_ok=exist_ok)\n        self.output_directory.mkdir(parents=True, exist_ok=exist_ok)\n\n        # Also create parent directory for log file\n        if self.log_file is not None:\n            self.log_file.parent.mkdir(parents=True, exist_ok=exist_ok)\n\n    def setup_logging(\n        self, level: int = 20, format_string: Optional[str] = None  # logging.INFO\n    ):\n        \"\"\"Set up logging configuration for the workflow.\n\n        Parameters\n        ----------\n        level : int, default=20 (INFO)\n            Logging level (DEBUG=10, INFO=20, WARNING=30, ERROR=40, CRITICAL=50).\n        format_string : str, optional\n            Custom format string for log messages.\n\n        Returns\n        -------\n        logging.Logger\n            Configured logger instance.\n\n        \"\"\"\n        import logging\n\n        if format_string is None:\n            format_string = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\n        logger = logging.getLogger(\"calibration_workflow\")\n        logger.setLevel(level)\n        logger.handlers.clear()\n\n        # Console handler\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(level)\n        console_handler.setFormatter(logging.Formatter(format_string))\n        logger.addHandler(console_handler)\n\n        # File handler\n        if self.log_file is not None:\n            self.log_file.parent.mkdir(parents=True, exist_ok=True)\n            file_handler = logging.FileHandler(self.log_file)\n            file_handler.setLevel(level)\n            file_handler.setFormatter(logging.Formatter(format_string))\n            logger.addHandler(file_handler)\n\n        return logger\n\n    def summary(self) -&gt; str:\n        \"\"\"Generate a human-readable summary of the workflow configuration.\n\n        Returns\n        -------\n        str\n            Multi-line summary string.\n\n        \"\"\"\n        lines = [\n            \"Calibration Workflow Configuration\",\n            \"=\" * 70,\n            \"\",\n            \"Directories:\",\n            f\"  Work directory:   {self.work_directory}\",\n            f\"  Output directory: {self.output_directory}\",\n            f\"  Log file:         {self.log_file}\",\n            f\"  Keep relative:    {self.keep_paths_relative}\",\n            \"\",\n        ]\n\n        # Input files\n        if self.input_options:\n            lines.extend(\n                [\n                    \"Input Files:\",\n                    f\"  DISP file:        {self.input_options.disp_file}\",\n                    (\n                        \"  Calibration grid:\"\n                        f\" {self.input_options.calibration_reference_grid}\"\n                    ),\n                    \"\",\n                ]\n            )\n        else:\n            lines.extend(\n                [\n                    \"Input Files:\",\n                    \"   Not configured!\",\n                    \"\",\n                ]\n            )\n\n        # Worker settings\n        lines.extend(\n            [\n                \"Worker Settings:\",\n                f\"  Workers:          {self.worker_settings.n_workers}\",\n                f\"  Threads/worker:   {self.worker_settings.threads_per_worker}\",\n                f\"  Total threads:    {self.worker_settings.total_threads}\",\n                f\"  Block shape:      {self.worker_settings.block_shape}\",\n                \"\",\n            ]\n        )\n\n        # Dynamic ancillary files\n        if self.dynamic_ancillary_file_options:\n            dynamic_files = self.dynamic_ancillary_file_options.get_all_files()\n            if dynamic_files:\n                lines.extend([\"Dynamic Ancillary Files:\"])\n                for name, path in list(dynamic_files.items())[:5]:  # Show first 5\n                    lines.append(f\"  {name}: {path}\")\n                if len(dynamic_files) &gt; 5:\n                    lines.append(f\"  ... and {len(dynamic_files) - 5} more\")\n                lines.append(\"\")\n\n        # Static ancillary files\n        if self.static_ancillary_file_options:\n            static_files = self.static_ancillary_file_options.get_all_files()\n            if static_files:\n                lines.extend([\"Static Ancillary Files:\"])\n                for name, path in static_files.items():\n                    lines.append(f\"  {name}: {path}\")\n                lines.append(\"\")\n\n        # Check readiness\n        status = self.validate_ready_to_run()\n        if not status[\"ready\"]:\n            lines.extend(\n                [\n                    \"  Workflow Status: NOT READY\",\n                    \"Errors:\",\n                ]\n            )\n            for error in status[\"errors\"]:\n                lines.append(f\"  - {error}\")\n        else:\n            lines.extend(\n                [\n                    \" Workflow Status: READY\",\n                ]\n            )\n\n        if status[\"warnings\"]:\n            lines.extend(\n                [\n                    \"\",\n                    \"Warnings:\",\n                ]\n            )\n            for warning in status[\"warnings\"]:\n                lines.append(f\"  \u26a0\ufe0f  {warning}\")\n\n        lines.append(\"=\" * 70)\n\n        return \"\\n\".join(lines)\n\n    @classmethod\n    def create_example(cls) -&gt; \"CalibrationWorkflow\":\n        \"\"\"Create an example workflow configuration.\n\n        Returns\n        -------\n        CalibrationWorkflow\n            Example configuration with placeholder values.\n\n        \"\"\"\n        return cls(\n            work_directory=Path(\"./work\"),\n            output_directory=Path(\"./output\"),\n            input_options=InputFileGroup(\n                disp_file=Path(\"input/disp.h5\"),\n                calibration_reference_grid=Path(\"input/cal_grid.parquet\"),\n                frame_id=1,\n            ),\n            worker_settings=WorkerSettings.create_standard(),\n            keep_paths_relative=True,\n        )\n\n    @classmethod\n    def create_minimal(cls) -&gt; \"CalibrationWorkflow\":\n        \"\"\"Create a minimal workflow configuration without input files.\n\n        Returns\n        -------\n        CalibrationWorkflow\n            Minimal configuration. Input files must be added before running.\n\n        \"\"\"\n        return cls(\n            work_directory=Path(\"./work\"),\n            output_directory=Path(\"./output\"),\n        )\n\n    model_config = STRICT_CONFIG_WITH_ALIASES\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.create_directories","title":"create_directories","text":"<pre><code>create_directories(exist_ok: bool = True) -&gt; None\n</code></pre> <p>Create work and output directories if they don't exist.</p> <p>Parameters:</p> Name Type Description Default <code>exist_ok</code> <code>bool</code> <p>If True, don't raise error if directories already exist.</p> <code>True</code> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>def create_directories(self, exist_ok: bool = True) -&gt; None:\n    \"\"\"Create work and output directories if they don't exist.\n\n    Parameters\n    ----------\n    exist_ok : bool, default=True\n        If True, don't raise error if directories already exist.\n\n    \"\"\"\n    self.work_directory.mkdir(parents=True, exist_ok=exist_ok)\n    self.output_directory.mkdir(parents=True, exist_ok=exist_ok)\n\n    # Also create parent directory for log file\n    if self.log_file is not None:\n        self.log_file.parent.mkdir(parents=True, exist_ok=exist_ok)\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.create_example","title":"create_example  <code>classmethod</code>","text":"<pre><code>create_example() -&gt; 'CalibrationWorkflow'\n</code></pre> <p>Create an example workflow configuration.</p> <p>Returns:</p> Type Description <code>CalibrationWorkflow</code> <p>Example configuration with placeholder values.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>@classmethod\ndef create_example(cls) -&gt; \"CalibrationWorkflow\":\n    \"\"\"Create an example workflow configuration.\n\n    Returns\n    -------\n    CalibrationWorkflow\n        Example configuration with placeholder values.\n\n    \"\"\"\n    return cls(\n        work_directory=Path(\"./work\"),\n        output_directory=Path(\"./output\"),\n        input_options=InputFileGroup(\n            disp_file=Path(\"input/disp.h5\"),\n            calibration_reference_grid=Path(\"input/cal_grid.parquet\"),\n            frame_id=1,\n        ),\n        worker_settings=WorkerSettings.create_standard(),\n        keep_paths_relative=True,\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.create_minimal","title":"create_minimal  <code>classmethod</code>","text":"<pre><code>create_minimal() -&gt; 'CalibrationWorkflow'\n</code></pre> <p>Create a minimal workflow configuration without input files.</p> <p>Returns:</p> Type Description <code>CalibrationWorkflow</code> <p>Minimal configuration. Input files must be added before running.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>@classmethod\ndef create_minimal(cls) -&gt; \"CalibrationWorkflow\":\n    \"\"\"Create a minimal workflow configuration without input files.\n\n    Returns\n    -------\n    CalibrationWorkflow\n        Minimal configuration. Input files must be added before running.\n\n    \"\"\"\n    return cls(\n        work_directory=Path(\"./work\"),\n        output_directory=Path(\"./output\"),\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.get_missing_files","title":"get_missing_files","text":"<pre><code>get_missing_files() -&gt; List[str]\n</code></pre> <p>Get list of missing required input files.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>def get_missing_files(self) -&gt; List[str]:\n    \"\"\"Get list of missing required input files.\"\"\"\n    status = self.validate_input_files_exist()\n    return [name for name, info in status.items() if not info[\"exists\"]]\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(level: int = 20, format_string: Optional[str] = None)\n</code></pre> <p>Set up logging configuration for the workflow.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Logging level (DEBUG=10, INFO=20, WARNING=30, ERROR=40, CRITICAL=50).</p> <code>20 (INFO)</code> <code>format_string</code> <code>str</code> <p>Custom format string for log messages.</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>Configured logger instance.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>def setup_logging(\n    self, level: int = 20, format_string: Optional[str] = None  # logging.INFO\n):\n    \"\"\"Set up logging configuration for the workflow.\n\n    Parameters\n    ----------\n    level : int, default=20 (INFO)\n        Logging level (DEBUG=10, INFO=20, WARNING=30, ERROR=40, CRITICAL=50).\n    format_string : str, optional\n        Custom format string for log messages.\n\n    Returns\n    -------\n    logging.Logger\n        Configured logger instance.\n\n    \"\"\"\n    import logging\n\n    if format_string is None:\n        format_string = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\n    logger = logging.getLogger(\"calibration_workflow\")\n    logger.setLevel(level)\n    logger.handlers.clear()\n\n    # Console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(level)\n    console_handler.setFormatter(logging.Formatter(format_string))\n    logger.addHandler(console_handler)\n\n    # File handler\n    if self.log_file is not None:\n        self.log_file.parent.mkdir(parents=True, exist_ok=True)\n        file_handler = logging.FileHandler(self.log_file)\n        file_handler.setLevel(level)\n        file_handler.setFormatter(logging.Formatter(format_string))\n        logger.addHandler(file_handler)\n\n    return logger\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.summary","title":"summary","text":"<pre><code>summary() -&gt; str\n</code></pre> <p>Generate a human-readable summary of the workflow configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Multi-line summary string.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>def summary(self) -&gt; str:\n    \"\"\"Generate a human-readable summary of the workflow configuration.\n\n    Returns\n    -------\n    str\n        Multi-line summary string.\n\n    \"\"\"\n    lines = [\n        \"Calibration Workflow Configuration\",\n        \"=\" * 70,\n        \"\",\n        \"Directories:\",\n        f\"  Work directory:   {self.work_directory}\",\n        f\"  Output directory: {self.output_directory}\",\n        f\"  Log file:         {self.log_file}\",\n        f\"  Keep relative:    {self.keep_paths_relative}\",\n        \"\",\n    ]\n\n    # Input files\n    if self.input_options:\n        lines.extend(\n            [\n                \"Input Files:\",\n                f\"  DISP file:        {self.input_options.disp_file}\",\n                (\n                    \"  Calibration grid:\"\n                    f\" {self.input_options.calibration_reference_grid}\"\n                ),\n                \"\",\n            ]\n        )\n    else:\n        lines.extend(\n            [\n                \"Input Files:\",\n                \"   Not configured!\",\n                \"\",\n            ]\n        )\n\n    # Worker settings\n    lines.extend(\n        [\n            \"Worker Settings:\",\n            f\"  Workers:          {self.worker_settings.n_workers}\",\n            f\"  Threads/worker:   {self.worker_settings.threads_per_worker}\",\n            f\"  Total threads:    {self.worker_settings.total_threads}\",\n            f\"  Block shape:      {self.worker_settings.block_shape}\",\n            \"\",\n        ]\n    )\n\n    # Dynamic ancillary files\n    if self.dynamic_ancillary_file_options:\n        dynamic_files = self.dynamic_ancillary_file_options.get_all_files()\n        if dynamic_files:\n            lines.extend([\"Dynamic Ancillary Files:\"])\n            for name, path in list(dynamic_files.items())[:5]:  # Show first 5\n                lines.append(f\"  {name}: {path}\")\n            if len(dynamic_files) &gt; 5:\n                lines.append(f\"  ... and {len(dynamic_files) - 5} more\")\n            lines.append(\"\")\n\n    # Static ancillary files\n    if self.static_ancillary_file_options:\n        static_files = self.static_ancillary_file_options.get_all_files()\n        if static_files:\n            lines.extend([\"Static Ancillary Files:\"])\n            for name, path in static_files.items():\n                lines.append(f\"  {name}: {path}\")\n            lines.append(\"\")\n\n    # Check readiness\n    status = self.validate_ready_to_run()\n    if not status[\"ready\"]:\n        lines.extend(\n            [\n                \"  Workflow Status: NOT READY\",\n                \"Errors:\",\n            ]\n        )\n        for error in status[\"errors\"]:\n            lines.append(f\"  - {error}\")\n    else:\n        lines.extend(\n            [\n                \" Workflow Status: READY\",\n            ]\n        )\n\n    if status[\"warnings\"]:\n        lines.extend(\n            [\n                \"\",\n                \"Warnings:\",\n            ]\n        )\n        for warning in status[\"warnings\"]:\n            lines.append(f\"  \u26a0\ufe0f  {warning}\")\n\n    lines.append(\"=\" * 70)\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.validate_input_files_exist","title":"validate_input_files_exist","text":"<pre><code>validate_input_files_exist() -&gt; Dict[str, Dict[str, Any]]\n</code></pre> <p>Check if all input files exist.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>def validate_input_files_exist(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Check if all input files exist.\"\"\"\n    results = {}\n\n    if self.input_options:\n        results.update(self.input_options.validate_files_exist())\n\n    if self.dynamic_ancillary_file_options:\n        results.update(self.dynamic_ancillary_file_options.validate_files_exist())\n\n    if self.static_ancillary_file_options:\n        results.update(self.static_ancillary_file_options.validate_files_exist())\n\n    return results\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.validate_ready_to_run","title":"validate_ready_to_run","text":"<pre><code>validate_ready_to_run() -&gt; ValidationResult\n</code></pre> <p>Check if workflow is ready to run.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>def validate_ready_to_run(self) -&gt; ValidationResult:\n    \"\"\"Check if workflow is ready to run.\"\"\"\n    errors = []\n    warnings = []\n\n    if self.input_options is None:\n        errors.append(\"input_options must be provided\")\n    else:\n        if self.input_options.disp_file is None:\n            errors.append(\"disp_file must be provided in input_options\")\n        if self.input_options.calibration_reference_grid is None:\n            errors.append(\n                \"calibration_reference_grid must be provided in input_options\"\n            )\n\n        missing = self.get_missing_files()\n        if missing:\n            warnings.append(f\"Missing files: {', '.join(missing)}\")\n\n    return {\n        \"ready\": len(errors) == 0,\n        \"errors\": errors,\n        \"warnings\": warnings,\n    }\n</code></pre>"},{"location":"api/#browse-image","title":"Browse Image","text":""},{"location":"api/#cal_disp.browse_image","title":"cal_disp.browse_image","text":"<p>Module for creating browse images for the output product.</p>"},{"location":"api/#cal_disp.browse_image.make_browse_image_from_arr","title":"make_browse_image_from_arr","text":"<pre><code>make_browse_image_from_arr(output_filename: PathOrStr, arr: ArrayLike, mask: ArrayLike, max_dim_allowed: int = 2048, cmap: str = DEFAULT_CMAP, vmin: float = -0.1, vmax: float = 0.1) -&gt; None\n</code></pre> <p>Create a PNG browse image for the output product from given array.</p> Source code in <code>src/cal_disp/browse_image.py</code> <pre><code>def make_browse_image_from_arr(\n    output_filename: PathOrStr,\n    arr: ArrayLike,\n    mask: ArrayLike,\n    max_dim_allowed: int = 2048,\n    cmap: str = DEFAULT_CMAP,\n    vmin: float = -0.10,\n    vmax: float = 0.10,\n) -&gt; None:\n    \"\"\"Create a PNG browse image for the output product from given array.\"\"\"\n    arr[mask == 0] = np.nan\n    arr = _resize_to_max_pixel_dim(arr, max_dim_allowed)\n    _save_to_disk_as_color(arr, output_filename, cmap, vmin, vmax)\n</code></pre>"},{"location":"api/#cal_disp.browse_image.make_browse_image_from_nc","title":"make_browse_image_from_nc","text":"<pre><code>make_browse_image_from_nc(output_filename: PathOrStr, input_filename: PathOrStr, max_dim_allowed: int = 2048, cmap: str = DEFAULT_CMAP, vmin: float = -0.1, vmax: float = 0.1) -&gt; None\n</code></pre> <p>Create a PNG browse image for the output product from product in NetCDF file.</p> Source code in <code>src/cal_disp/browse_image.py</code> <pre><code>def make_browse_image_from_nc(\n    output_filename: PathOrStr,\n    input_filename: PathOrStr,\n    max_dim_allowed: int = 2048,\n    cmap: str = DEFAULT_CMAP,\n    vmin: float = -0.10,\n    vmax: float = 0.10,\n) -&gt; None:\n    \"\"\"Create a PNG browse image for the output product from product in NetCDF file.\"\"\"\n    arr = np.nanarray(input_filename)  # placeholder\n    mask = np.nanarray  # placeholder\n\n    make_browse_image_from_arr(\n        output_filename, arr, mask, max_dim_allowed, cmap, vmin, vmax\n    )\n</code></pre>"}]}