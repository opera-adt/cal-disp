{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#cli","title":"CLI","text":""},{"location":"api/#cal_disp.cli","title":"cal_disp.cli","text":""},{"location":"api/#cal_disp.cli.cli_app","title":"cli_app","text":"<pre><code>cli_app(ctx: Context, debug: bool) -&gt; None\n</code></pre> <p>Run a DISP calibration workflow.</p> Source code in <code>src/cal_disp/cli/__init__.py</code> <pre><code>@click.group(name=\"cal-disp\")\n@click.version_option()\n@click.option(\"--debug\", is_flag=True, help=\"Add debug messages to the log.\")\n@click.pass_context\ndef cli_app(ctx: click.Context, debug: bool) -&gt; None:\n    \"\"\"Run a DISP calibration workflow.\"\"\"\n    # https://click.palletsprojects.com/en/8.1.x/commands/#nested-handling-and-contexts\n    ctx.ensure_object(dict)\n    ctx.obj[\"debug\"] = debug\n</code></pre>"},{"location":"api/#cal_disp.cli.download","title":"download","text":""},{"location":"api/#cal_disp.cli.download.burst_bounds","title":"burst_bounds","text":"<pre><code>burst_bounds(input_file: Path, output_dir: Path) -&gt; None\n</code></pre> <p>Download S1 CSLC boundary tiles for a DISP-S1 file.</p> <p>Extracts frame ID and sensing times from the DISP-S1 filename, then downloads corresponding Sentinel-1 burst boundary geometries. Output directory is created if it doesn't exist.</p> <p>Only supports DISP-S1 products (sensor must be S1).</p> <p>Examples:</p> <p>Basic usage:     $ cal-disp download burst-bounds -i disp_product.nc -o ./burst_data</p> <p>Full path example:     $ cal-disp download burst-bounds \\     -i OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0.nc \\     -o ./burst_bounds</p> Source code in <code>src/cal_disp/cli/download.py</code> <pre><code>@download_group.command(name=\"burst-bounds\")\n@click.option(\n    \"--input-file\",\n    \"-i\",\n    type=click.Path(exists=True, dir_okay=False, path_type=Path),\n    required=True,\n    help=\"Path to OPERA DISP-S1 product (.nc file).\",\n)\n@click.option(\n    \"--output-dir\",\n    \"-o\",\n    type=click.Path(file_okay=False, path_type=Path),\n    required=True,\n    help=\"Directory to save burst boundary tiles.\",\n)\ndef burst_bounds(\n    input_file: Path,\n    output_dir: Path,\n) -&gt; None:\n    r\"\"\"Download S1 CSLC boundary tiles for a DISP-S1 file.\n\n    Extracts frame ID and sensing times from the DISP-S1 filename,\n    then downloads corresponding Sentinel-1 burst boundary geometries.\n    Output directory is created if it doesn't exist.\n\n    Only supports DISP-S1 products (sensor must be S1).\n\n    Examples\n    --------\n    Basic usage:\n        $ cal-disp download burst-bounds -i disp_product.nc -o ./burst_data\n\n    Full path example:\n        $ cal-disp download burst-bounds \\\\\n        -i OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0.nc \\\\\n        -o ./burst_bounds\n\n    \"\"\"\n    from cal_disp.download import generate_s1_burst_tiles\n\n    # Parse filename: OPERA_L3_DISP-S1_IW_F{frame}_VV_{dates}...\n    parts = input_file.stem.split(\"_\")\n    sensor = parts[2].split(\"-\")[1]  # DISP-S1 -&gt; S1\n    frame_id = int(parts[4].lstrip(\"F\"))  # F08882 -&gt; 8882\n\n    if sensor != \"S1\":\n        raise click.ClickException(f\"Only DISP-S1 products supported, got: {sensor}\")\n\n    sensing_times = extract_sensing_times_from_file(input_file)\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    for sensing_time in sensing_times:\n        click.echo(f\"Downloading burst bounds for {sensing_time}\")\n        generate_s1_burst_tiles(\n            frame_id=frame_id,\n            sensing_time=sensing_time,\n            output_dir=output_dir,\n        )\n\n    click.echo(f\"Download complete: {output_dir}\")\n</code></pre>"},{"location":"api/#cal_disp.cli.download.disp_s1","title":"disp_s1","text":"<pre><code>disp_s1(frame_id: int, output_dir: Path, start: datetime | None, end: datetime | None, num_workers: int) -&gt; None\n</code></pre> <p>Download OPERA DISP-S1 products for a frame.</p> <p>Downloads displacement products from the OPERA DISP-S1 archive for the specified frame and date range. Products are filtered based on the secondary date of each interferogram.</p> <p>Examples:</p> <p>Download all products for a frame:     $ cal-disp download disp-s1 --frame-id 8882 -o ./disp_data</p> <p>Download products for specific date range:     $ cal-disp download disp-s1 --frame-id 8882 -o ./disp_data \\         --start 2022-07-01 --end 2022-07-31</p> <p>Use more workers for faster downloads:     $ cal-disp download disp-s1 --frame-id 8882 -o ./disp_data -n 8</p> Source code in <code>src/cal_disp/cli/download.py</code> <pre><code>@download_group.command()\n@click.option(\n    \"--frame-id\",\n    type=int,\n    required=True,\n    help=\"OPERA DISP-S1 frame ID.\",\n    show_default=True,\n)\n@click.option(\n    \"--output-dir\",\n    \"-o\",\n    type=click.Path(file_okay=False, path_type=Path),\n    required=True,\n    help=\"Directory to save downloaded DISP products.\",\n)\n@click.option(\n    \"--start\",\n    \"-s\",\n    type=click.DateTime(formats=[\"%Y-%m-%d\"]),\n    default=None,\n    help=\"Start date (YYYY-MM-DD). Based on secondary date of DISP.\",\n)\n@click.option(\n    \"--end\",\n    \"-e\",\n    type=click.DateTime(formats=[\"%Y-%m-%d\"]),\n    default=None,\n    help=\"End date (YYYY-MM-DD). Based on secondary date of DISP.\",\n)\n@click.option(\n    \"--num-workers\",\n    \"-n\",\n    type=int,\n    default=1,\n    help=\"Number of parallel download workers.\",\n    show_default=True,\n)\ndef disp_s1(\n    frame_id: int,\n    output_dir: Path,\n    start: datetime | None,\n    end: datetime | None,\n    num_workers: int,\n) -&gt; None:\n    r\"\"\"Download OPERA DISP-S1 products for a frame.\n\n    Downloads displacement products from the OPERA DISP-S1 archive for\n    the specified frame and date range. Products are filtered based on\n    the secondary date of each interferogram.\n\n    Examples\n    --------\n    Download all products for a frame:\n        $ cal-disp download disp-s1 --frame-id 8882 -o ./disp_data\n\n    Download products for specific date range:\n        $ cal-disp download disp-s1 --frame-id 8882 -o ./disp_data \\\\\n            --start 2022-07-01 --end 2022-07-31\n\n    Use more workers for faster downloads:\n        $ cal-disp download disp-s1 --frame-id 8882 -o ./disp_data -n 8\n\n    \"\"\"\n    from cal_disp.download import download_disp\n\n    download_disp(\n        frame_id=frame_id,\n        output_dir=output_dir,\n        start=start,\n        end=end,\n        num_workers=num_workers,\n    )\n    click.echo(f\"Download complete: {output_dir}\")\n</code></pre>"},{"location":"api/#cal_disp.cli.download.download_group","title":"download_group","text":"<pre><code>download_group()\n</code></pre> <p>Sub-commands for downloading prerequisite data.</p> Source code in <code>src/cal_disp/cli/download.py</code> <pre><code>@click.group(name=\"download\")\ndef download_group():\n    \"\"\"Sub-commands for downloading prerequisite data.\"\"\"\n    from cal_disp._log import setup_logging\n\n    setup_logging(logger_name=\"cal_disp\")\n</code></pre>"},{"location":"api/#cal_disp.cli.download.tropo","title":"tropo","text":"<pre><code>tropo(input_file: Path, output_dir: Path, num_workers: int, interp: bool) -&gt; None\n</code></pre> <p>Download OPERA TROPO for a DISP-S1 file.</p> <p>Extracts sensing times from the input DISP-S1 product filename and downloads corresponding OPERA TROPO data. Output directory is created if it doesn't exist.</p> <p>The input file must follow OPERA naming convention: OPERA_L3_DISP-S1_IW_F{frame}VV{ref_date}{sec_date}_v1.0{proc_date}.nc</p> <p>Examples:</p> <p>Basic usage:     $ cal-disp download tropo -i disp_product.nc -o ./tropo_data</p> <p>With temporal interpolation:     $ cal-disp download tropo -i disp_product.nc -o ./tropo_data --interp</p> <p>Using more workers:     $ cal-disp download tropo -i disp_product.nc -o ./tropo_data -n 8</p> Source code in <code>src/cal_disp/cli/download.py</code> <pre><code>@download_group.command()\n@click.option(\n    \"--input-file\",\n    \"-i\",\n    type=click.Path(exists=True, dir_okay=False, path_type=Path),\n    required=True,\n    help=\"Path to OPERA DISP-S1 product (.nc file).\",\n)\n@click.option(\n    \"--output-dir\",\n    \"-o\",\n    type=click.Path(file_okay=False, path_type=Path),\n    required=True,\n    help=\"Directory to save downloaded TROPO products.\",\n)\n@click.option(\n    \"--num-workers\",\n    \"-n\",\n    type=int,\n    default=4,\n    help=\"Number of parallel download workers.\",\n    show_default=True,\n)\n@click.option(\n    \"--interp\",\n    is_flag=True,\n    help=\"Download 2 scenes per date for temporal interpolation.\",\n)\ndef tropo(\n    input_file: Path,\n    output_dir: Path,\n    num_workers: int,\n    interp: bool,\n) -&gt; None:\n    \"\"\"Download OPERA TROPO for a DISP-S1 file.\n\n    Extracts sensing times from the input DISP-S1 product filename and\n    downloads corresponding OPERA TROPO data. Output directory\n    is created if it doesn't exist.\n\n    The input file must follow OPERA naming convention:\n    OPERA_L3_DISP-S1_IW_F{frame}_VV_{ref_date}_{sec_date}_v1.0_{proc_date}.nc\n\n    Examples\n    --------\n    Basic usage:\n        $ cal-disp download tropo -i disp_product.nc -o ./tropo_data\n\n    With temporal interpolation:\n        $ cal-disp download tropo -i disp_product.nc -o ./tropo_data --interp\n\n    Using more workers:\n        $ cal-disp download tropo -i disp_product.nc -o ./tropo_data -n 8\n\n    \"\"\"\n    from cal_disp.download import download_tropo\n\n    sensing_times = extract_sensing_times_from_file(input_file)\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    download_tropo(\n        disp_times=sensing_times,\n        output_dir=output_dir,\n        num_workers=num_workers,\n        interp=interp,\n    )\n    click.echo(f\"Download complete: {output_dir}\")\n</code></pre>"},{"location":"api/#cal_disp.cli.download.unr","title":"unr","text":"<pre><code>unr(frame_id: int, output_dir: Path, start: datetime | None, end: datetime | None, margin: float) -&gt; None\n</code></pre> <p>Download UNR GPS timeseries data for a DISP-S1 frame.</p> <p>Downloads GPS timeseries grid from the Nevada Geodetic Laboratory (UNR) within the frame's bounding box (expanded by margin). Data is saved as a parquet file for efficient loading. Output directory is created if it doesn't exist.</p> <p>Examples:</p> <p>Download UNR data for a frame:     $ cal-disp download unr --frame-id 8882 -o ./unr_data</p> <p>With date range:     $ cal-disp download unr --frame-id 8882 -o ./unr_data \\         --start 2022-01-01 --end 2023-12-31</p> <p>Expand bounding box by 1 degree:     $ cal-disp download unr --frame-id 8882 -o ./unr_data -m 1.0</p> Source code in <code>src/cal_disp/cli/download.py</code> <pre><code>@download_group.command()\n@click.option(\n    \"--frame-id\",\n    type=int,\n    required=True,\n    help=\"OPERA DISP-S1 frame ID.\",\n    show_default=True,\n)\n@click.option(\n    \"--output-dir\",\n    \"-o\",\n    type=click.Path(file_okay=False, path_type=Path),\n    required=True,\n    help=\"Directory to save downloaded UNR parquet file.\",\n)\n@click.option(\n    \"--start\",\n    \"-s\",\n    type=click.DateTime(formats=[\"%Y-%m-%d\"]),\n    default=None,\n    help=\"Start date of timeseries (YYYY-MM-DD).\",\n)\n@click.option(\n    \"--end\",\n    \"-e\",\n    type=click.DateTime(formats=[\"%Y-%m-%d\"]),\n    default=None,\n    help=\"End date of timeseries (YYYY-MM-DD).\",\n)\n@click.option(\n    \"--margin\",\n    \"-m\",\n    type=float,\n    default=0.5,\n    help=\"Margin in degrees to expand frame bounding box.\",\n    show_default=True,\n)\ndef unr(\n    frame_id: int,\n    output_dir: Path,\n    start: datetime | None,\n    end: datetime | None,\n    margin: float,\n) -&gt; None:\n    r\"\"\"Download UNR GPS timeseries data for a DISP-S1 frame.\n\n    Downloads GPS timeseries grid from the Nevada Geodetic Laboratory (UNR)\n    within the frame's bounding box (expanded by margin). Data is saved\n    as a parquet file for efficient loading. Output directory is created\n    if it doesn't exist.\n\n    Examples\n    --------\n    Download UNR data for a frame:\n        $ cal-disp download unr --frame-id 8882 -o ./unr_data\n\n    With date range:\n        $ cal-disp download unr --frame-id 8882 -o ./unr_data \\\\\n            --start 2022-01-01 --end 2023-12-31\n\n    Expand bounding box by 1 degree:\n        $ cal-disp download unr --frame-id 8882 -o ./unr_data -m 1.0\n\n    \"\"\"\n    from cal_disp.download import download_unr_grid\n\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    download_unr_grid(\n        frame_id=frame_id,\n        output_dir=output_dir,\n        start=start,\n        end=end,\n        margin_deg=margin,\n    )\n    click.echo(f\"Download complete: {output_dir}\")\n</code></pre>"},{"location":"api/#download","title":"Download","text":""},{"location":"api/#cal_disp.download","title":"cal_disp.download","text":""},{"location":"api/#cal_disp.download.download_disp","title":"download_disp","text":"<pre><code>download_disp(frame_id: int, output_dir: Path, start: datetime | None = None, end: datetime | None = None, num_workers: int = DEFAULT_NUM_WORKERS) -&gt; None\n</code></pre> <p>Download DISP-S1 products for a frame.</p> <p>Downloads displacement products from the OPERA DISP-S1 archive for the specified frame and date range. Products are filtered based on the secondary date of each interferogram.</p> <p>Parameters:</p> Name Type Description Default <code>frame_id</code> <code>int</code> <p>OPERA frame identifier.</p> required <code>output_dir</code> <code>Path</code> <p>Directory where products will be saved.</p> required <code>start</code> <code>datetime or None</code> <p>Start date for query (based on secondary date). Default is None (no start limit).</p> <code>None</code> <code>end</code> <code>datetime or None</code> <p>End date for query (based on secondary date). Default is None (no end limit).</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>Number of parallel download workers. Default is 2.</p> <code>DEFAULT_NUM_WORKERS</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If frame ID is not in the database or no products are found.</p> Notes <p>Date queries are based on the secondary (later) date of each interferometric pair. If start and end dates are identical, the range is automatically expanded by \u00b11 day and num_workers is set to 1 to ensure the specific product is captured.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; download_frame_products(\n...     frame_id=8887,\n...     output_dir=Path(\"./data\"),\n...     start=datetime(2024, 1, 1),\n...     end=datetime(2024, 12, 31)\n... )\n</code></pre> Source code in <code>src/cal_disp/download/_stage_disp.py</code> <pre><code>def download_disp(\n    frame_id: int,\n    output_dir: Path,\n    start: datetime | None = None,\n    end: datetime | None = None,\n    num_workers: int = DEFAULT_NUM_WORKERS,\n) -&gt; None:\n    \"\"\"Download DISP-S1 products for a frame.\n\n    Downloads displacement products from the OPERA DISP-S1 archive for\n    the specified frame and date range. Products are filtered based on\n    the secondary date of each interferogram.\n\n    Parameters\n    ----------\n    frame_id : int\n        OPERA frame identifier.\n    output_dir : Path\n        Directory where products will be saved.\n    start : datetime or None, optional\n        Start date for query (based on secondary date).\n        Default is None (no start limit).\n    end : datetime or None, optional\n        End date for query (based on secondary date).\n        Default is None (no end limit).\n    num_workers : int, optional\n        Number of parallel download workers. Default is 2.\n\n    Raises\n    ------\n    ValueError\n        If frame ID is not in the database or no products are found.\n\n    Notes\n    -----\n    Date queries are based on the secondary (later) date of each\n    interferometric pair. If start and end dates are identical,\n    the range is automatically expanded by \u00b11 day and num_workers\n    is set to 1 to ensure the specific product is captured.\n\n    Examples\n    --------\n    &gt;&gt;&gt; download_frame_products(\n    ...     frame_id=8887,\n    ...     output_dir=Path(\"./data\"),\n    ...     start=datetime(2024, 1, 1),\n    ...     end=datetime(2024, 12, 31)\n    ... )\n\n    \"\"\"\n    logger.info(f\"Downloading DISP-S1 products for frame {frame_id}\")\n\n    start_adjusted, end_adjusted = _adjust_single_date_range(start, end)\n    logger.info(f\"Search window: {start_adjusted} - {end_adjusted}\")\n\n    workers = 1 if (start and end and start == end) else num_workers\n\n    run_download(\n        frame_id,\n        start_datetime=start_adjusted,\n        end_datetime=end_adjusted,\n        output_dir=Path(output_dir),\n        num_workers=workers,\n    )\n\n    logger.info(f\"Download complete: files saved to {output_dir}\")\n</code></pre>"},{"location":"api/#cal_disp.download.download_tropo","title":"download_tropo","text":"<pre><code>download_tropo(disp_times: list[datetime | Timestamp], output_dir: Path | str, num_workers: int = 4, interp: bool = True) -&gt; None\n</code></pre> <p>Download tropospheric correction data for displacement times.</p> <p>Parameters:</p> Name Type Description Default <code>disp_times</code> <code>list of datetime or pd.Timestamp</code> <p>Displacement measurement times</p> required <code>output_dir</code> <code>Path or str</code> <p>Output directory for downloads</p> required <code>num_workers</code> <code>int</code> <p>Parallel download workers</p> <code>4</code> <code>interp</code> <code>bool</code> <p>If True, get 2 scenes per time (for interpolation). If False, get single nearest scene.</p> <code>True</code> Source code in <code>src/cal_disp/download/_stage_tropo.py</code> <pre><code>def download_tropo(\n    disp_times: list[datetime | pd.Timestamp],\n    output_dir: Path | str,\n    num_workers: int = 4,\n    interp: bool = True,\n) -&gt; None:\n    \"\"\"Download tropospheric correction data for displacement times.\n\n    Parameters\n    ----------\n    disp_times : list of datetime or pd.Timestamp\n        Displacement measurement times\n    output_dir : Path or str\n        Output directory for downloads\n    num_workers : int\n        Parallel download workers\n    interp : bool\n        If True, get 2 scenes per time (for interpolation).\n        If False, get single nearest scene.\n\n    \"\"\"\n    out = Path(output_dir)\n    out.mkdir(exist_ok=True, parents=True)\n\n    n_scenes = 2 if interp else 1\n    all_scenes = []\n\n    for t in disp_times:\n        scenes = find_nearest_scenes(t, num_scenes=n_scenes)\n        all_scenes.extend(scenes)\n\n    combined = asf.ASFSearchResults(all_scenes)\n    logger.info(f\"Downloading {len(combined)} scenes to {out}\")\n    combined.download(path=out, processes=num_workers)\n</code></pre>"},{"location":"api/#cal_disp.download.download_unr_grid","title":"download_unr_grid","text":"<pre><code>download_unr_grid(frame_id: int, output_dir: Path, start: datetime | None = None, end: datetime | None = None, margin_deg: float = 0.5, plate: Literal['NA', 'PA', 'IGS14', 'IGS20'] = 'IGS20', version: Literal['0.1', '0.2'] = '0.2') -&gt; None\n</code></pre> <p>Download UNR gridded GNSS timeseries for a given frame.</p> <p>Parameters:</p> Name Type Description Default <code>frame_id</code> <code>int</code> <p>OPERA frame identifier.</p> required <code>output_dir</code> <code>Path</code> <p>Output directory for downloaded data.</p> required <code>start</code> <code>datetime or None</code> <p>Start date for timeseries. If None, downloads from beginning.</p> <code>None</code> <code>end</code> <code>datetime or None</code> <p>End date for timeseries. If None, downloads until present.</p> <code>None</code> <code>margin_deg</code> <code>float</code> <p>Margin in degrees to expand frame bounding box.</p> <code>0.5</code> <code>plate</code> <code>(NA, PA, IGS14, IGS20)</code> <p>Reference plate for velocity computation.</p> <code>\"NA\"</code> <code>version</code> <code>(0.1, 0.2)</code> <p>UNR grid version to download.</p> <code>\"0.1\"</code> Source code in <code>src/cal_disp/download/_stage_unr.py</code> <pre><code>def download_unr_grid(\n    frame_id: int,\n    output_dir: Path,\n    start: datetime | None = None,\n    end: datetime | None = None,\n    margin_deg: float = 0.5,\n    plate: Literal[\"NA\", \"PA\", \"IGS14\", \"IGS20\"] = \"IGS20\",\n    version: Literal[\"0.1\", \"0.2\"] = \"0.2\",\n) -&gt; None:\n    \"\"\"Download UNR gridded GNSS timeseries for a given frame.\n\n    Parameters\n    ----------\n    frame_id : int\n        OPERA frame identifier.\n    output_dir : Path\n        Output directory for downloaded data.\n    start : datetime or None, optional\n        Start date for timeseries. If None, downloads from beginning.\n    end : datetime or None, optional\n        End date for timeseries. If None, downloads until present.\n    margin_deg : float, default=0.5\n        Margin in degrees to expand frame bounding box.\n    plate : {\"NA\", \"PA\", \"IGS14\", \"IGS20\"}, default=\"IGS20\"\n        Reference plate for velocity computation.\n    version : {\"0.1\", \"0.2\"}, default=\"0.2\"\n        UNR grid version to download.\n\n    \"\"\"\n    logger.info(\n        f\"Downloading UNR gridded GNSS timeseries for frame {frame_id} \"\n        f\"from {start or 'beginning'} to {end or 'present'}\"\n    )\n\n    selected_frame = get_frame_geojson([frame_id], as_geodataframe=True)\n    frame_bounds = tuple(selected_frame.bounds.values[0])\n\n    extended_bbox = (\n        frame_bounds[0] - margin_deg,\n        frame_bounds[1] - margin_deg,\n        frame_bounds[2] + margin_deg,\n        frame_bounds[3] + margin_deg,\n    )\n\n    grid = UnrGridSource(version=version)\n    ts_grid_df = grid.timeseries_many(bbox=extended_bbox)\n    ts_grid_df[\"date\"] = pd.to_datetime(ts_grid_df[\"date\"])\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    _save_parquet(ts_grid_df, frame_id, plate, output_dir)\n</code></pre>"},{"location":"api/#cal_disp.download.generate_s1_burst_tiles","title":"generate_s1_burst_tiles","text":"<pre><code>generate_s1_burst_tiles(frame_id: int, sensing_time: datetime, output_dir: Path, time_window_hours: float = 2.0, n_download_processes: int = 5) -&gt; Path\n</code></pre> <p>Generate non-overlapping burst tiles for a frame and sensing time.</p> <p>Downloads CSLC data, processes bursts to create non-overlapping polygons, and saves to GeoJSON. Priority: IW1 &gt; IW2 &gt; IW3, lower burst_id first.</p> <p>Parameters:</p> Name Type Description Default <code>frame_id</code> <code>int</code> <p>OPERA frame identifier.</p> required <code>sensing_time</code> <code>datetime</code> <p>Sensing time to search for CSLC products.</p> required <code>output_dir</code> <code>Path</code> <p>Directory to save output GeoJSON and temporary files.</p> required <code>time_window_hours</code> <code>float</code> <p>Time window in hours for searching CSLC products. Default is 2.0.</p> <code>2.0</code> <code>n_download_processes</code> <code>int</code> <p>Number of parallel download processes. Default is 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the generated GeoJSON file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no bursts are found or download fails.</p> Source code in <code>src/cal_disp/download/_stage_burst_bounds.py</code> <pre><code>def generate_s1_burst_tiles(\n    frame_id: int,\n    sensing_time: datetime,\n    output_dir: Path,\n    time_window_hours: float = 2.0,\n    n_download_processes: int = 5,\n) -&gt; Path:\n    \"\"\"Generate non-overlapping burst tiles for a frame and sensing time.\n\n    Downloads CSLC data, processes bursts to create non-overlapping polygons,\n    and saves to GeoJSON. Priority: IW1 &gt; IW2 &gt; IW3, lower burst_id first.\n\n    Parameters\n    ----------\n    frame_id : int\n        OPERA frame identifier.\n    sensing_time : datetime\n        Sensing time to search for CSLC products.\n    output_dir : Path\n        Directory to save output GeoJSON and temporary files.\n    time_window_hours : float, optional\n        Time window in hours for searching CSLC products. Default is 2.0.\n    n_download_processes : int, optional\n        Number of parallel download processes. Default is 5.\n\n    Returns\n    -------\n    Path\n        Path to the generated GeoJSON file.\n\n    Raises\n    ------\n    ValueError\n        If no bursts are found or download fails.\n\n    \"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.info(f\"Starting burst tile generation for frame {frame_id}\")\n\n    # Get burst IDs and EPSG for frame\n    burst_ids = opera_utils.get_burst_ids_for_frame(frame_id)\n    burst_ids = [b.upper() for b in burst_ids]\n    logger.info(f\"Found {len(burst_ids)} burst IDs for frame {frame_id}\")\n\n    epsg = opera_utils.get_frame_bbox(frame_id)[0]\n    logger.debug(f\"Target CRS for frame: EPSG:{epsg}\")\n\n    # Search for CSLC products\n    results = search_cslc_bursts(burst_ids, sensing_time, time_window_hours)\n    logger.info(f\"Found {len(results)} CSLC products\")\n\n    # Download files\n    target_date = sensing_time.date()\n    cslc_files = download_cslc_files(\n        results, output_dir, target_date, n_download_processes\n    )\n    logger.info(f\"Downloaded {len(cslc_files)} CSLC files\")\n\n    # Process bounds\n    cslc_gdf = process_cslc_bounds(cslc_files, epsg=epsg)\n\n    # Create non-overlapping tiles\n    burst_gdf = create_nonoverlapping_tiles(cslc_gdf)\n\n    # Log summary statistics\n    swath_counts = burst_gdf.groupby(\"swath\").size()\n    logger.info(f\"Generated {len(burst_gdf)} non-overlapping tiles\")\n    for swath_num, count in swath_counts.items():\n        logger.info(f\"  IW{swath_num}: {count} tiles\")\n\n    # Save to GeoJSON\n    output_file = output_dir / f\"{target_date}_tiles.geojson\"\n    burst_gdf.to_file(output_file)\n    logger.info(f\"Saved tiles to {output_file}\")\n\n    # Clean up temp directory\n    temp_dir = output_dir / \"tmp\"\n    if temp_dir.exists():\n        shutil.rmtree(temp_dir)\n        logger.debug(f\"Cleaned up temporary directory: {temp_dir}\")\n\n    return output_file\n</code></pre>"},{"location":"api/#cal_disp.download.utils","title":"utils","text":""},{"location":"api/#cal_disp.download.utils.extract_sensing_times_from_file","title":"extract_sensing_times_from_file","text":"<pre><code>extract_sensing_times_from_file(disp_file: Path) -&gt; list[datetime]\n</code></pre> <p>Extract sensing times from DISP-S1 NetCDF filename.</p> <p>Parses DISP-S1 filename to extract reference and secondary dates. Expected format: OPERA_L3_DISP-S1_IW_F{frame}VV{ref_date}{sec_date}_v{version}{prod_date}.nc</p> <p>Parameters:</p> Name Type Description Default <code>disp_file</code> <code>Path</code> <p>Path to DISP-S1 NetCDF file.</p> required <p>Returns:</p> Type Description <code>list[datetime]</code> <p>List of unique sensing times from reference and secondary dates.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If DISP file does not exist.</p> <code>ValueError</code> <p>If dates cannot be parsed from filename.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; filename = Path(\"OPERA_L3_*.nc\")\n&gt;&gt;&gt; times = extract_sensing_times_from_file(filename)\n&gt;&gt;&gt; len(times)\n2\n</code></pre> Source code in <code>src/cal_disp/download/utils.py</code> <pre><code>def extract_sensing_times_from_file(disp_file: Path) -&gt; list[datetime]:\n    \"\"\"Extract sensing times from DISP-S1 NetCDF filename.\n\n    Parses DISP-S1 filename to extract reference and secondary dates.\n    Expected format:\n    OPERA_L3_DISP-S1_IW_F{frame}_VV_{ref_date}_{sec_date}_v{version}_{prod_date}.nc\n\n    Parameters\n    ----------\n    disp_file : Path\n        Path to DISP-S1 NetCDF file.\n\n    Returns\n    -------\n    list[datetime]\n        List of unique sensing times from reference and secondary dates.\n\n    Raises\n    ------\n    FileNotFoundError\n        If DISP file does not exist.\n    ValueError\n        If dates cannot be parsed from filename.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt; filename = Path(\"OPERA_L3_*.nc\")\n    &gt;&gt;&gt; times = extract_sensing_times_from_file(filename)  # doctest: +SKIP\n    &gt;&gt;&gt; len(times)  # doctest: +SKIP\n    2\n\n    \"\"\"\n    if not disp_file.exists():\n        msg = f\"DISP file not found: {disp_file}\"\n        raise FileNotFoundError(msg)\n\n    logger.info(f\"Extracting sensing times from {disp_file.name}\")\n\n    filename = disp_file.name\n    parts = filename.split(\"_\")\n\n    # Find parts matching datetime format (YYYYMMDDTHHMMSSZ)\n    datetime_parts = [p for p in parts if len(p) == 16 and p.endswith(\"Z\") and \"T\" in p]\n\n    if len(datetime_parts) &lt; 2:\n        msg = (\n            f\"Cannot parse reference and secondary dates from filename: {filename}. \"\n            \"Expected format: \"\n            \"OPERA_L3_DISP-S1_IW_F{{frame}}_VV_{{ref_date}}_{{sec_date}}_\"\n            \"v{{version}}_{{prod_date}}.nc\"\n        )\n        raise ValueError(msg)\n\n    ref_date_str = datetime_parts[0]\n    sec_date_str = datetime_parts[1]\n\n    try:\n        ref_date = datetime.strptime(ref_date_str, \"%Y%m%dT%H%M%SZ\")\n        sec_date = datetime.strptime(sec_date_str, \"%Y%m%dT%H%M%SZ\")\n    except ValueError as e:\n        msg = f\"Failed to parse dates from filename {filename}: {e}\"\n        raise ValueError(msg) from e\n\n    sensing_times = sorted({ref_date, sec_date})\n    logger.info(\n        f\"Parsed sensing times: {ref_date.isoformat()} (ref), \"\n        f\"{sec_date.isoformat()} (sec)\"\n    )\n\n    return sensing_times\n</code></pre>"},{"location":"api/#product","title":"Product","text":""},{"location":"api/#cal_disp.product","title":"cal_disp.product","text":""},{"location":"api/#cal_disp.product.CalProduct","title":"CalProduct  <code>dataclass</code>","text":"<p>Calibrated OPERA DISP displacement product.</p> <p>Represents a calibration correction product that should be subtracted from OPERA DISP products. Main group contains calibration at full resolution. Optional model_3d group contains 3D displacement components at coarser resolution.</p> Groups <p>Main group:     - calibration: Correction to subtract from DISP (full resolution)     - calibration_std: Calibration uncertainty (full resolution)</p> <p>model_3d group (optional):     - north_south: North-south displacement component (coarse resolution)     - east_west: East-west displacement component (coarse resolution)     - up_down: Up-down displacement component (coarse resolution)     - north_south_std: Uncertainty in north-south     - east_west_std: Uncertainty in east-west     - up_down_std: Uncertainty in up-down</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the calibration product NetCDF file.</p> required <code>frame_id</code> <code>int</code> <p>OPERA frame identifier.</p> required <code>primary_date</code> <code>datetime</code> <p>Earlier acquisition date (reference).</p> required <code>secondary_date</code> <code>datetime</code> <p>Later acquisition date.</p> required <code>polarization</code> <code>str</code> <p>Radar polarization (e.g., \"VV\", \"VH\").</p> required <code>sensor</code> <code>str</code> <p>Sensor type: \"S1\" (Sentinel-1) or \"NI\" (NISAR).</p> required <code>version</code> <code>str</code> <p>Product version string.</p> required <code>production_date</code> <code>datetime</code> <p>Date when product was generated.</p> required <code>mode</code> <code>str</code> <p>Acquisition mode (e.g., \"IW\" for S1, \"LSAR\" for NI).</p> <code>'IW'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create product with both groups\n&gt;&gt;&gt; cal = CalProduct.create(\n...     calibration=cal_correction,\n...     disp_product=disp,\n...     output_dir=\"output/\",\n...     model_3d={\"north_south\": vel_ns, \"east_west\": vel_ew, \"up_down\": vel_ud},\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Access main calibration\n&gt;&gt;&gt; ds_main = cal.open_dataset()\n&gt;&gt;&gt; calibration = ds_main[\"calibration\"]\n</code></pre> <pre><code>&gt;&gt;&gt; # Access 3D model (coarse resolution)\n&gt;&gt;&gt; ds_model = cal.open_model_3d()\n&gt;&gt;&gt; model_up = ds_model[\"up_down\"]\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>@dataclass\nclass CalProduct:\n    \"\"\"Calibrated OPERA DISP displacement product.\n\n    Represents a calibration correction product that should be subtracted\n    from OPERA DISP products. Main group contains calibration at full\n    resolution. Optional model_3d group contains 3D displacement\n    components at coarser resolution.\n\n    Groups\n    ------\n    Main group:\n        - calibration: Correction to subtract from DISP (full resolution)\n        - calibration_std: Calibration uncertainty (full resolution)\n\n    model_3d group (optional):\n        - north_south: North-south displacement component (coarse resolution)\n        - east_west: East-west displacement component (coarse resolution)\n        - up_down: Up-down displacement component (coarse resolution)\n        - north_south_std: Uncertainty in north-south\n        - east_west_std: Uncertainty in east-west\n        - up_down_std: Uncertainty in up-down\n\n    Parameters\n    ----------\n    path : Path\n        Path to the calibration product NetCDF file.\n    frame_id : int\n        OPERA frame identifier.\n    primary_date : datetime\n        Earlier acquisition date (reference).\n    secondary_date : datetime\n        Later acquisition date.\n    polarization : str\n        Radar polarization (e.g., \"VV\", \"VH\").\n    sensor : str\n        Sensor type: \"S1\" (Sentinel-1) or \"NI\" (NISAR).\n    version : str\n        Product version string.\n    production_date : datetime\n        Date when product was generated.\n    mode : str\n        Acquisition mode (e.g., \"IW\" for S1, \"LSAR\" for NI).\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Create product with both groups\n    &gt;&gt;&gt; cal = CalProduct.create(\n    ...     calibration=cal_correction,\n    ...     disp_product=disp,\n    ...     output_dir=\"output/\",\n    ...     model_3d={\"north_south\": vel_ns, \"east_west\": vel_ew, \"up_down\": vel_ud},\n    ... )\n\n    &gt;&gt;&gt; # Access main calibration\n    &gt;&gt;&gt; ds_main = cal.open_dataset()\n    &gt;&gt;&gt; calibration = ds_main[\"calibration\"]\n\n    &gt;&gt;&gt; # Access 3D model (coarse resolution)\n    &gt;&gt;&gt; ds_model = cal.open_model_3d()\n    &gt;&gt;&gt; model_up = ds_model[\"up_down\"]\n\n    \"\"\"\n\n    path: Path\n    frame_id: int\n    primary_date: datetime\n    secondary_date: datetime\n    polarization: str\n    sensor: str\n    version: str\n    production_date: datetime\n    mode: str = \"IW\"\n\n    # Filename pattern supporting both S1 and NI sensors\n    _PATTERN = re.compile(\n        r\"OPERA_L4_CAL-DISP-(?P&lt;sensor&gt;S1|NI)_\"\n        r\"(?P&lt;mode&gt;\\w+)_\"\n        r\"F(?P&lt;frame_id&gt;\\d+)_\"\n        r\"(?P&lt;pol&gt;\\w+)_\"\n        r\"(?P&lt;primary&gt;\\d{8}T\\d{6}Z)_\"\n        r\"(?P&lt;secondary&gt;\\d{8}T\\d{6}Z)_\"\n        r\"v(?P&lt;version&gt;[\\d.]+)_\"\n        r\"(?P&lt;production&gt;\\d{8}T\\d{6}Z)\"\n        r\"\\.nc$\"\n    )\n\n    # Main group layers (full resolution)\n    CAL_LAYERS = [\n        \"calibration\",  # Main correction (subtract from DISP)\n        \"calibration_std\",  # Uncertainty\n    ]\n\n    # model_3d group layers (coarse resolution)\n    MODEL_3D_LAYERS = [\n        \"north_south\",  # North-south displacement component\n        \"east_west\",  # East-west displacement component\n        \"up_down\",  # Up-down displacement component\n        \"north_south_std\",  # Uncertainty - north\n        \"east_west_std\",  # Uncertainty - east\n        \"up_down_std\",  # Uncertainty - up\n    ]\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate product after construction.\"\"\"\n        self.path = Path(self.path)\n\n        if self.frame_id &lt;= 0:\n            raise ValueError(f\"frame_id must be positive, got {self.frame_id}\")\n\n        if self.secondary_date &lt;= self.primary_date:\n            raise ValueError(\n                f\"Secondary date ({self.secondary_date}) must be after \"\n                f\"primary date ({self.primary_date})\"\n            )\n\n        if self.polarization not in {\"VV\", \"VH\", \"HH\", \"HV\"}:\n            raise ValueError(f\"Invalid polarization: {self.polarization}\")\n\n        if self.sensor not in {\"S1\", \"NI\"}:\n            raise ValueError(f\"Invalid sensor: {self.sensor}. Must be 'S1' or 'NI'\")\n\n    @classmethod\n    def from_path(cls, path: Path | str) -&gt; \"CalProduct\":\n        \"\"\"Parse product metadata from filename.\n\n        Parameters\n        ----------\n        path : Path or str\n            Path to calibration product NetCDF file.\n\n        Returns\n        -------\n        CalProduct\n            Parsed calibration product instance.\n\n        Raises\n        ------\n        ValueError\n            If filename doesn't match OPERA CAL-DISP pattern.\n\n        Examples\n        --------\n        &gt;&gt;&gt; cal = CalProduct.from_path(\n        \"OPERA_L4_CAL-DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251227T123456Z.nc\")\n        &gt;&gt;&gt; cal.sensor\n        'S1'\n\n        \"\"\"\n        path = Path(path)\n        match = cls._PATTERN.match(path.name)\n\n        if not match:\n            raise ValueError(\n                f\"Filename does not match OPERA CAL-DISP pattern: {path.name}\"\n            )\n\n        return cls(\n            path=path,\n            frame_id=int(match.group(\"frame_id\")),\n            primary_date=datetime.strptime(match.group(\"primary\"), \"%Y%m%dT%H%M%SZ\"),\n            secondary_date=datetime.strptime(\n                match.group(\"secondary\"), \"%Y%m%dT%H%M%SZ\"\n            ),\n            polarization=match.group(\"pol\"),\n            sensor=match.group(\"sensor\"),\n            version=match.group(\"version\"),\n            production_date=datetime.strptime(\n                match.group(\"production\"), \"%Y%m%dT%H%M%SZ\"\n            ),\n            mode=match.group(\"mode\"),\n        )\n\n    @classmethod\n    def create(\n        cls,\n        calibration: xr.DataArray,\n        disp_product: \"DispProduct\",\n        output_dir: Path | str,\n        sensor: str = \"S1\",\n        calibration_std: xr.DataArray | None = None,\n        model_3d: dict[str, xr.DataArray] | None = None,\n        model_3d_std: dict[str, xr.DataArray] | None = None,\n        metadata: dict[str, str] | None = None,\n        version: str = \"1.0\",\n    ) -&gt; \"CalProduct\":\n        \"\"\"Create calibration product with optional model_3d group.\n\n        Parameters\n        ----------\n        calibration : xr.DataArray\n            Calibration correction at full DISP resolution.\n        disp_product : DispProduct\n            Original DISP product (for metadata).\n        output_dir : Path or str\n            Output directory for NetCDF file.\n        sensor : str, optional\n            Sensor type: \"S1\" or \"NI\". Default is \"S1\".\n        calibration_std : xr.DataArray or None, optional\n            Calibration uncertainty at full resolution. Default is None.\n        model_3d : dict[str, xr.DataArray] or None, optional\n            3D displacement components (coarse resolution) with keys:\n            \"north_south\", \"east_west\", \"up_down\". Default is None.\n        model_3d_std : dict[str, xr.DataArray] or None, optional\n            3D displacement uncertainties (coarse resolution). Default is None.\n        metadata : dict[str, str] or None, optional\n            Additional metadata (e.g., GNSS reference epoch). Default is None.\n        version : str, optional\n            Product version. Default is \"1.0\".\n\n        Returns\n        -------\n        CalProduct\n            Created calibration product.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from product import DispProduct, CalProduct\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; disp = DispProduct.from_path(\n        \"OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Full resolution calibration\n        &gt;&gt;&gt; cal_full = calibration_at_30m_resolution\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Coarse resolution 3D model (e.g., 90m from GNSS interpolation)\n        &gt;&gt;&gt; model_coarse = {\n        ...     \"north_south\": disp_ns_90m,\n        ...     \"east_west\": disp_ew_90m,\n        ...     \"up_down\": disp_ud_90m,\n        ... }\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; cal = CalProduct.create(\n        ...     calibration=cal_full,\n        ...     disp_product=disp,\n        ...     output_dir=\"output/\",\n        ...     calibration_std=cal_std,\n        ...     model_3d=model_coarse,\n        ...     model_3d_std={\n        ...         \"north_south_std\": disp_ns_std_90m,\n        ...         \"east_west_std\": disp_ew_std_90m,\n        ...         \"up_down_std\": disp_ud_std_90m,\n        ...     },\n        ...     metadata={\n        ...         \"gnss_reference_epoch\": \"2020-01-01T00:00:00Z\",\n        ...         \"model_3d_resolution\": \"90m\",\n        ...     },\n        ... )\n\n        \"\"\"\n        if sensor not in {\"S1\", \"NI\"}:\n            raise ValueError(f\"Invalid sensor: {sensor}. Must be 'S1' or 'NI'\")\n\n        output_dir = Path(output_dir)\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Generate OPERA-compliant filename\n        production_date = datetime.utcnow()\n        filename = (\n            f\"OPERA_L4_CAL-DISP-{sensor}_\"\n            f\"{disp_product.mode}_\"\n            f\"F{disp_product.frame_id:05d}_\"\n            f\"{disp_product.polarization}_\"\n            f\"{disp_product.primary_date:%Y%m%dT%H%M%S}Z_\"\n            f\"{disp_product.secondary_date:%Y%m%dT%H%M%S}Z_\"\n            f\"v{version}_\"\n            f\"{production_date:%Y%m%dT%H%M%S}Z\"\n            \".nc\"\n        )\n\n        output_path = output_dir / filename\n\n        # Build main group dataset (full resolution)\n        data_vars = {\"calibration\": calibration}\n\n        if calibration_std is not None:\n            data_vars[\"calibration_std\"] = calibration_std\n\n        # Create main dataset\n        ds = xr.Dataset(data_vars)\n\n        # Add global attributes\n        ds.attrs.update(\n            {\n                \"product_type\": f\"OPERA_L4_CAL-DISP-{sensor}\",\n                \"sensor\": sensor,\n                \"frame_id\": disp_product.frame_id,\n                \"mode\": disp_product.mode,\n                \"polarization\": disp_product.polarization,\n                \"primary_datetime\": disp_product.primary_date.isoformat(),\n                \"secondary_datetime\": disp_product.secondary_date.isoformat(),\n                \"production_datetime\": production_date.isoformat(),\n                \"product_version\": version,\n                \"description\": (\n                    f\"Calibration correction for {sensor} InSAR displacement (subtract\"\n                    \" from DISP)\"\n                ),\n                \"source_product\": disp_product.filename,\n                \"usage\": (\n                    \"Subtract calibration layer from DISP displacement to obtain\"\n                    \" calibrated displacement\"\n                ),\n            }\n        )\n\n        # Add custom metadata\n        if metadata:\n            ds.attrs.update(metadata)\n\n        # Save main group\n        ds.to_netcdf(output_path, engine=\"h5netcdf\")\n\n        # Create model_3d group if 3D components provided (coarse resolution)\n        if model_3d or model_3d_std:\n            model_data_vars = {}\n\n            # Add 3D displacement components\n            if model_3d:\n                for comp in [\"north_south\", \"east_west\", \"up_down\"]:\n                    if comp in model_3d:\n                        model_data_vars[comp] = model_3d[comp]\n\n            # Add 3D displacement uncertainties\n            if model_3d_std:\n                for comp in [\"north_south_std\", \"east_west_std\", \"up_down_std\"]:\n                    if comp in model_3d_std:\n                        model_data_vars[comp] = model_3d_std[comp]\n\n            if model_data_vars:\n                ds_model = xr.Dataset(model_data_vars)\n\n                # Add model group attributes\n                ds_model.attrs.update(\n                    {\n                        \"description\": (\n                            \"3D displacement model at coarse resolution (e.g., from\"\n                            \" GNSS interpolation or deformation model)\"\n                        ),\n                        \"units\": \"meters\",\n                        \"reference_frame\": \"ENU (East-North-Up)\",\n                    }\n                )\n\n                # Append to existing file as model_3d group\n                ds_model.to_netcdf(\n                    output_path,\n                    mode=\"a\",\n                    group=\"model_3d\",\n                    engine=\"h5netcdf\",\n                )\n\n        return cls(\n            path=output_path,\n            frame_id=disp_product.frame_id,\n            primary_date=disp_product.primary_date,\n            secondary_date=disp_product.secondary_date,\n            polarization=disp_product.polarization,\n            sensor=sensor,\n            version=version,\n            production_date=production_date,\n            mode=disp_product.mode,\n        )\n\n    def open_dataset(self, group: str | None = None) -&gt; xr.Dataset:\n        \"\"\"Open calibration dataset.\n\n        Parameters\n        ----------\n        group : str or None, optional\n            Group to open: None for main, \"model_3d\" for 3D model.\n            Default is None (main group).\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing requested group.\n\n        Raises\n        ------\n        FileNotFoundError\n            If product file does not exist.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Open main calibration (full resolution)\n        &gt;&gt;&gt; ds_main = cal.open_dataset()\n        &gt;&gt;&gt; calibration = ds_main[\"calibration\"]\n\n        &gt;&gt;&gt; # Open model_3d group (coarse resolution)\n        &gt;&gt;&gt; ds_model = cal.open_dataset(group=\"model_3d\")\n        &gt;&gt;&gt; model_up = ds_model[\"up_down\"]\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n        if group == \"model_3d\":\n            return xr.open_dataset(self.path, group=\"model_3d\", engine=\"h5netcdf\")\n\n        return xr.open_dataset(self.path, engine=\"h5netcdf\")\n\n    def open_model_3d(self) -&gt; xr.Dataset:\n        \"\"\"Open model_3d group dataset.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing 3D displacement model at coarse resolution.\n\n        Raises\n        ------\n        FileNotFoundError\n            If product file does not exist.\n        ValueError\n            If model_3d group does not exist.\n\n        Examples\n        --------\n        &gt;&gt;&gt; ds_model = cal.open_model_3d()\n        &gt;&gt;&gt; disp_ns = ds_model[\"north_south\"]\n        &gt;&gt;&gt; disp_ew = ds_model[\"east_west\"]\n        &gt;&gt;&gt; disp_up = ds_model[\"up_down\"]\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n        try:\n            return xr.open_dataset(self.path, group=\"model_3d\", engine=\"h5netcdf\")\n        except (OSError, ValueError) as e:\n            raise ValueError(\n                f\"model_3d group not found in {self.filename}. \"\n                \"Product may not contain 3D displacement model.\"\n            ) from e\n\n    def has_model_3d(self) -&gt; bool:\n        \"\"\"Check if product contains model_3d group.\n\n        Returns\n        -------\n        bool\n            True if model_3d group exists.\n\n        \"\"\"\n        try:\n            self.open_model_3d()\n            return True\n        except (FileNotFoundError, ValueError):\n            return False\n\n    def get_epsg(self) -&gt; int | None:\n        \"\"\"Get EPSG code from spatial reference.\"\"\"\n        ds = self.open_dataset()\n\n        if \"spatial_ref\" in ds:\n            crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n            if crs_wkt:\n                crs = CRS.from_wkt(crs_wkt)\n                return crs.to_epsg()\n\n        return None\n\n    def get_bounds(self) -&gt; dict[str, float]:\n        \"\"\"Get bounds in native projection.\"\"\"\n        ds = self.open_dataset()\n\n        x = ds.x.values\n        y = ds.y.values\n\n        return {\n            \"left\": float(x.min()),\n            \"bottom\": float(y.min()),\n            \"right\": float(x.max()),\n            \"top\": float(y.max()),\n        }\n\n    def get_bounds_wgs84(self) -&gt; dict[str, float]:\n        \"\"\"Get bounds transformed to WGS84.\"\"\"\n        ds = self.open_dataset()\n\n        x = ds.x.values\n        y = ds.y.values\n        left = float(x.min())\n        bottom = float(y.min())\n        right = float(x.max())\n        top = float(y.max())\n\n        if \"spatial_ref\" not in ds:\n            raise ValueError(\"Dataset missing spatial_ref\")\n\n        crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n        if not crs_wkt:\n            raise ValueError(\"spatial_ref missing crs_wkt\")\n\n        src_crs = CRS.from_wkt(crs_wkt)\n\n        west, south, east, north = transform_bounds(\n            src_crs,\n            CRS.from_epsg(4326),\n            left,\n            bottom,\n            right,\n            top,\n        )\n\n        return {\n            \"west\": west,\n            \"south\": south,\n            \"east\": east,\n            \"north\": north,\n        }\n\n    def to_geotiff(\n        self,\n        layer: str,\n        output_path: Path | str,\n        group: str | None = None,\n        compress: str = \"DEFLATE\",\n        **kwargs,\n    ) -&gt; Path:\n        \"\"\"Export layer to GeoTIFF.\n\n        Parameters\n        ----------\n        layer : str\n            Name of layer to export.\n        output_path : Path or str\n            Output GeoTIFF path.\n        group : str or None, optional\n            Group containing layer. Default is None (main group).\n        compress : str, optional\n            Compression method. Default is \"DEFLATE\".\n        **kwargs\n            Additional rasterio creation options.\n\n        Returns\n        -------\n        Path\n            Path to created GeoTIFF.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Export main calibration\n        &gt;&gt;&gt; cal.to_geotiff(\"calibration\", \"calibration.tif\")\n\n        &gt;&gt;&gt; # Export 3D model component\n        &gt;&gt;&gt; cal.to_geotiff(\"up_down\", \"model_up.tif\", group=\"model_3d\")\n\n        \"\"\"\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        ds = self.open_dataset(group=group)\n\n        if layer not in ds:\n            available = list(ds.data_vars)\n            group_str = f\" in {group} group\" if group else \"\"\n            raise ValueError(\n                f\"Layer '{layer}' not found{group_str}. Available: {available}\"\n            )\n\n        da = ds[layer]\n        data = da.values\n\n        # Extract spatial information\n        if \"spatial_ref\" in ds:\n            transform = self._get_transform(ds)\n            crs = self._get_crs(ds)\n        else:\n            transform = Affine.translation(\n                float(ds.x.values[0]),\n                float(ds.y.values[0]),\n            ) * Affine.scale(\n                float(ds.x.values[1] - ds.x.values[0]),\n                float(ds.y.values[1] - ds.y.values[0]),\n            )\n            crs = None\n\n        # Write GeoTIFF\n        profile = {\n            \"driver\": \"GTiff\",\n            \"height\": data.shape[0],\n            \"width\": data.shape[1],\n            \"count\": 1,\n            \"dtype\": np.float32,\n            \"transform\": transform,\n            \"compress\": compress,\n            \"tiled\": True,\n            \"blockxsize\": 512,\n            \"blockysize\": 512,\n            **kwargs,\n        }\n\n        if crs:\n            profile[\"crs\"] = crs\n\n        with rasterio.open(output_path, \"w\", **profile) as dst:\n            dst.write(data.astype(np.float32), 1)\n            dst.set_band_description(1, layer)\n\n            # Add OPERA metadata tags\n            dst.update_tags(\n                product_type=f\"OPERA_L4_CAL-DISP-{self.sensor}\",\n                sensor=self.sensor,\n                frame_id=self.frame_id,\n                polarization=self.polarization,\n                primary_date=self.primary_date.isoformat(),\n                secondary_date=self.secondary_date.isoformat(),\n                layer=layer,\n                group=group if group else \"main\",\n            )\n\n        return output_path\n\n    def get_calibration_summary(self) -&gt; dict[str, dict[str, float]]:\n        \"\"\"Get summary statistics of all layers.\n\n        Returns\n        -------\n        dict[str, dict[str, float]]\n            Statistics for main and model_3d groups.\n\n        Examples\n        --------\n        &gt;&gt;&gt; summary = cal.get_calibration_summary()\n        &gt;&gt;&gt; summary[\"main\"][\"calibration\"]\n        {'mean': 0.023, 'std': 0.015, 'min': -0.05, 'max': 0.08}\n        &gt;&gt;&gt; summary[\"model_3d\"][\"up_down\"]\n        {'mean': 0.001, 'std': 0.003, 'min': -0.01, 'max': 0.02}\n\n        \"\"\"\n        summary: dict = {\"main\": {}}\n\n        # Main group\n        ds = self.open_dataset()\n        for var in ds.data_vars:\n            data = ds[var].values\n            valid_data = data[~np.isnan(data)]\n\n            if len(valid_data) &gt; 0:\n                summary[\"main\"][var] = {\n                    \"mean\": float(np.mean(valid_data)),\n                    \"std\": float(np.std(valid_data)),\n                    \"min\": float(np.min(valid_data)),\n                    \"max\": float(np.max(valid_data)),\n                }\n\n        # model_3d group if exists\n        if self.has_model_3d():\n            summary[\"model_3d\"] = {}\n            ds_model = self.open_model_3d()\n\n            for var in ds_model.data_vars:\n                data = ds_model[var].values\n                valid_data = data[~np.isnan(data)]\n\n                if len(valid_data) &gt; 0:\n                    summary[\"model_3d\"][var] = {\n                        \"mean\": float(np.mean(valid_data)),\n                        \"std\": float(np.std(valid_data)),\n                        \"min\": float(np.min(valid_data)),\n                        \"max\": float(np.max(valid_data)),\n                    }\n\n        return summary\n\n    def _get_transform(self, ds: xr.Dataset) -&gt; Affine:\n        \"\"\"Extract affine transform from dataset.\"\"\"\n        gt = ds.spatial_ref.attrs.get(\"GeoTransform\")\n        if gt is None:\n            raise ValueError(\"No GeoTransform found in spatial_ref\")\n\n        vals = [float(x) for x in gt.split()]\n        return Affine(vals[1], vals[2], vals[0], vals[4], vals[5], vals[3])\n\n    def _get_crs(self, ds: xr.Dataset) -&gt; str:\n        \"\"\"Extract CRS from dataset.\"\"\"\n        crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n        if crs_wkt is None:\n            raise ValueError(\"No crs_wkt found in spatial_ref\")\n        return crs_wkt\n\n    @property\n    def baseline_days(self) -&gt; int:\n        \"\"\"Temporal baseline in days.\"\"\"\n        return (self.secondary_date - self.primary_date).days\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Product filename.\"\"\"\n        return self.path.name\n\n    @property\n    def exists(self) -&gt; bool:\n        \"\"\"Check if product file exists.\"\"\"\n        return self.path.exists()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation.\"\"\"\n        model_str = \"+model_3d\" if self.exists and self.has_model_3d() else \"\"\n        return (\n            f\"CalProduct(sensor={self.sensor}, frame={self.frame_id}, \"\n            f\"{self.primary_date.date()} \u2192 {self.secondary_date.date()}, \"\n            f\"{self.polarization}{model_str})\"\n        )\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.baseline_days","title":"baseline_days  <code>property</code>","text":"<pre><code>baseline_days: int\n</code></pre> <p>Temporal baseline in days.</p>"},{"location":"api/#cal_disp.product.CalProduct.exists","title":"exists  <code>property</code>","text":"<pre><code>exists: bool\n</code></pre> <p>Check if product file exists.</p>"},{"location":"api/#cal_disp.product.CalProduct.filename","title":"filename  <code>property</code>","text":"<pre><code>filename: str\n</code></pre> <p>Product filename.</p>"},{"location":"api/#cal_disp.product.CalProduct.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(calibration: DataArray, disp_product: DispProduct, output_dir: Path | str, sensor: str = 'S1', calibration_std: DataArray | None = None, model_3d: dict[str, DataArray] | None = None, model_3d_std: dict[str, DataArray] | None = None, metadata: dict[str, str] | None = None, version: str = '1.0') -&gt; CalProduct\n</code></pre> <p>Create calibration product with optional model_3d group.</p> <p>Parameters:</p> Name Type Description Default <code>calibration</code> <code>DataArray</code> <p>Calibration correction at full DISP resolution.</p> required <code>disp_product</code> <code>DispProduct</code> <p>Original DISP product (for metadata).</p> required <code>output_dir</code> <code>Path or str</code> <p>Output directory for NetCDF file.</p> required <code>sensor</code> <code>str</code> <p>Sensor type: \"S1\" or \"NI\". Default is \"S1\".</p> <code>'S1'</code> <code>calibration_std</code> <code>DataArray or None</code> <p>Calibration uncertainty at full resolution. Default is None.</p> <code>None</code> <code>model_3d</code> <code>dict[str, DataArray] or None</code> <p>3D displacement components (coarse resolution) with keys: \"north_south\", \"east_west\", \"up_down\". Default is None.</p> <code>None</code> <code>model_3d_std</code> <code>dict[str, DataArray] or None</code> <p>3D displacement uncertainties (coarse resolution). Default is None.</p> <code>None</code> <code>metadata</code> <code>dict[str, str] or None</code> <p>Additional metadata (e.g., GNSS reference epoch). Default is None.</p> <code>None</code> <code>version</code> <code>str</code> <p>Product version. Default is \"1.0\".</p> <code>'1.0'</code> <p>Returns:</p> Type Description <code>CalProduct</code> <p>Created calibration product.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from product import DispProduct, CalProduct\n&gt;&gt;&gt;\n&gt;&gt;&gt; disp = DispProduct.from_path(\n\"OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Full resolution calibration\n&gt;&gt;&gt; cal_full = calibration_at_30m_resolution\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Coarse resolution 3D model (e.g., 90m from GNSS interpolation)\n&gt;&gt;&gt; model_coarse = {\n...     \"north_south\": disp_ns_90m,\n...     \"east_west\": disp_ew_90m,\n...     \"up_down\": disp_ud_90m,\n... }\n&gt;&gt;&gt;\n&gt;&gt;&gt; cal = CalProduct.create(\n...     calibration=cal_full,\n...     disp_product=disp,\n...     output_dir=\"output/\",\n...     calibration_std=cal_std,\n...     model_3d=model_coarse,\n...     model_3d_std={\n...         \"north_south_std\": disp_ns_std_90m,\n...         \"east_west_std\": disp_ew_std_90m,\n...         \"up_down_std\": disp_ud_std_90m,\n...     },\n...     metadata={\n...         \"gnss_reference_epoch\": \"2020-01-01T00:00:00Z\",\n...         \"model_3d_resolution\": \"90m\",\n...     },\n... )\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    calibration: xr.DataArray,\n    disp_product: \"DispProduct\",\n    output_dir: Path | str,\n    sensor: str = \"S1\",\n    calibration_std: xr.DataArray | None = None,\n    model_3d: dict[str, xr.DataArray] | None = None,\n    model_3d_std: dict[str, xr.DataArray] | None = None,\n    metadata: dict[str, str] | None = None,\n    version: str = \"1.0\",\n) -&gt; \"CalProduct\":\n    \"\"\"Create calibration product with optional model_3d group.\n\n    Parameters\n    ----------\n    calibration : xr.DataArray\n        Calibration correction at full DISP resolution.\n    disp_product : DispProduct\n        Original DISP product (for metadata).\n    output_dir : Path or str\n        Output directory for NetCDF file.\n    sensor : str, optional\n        Sensor type: \"S1\" or \"NI\". Default is \"S1\".\n    calibration_std : xr.DataArray or None, optional\n        Calibration uncertainty at full resolution. Default is None.\n    model_3d : dict[str, xr.DataArray] or None, optional\n        3D displacement components (coarse resolution) with keys:\n        \"north_south\", \"east_west\", \"up_down\". Default is None.\n    model_3d_std : dict[str, xr.DataArray] or None, optional\n        3D displacement uncertainties (coarse resolution). Default is None.\n    metadata : dict[str, str] or None, optional\n        Additional metadata (e.g., GNSS reference epoch). Default is None.\n    version : str, optional\n        Product version. Default is \"1.0\".\n\n    Returns\n    -------\n    CalProduct\n        Created calibration product.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from product import DispProduct, CalProduct\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; disp = DispProduct.from_path(\n    \"OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc\")\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Full resolution calibration\n    &gt;&gt;&gt; cal_full = calibration_at_30m_resolution\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Coarse resolution 3D model (e.g., 90m from GNSS interpolation)\n    &gt;&gt;&gt; model_coarse = {\n    ...     \"north_south\": disp_ns_90m,\n    ...     \"east_west\": disp_ew_90m,\n    ...     \"up_down\": disp_ud_90m,\n    ... }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; cal = CalProduct.create(\n    ...     calibration=cal_full,\n    ...     disp_product=disp,\n    ...     output_dir=\"output/\",\n    ...     calibration_std=cal_std,\n    ...     model_3d=model_coarse,\n    ...     model_3d_std={\n    ...         \"north_south_std\": disp_ns_std_90m,\n    ...         \"east_west_std\": disp_ew_std_90m,\n    ...         \"up_down_std\": disp_ud_std_90m,\n    ...     },\n    ...     metadata={\n    ...         \"gnss_reference_epoch\": \"2020-01-01T00:00:00Z\",\n    ...         \"model_3d_resolution\": \"90m\",\n    ...     },\n    ... )\n\n    \"\"\"\n    if sensor not in {\"S1\", \"NI\"}:\n        raise ValueError(f\"Invalid sensor: {sensor}. Must be 'S1' or 'NI'\")\n\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Generate OPERA-compliant filename\n    production_date = datetime.utcnow()\n    filename = (\n        f\"OPERA_L4_CAL-DISP-{sensor}_\"\n        f\"{disp_product.mode}_\"\n        f\"F{disp_product.frame_id:05d}_\"\n        f\"{disp_product.polarization}_\"\n        f\"{disp_product.primary_date:%Y%m%dT%H%M%S}Z_\"\n        f\"{disp_product.secondary_date:%Y%m%dT%H%M%S}Z_\"\n        f\"v{version}_\"\n        f\"{production_date:%Y%m%dT%H%M%S}Z\"\n        \".nc\"\n    )\n\n    output_path = output_dir / filename\n\n    # Build main group dataset (full resolution)\n    data_vars = {\"calibration\": calibration}\n\n    if calibration_std is not None:\n        data_vars[\"calibration_std\"] = calibration_std\n\n    # Create main dataset\n    ds = xr.Dataset(data_vars)\n\n    # Add global attributes\n    ds.attrs.update(\n        {\n            \"product_type\": f\"OPERA_L4_CAL-DISP-{sensor}\",\n            \"sensor\": sensor,\n            \"frame_id\": disp_product.frame_id,\n            \"mode\": disp_product.mode,\n            \"polarization\": disp_product.polarization,\n            \"primary_datetime\": disp_product.primary_date.isoformat(),\n            \"secondary_datetime\": disp_product.secondary_date.isoformat(),\n            \"production_datetime\": production_date.isoformat(),\n            \"product_version\": version,\n            \"description\": (\n                f\"Calibration correction for {sensor} InSAR displacement (subtract\"\n                \" from DISP)\"\n            ),\n            \"source_product\": disp_product.filename,\n            \"usage\": (\n                \"Subtract calibration layer from DISP displacement to obtain\"\n                \" calibrated displacement\"\n            ),\n        }\n    )\n\n    # Add custom metadata\n    if metadata:\n        ds.attrs.update(metadata)\n\n    # Save main group\n    ds.to_netcdf(output_path, engine=\"h5netcdf\")\n\n    # Create model_3d group if 3D components provided (coarse resolution)\n    if model_3d or model_3d_std:\n        model_data_vars = {}\n\n        # Add 3D displacement components\n        if model_3d:\n            for comp in [\"north_south\", \"east_west\", \"up_down\"]:\n                if comp in model_3d:\n                    model_data_vars[comp] = model_3d[comp]\n\n        # Add 3D displacement uncertainties\n        if model_3d_std:\n            for comp in [\"north_south_std\", \"east_west_std\", \"up_down_std\"]:\n                if comp in model_3d_std:\n                    model_data_vars[comp] = model_3d_std[comp]\n\n        if model_data_vars:\n            ds_model = xr.Dataset(model_data_vars)\n\n            # Add model group attributes\n            ds_model.attrs.update(\n                {\n                    \"description\": (\n                        \"3D displacement model at coarse resolution (e.g., from\"\n                        \" GNSS interpolation or deformation model)\"\n                    ),\n                    \"units\": \"meters\",\n                    \"reference_frame\": \"ENU (East-North-Up)\",\n                }\n            )\n\n            # Append to existing file as model_3d group\n            ds_model.to_netcdf(\n                output_path,\n                mode=\"a\",\n                group=\"model_3d\",\n                engine=\"h5netcdf\",\n            )\n\n    return cls(\n        path=output_path,\n        frame_id=disp_product.frame_id,\n        primary_date=disp_product.primary_date,\n        secondary_date=disp_product.secondary_date,\n        polarization=disp_product.polarization,\n        sensor=sensor,\n        version=version,\n        production_date=production_date,\n        mode=disp_product.mode,\n    )\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(path: Path | str) -&gt; CalProduct\n</code></pre> <p>Parse product metadata from filename.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path or str</code> <p>Path to calibration product NetCDF file.</p> required <p>Returns:</p> Type Description <code>CalProduct</code> <p>Parsed calibration product instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If filename doesn't match OPERA CAL-DISP pattern.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cal = CalProduct.from_path(\n\"OPERA_L4_CAL-DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251227T123456Z.nc\")\n&gt;&gt;&gt; cal.sensor\n'S1'\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>@classmethod\ndef from_path(cls, path: Path | str) -&gt; \"CalProduct\":\n    \"\"\"Parse product metadata from filename.\n\n    Parameters\n    ----------\n    path : Path or str\n        Path to calibration product NetCDF file.\n\n    Returns\n    -------\n    CalProduct\n        Parsed calibration product instance.\n\n    Raises\n    ------\n    ValueError\n        If filename doesn't match OPERA CAL-DISP pattern.\n\n    Examples\n    --------\n    &gt;&gt;&gt; cal = CalProduct.from_path(\n    \"OPERA_L4_CAL-DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251227T123456Z.nc\")\n    &gt;&gt;&gt; cal.sensor\n    'S1'\n\n    \"\"\"\n    path = Path(path)\n    match = cls._PATTERN.match(path.name)\n\n    if not match:\n        raise ValueError(\n            f\"Filename does not match OPERA CAL-DISP pattern: {path.name}\"\n        )\n\n    return cls(\n        path=path,\n        frame_id=int(match.group(\"frame_id\")),\n        primary_date=datetime.strptime(match.group(\"primary\"), \"%Y%m%dT%H%M%SZ\"),\n        secondary_date=datetime.strptime(\n            match.group(\"secondary\"), \"%Y%m%dT%H%M%SZ\"\n        ),\n        polarization=match.group(\"pol\"),\n        sensor=match.group(\"sensor\"),\n        version=match.group(\"version\"),\n        production_date=datetime.strptime(\n            match.group(\"production\"), \"%Y%m%dT%H%M%SZ\"\n        ),\n        mode=match.group(\"mode\"),\n    )\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.get_bounds","title":"get_bounds","text":"<pre><code>get_bounds() -&gt; dict[str, float]\n</code></pre> <p>Get bounds in native projection.</p> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def get_bounds(self) -&gt; dict[str, float]:\n    \"\"\"Get bounds in native projection.\"\"\"\n    ds = self.open_dataset()\n\n    x = ds.x.values\n    y = ds.y.values\n\n    return {\n        \"left\": float(x.min()),\n        \"bottom\": float(y.min()),\n        \"right\": float(x.max()),\n        \"top\": float(y.max()),\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.get_bounds_wgs84","title":"get_bounds_wgs84","text":"<pre><code>get_bounds_wgs84() -&gt; dict[str, float]\n</code></pre> <p>Get bounds transformed to WGS84.</p> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def get_bounds_wgs84(self) -&gt; dict[str, float]:\n    \"\"\"Get bounds transformed to WGS84.\"\"\"\n    ds = self.open_dataset()\n\n    x = ds.x.values\n    y = ds.y.values\n    left = float(x.min())\n    bottom = float(y.min())\n    right = float(x.max())\n    top = float(y.max())\n\n    if \"spatial_ref\" not in ds:\n        raise ValueError(\"Dataset missing spatial_ref\")\n\n    crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n    if not crs_wkt:\n        raise ValueError(\"spatial_ref missing crs_wkt\")\n\n    src_crs = CRS.from_wkt(crs_wkt)\n\n    west, south, east, north = transform_bounds(\n        src_crs,\n        CRS.from_epsg(4326),\n        left,\n        bottom,\n        right,\n        top,\n    )\n\n    return {\n        \"west\": west,\n        \"south\": south,\n        \"east\": east,\n        \"north\": north,\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.get_calibration_summary","title":"get_calibration_summary","text":"<pre><code>get_calibration_summary() -&gt; dict[str, dict[str, float]]\n</code></pre> <p>Get summary statistics of all layers.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, float]]</code> <p>Statistics for main and model_3d groups.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; summary = cal.get_calibration_summary()\n&gt;&gt;&gt; summary[\"main\"][\"calibration\"]\n{'mean': 0.023, 'std': 0.015, 'min': -0.05, 'max': 0.08}\n&gt;&gt;&gt; summary[\"model_3d\"][\"up_down\"]\n{'mean': 0.001, 'std': 0.003, 'min': -0.01, 'max': 0.02}\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def get_calibration_summary(self) -&gt; dict[str, dict[str, float]]:\n    \"\"\"Get summary statistics of all layers.\n\n    Returns\n    -------\n    dict[str, dict[str, float]]\n        Statistics for main and model_3d groups.\n\n    Examples\n    --------\n    &gt;&gt;&gt; summary = cal.get_calibration_summary()\n    &gt;&gt;&gt; summary[\"main\"][\"calibration\"]\n    {'mean': 0.023, 'std': 0.015, 'min': -0.05, 'max': 0.08}\n    &gt;&gt;&gt; summary[\"model_3d\"][\"up_down\"]\n    {'mean': 0.001, 'std': 0.003, 'min': -0.01, 'max': 0.02}\n\n    \"\"\"\n    summary: dict = {\"main\": {}}\n\n    # Main group\n    ds = self.open_dataset()\n    for var in ds.data_vars:\n        data = ds[var].values\n        valid_data = data[~np.isnan(data)]\n\n        if len(valid_data) &gt; 0:\n            summary[\"main\"][var] = {\n                \"mean\": float(np.mean(valid_data)),\n                \"std\": float(np.std(valid_data)),\n                \"min\": float(np.min(valid_data)),\n                \"max\": float(np.max(valid_data)),\n            }\n\n    # model_3d group if exists\n    if self.has_model_3d():\n        summary[\"model_3d\"] = {}\n        ds_model = self.open_model_3d()\n\n        for var in ds_model.data_vars:\n            data = ds_model[var].values\n            valid_data = data[~np.isnan(data)]\n\n            if len(valid_data) &gt; 0:\n                summary[\"model_3d\"][var] = {\n                    \"mean\": float(np.mean(valid_data)),\n                    \"std\": float(np.std(valid_data)),\n                    \"min\": float(np.min(valid_data)),\n                    \"max\": float(np.max(valid_data)),\n                }\n\n    return summary\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.get_epsg","title":"get_epsg","text":"<pre><code>get_epsg() -&gt; int | None\n</code></pre> <p>Get EPSG code from spatial reference.</p> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def get_epsg(self) -&gt; int | None:\n    \"\"\"Get EPSG code from spatial reference.\"\"\"\n    ds = self.open_dataset()\n\n    if \"spatial_ref\" in ds:\n        crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n        if crs_wkt:\n            crs = CRS.from_wkt(crs_wkt)\n            return crs.to_epsg()\n\n    return None\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.has_model_3d","title":"has_model_3d","text":"<pre><code>has_model_3d() -&gt; bool\n</code></pre> <p>Check if product contains model_3d group.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if model_3d group exists.</p> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def has_model_3d(self) -&gt; bool:\n    \"\"\"Check if product contains model_3d group.\n\n    Returns\n    -------\n    bool\n        True if model_3d group exists.\n\n    \"\"\"\n    try:\n        self.open_model_3d()\n        return True\n    except (FileNotFoundError, ValueError):\n        return False\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.open_dataset","title":"open_dataset","text":"<pre><code>open_dataset(group: str | None = None) -&gt; xr.Dataset\n</code></pre> <p>Open calibration dataset.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>str or None</code> <p>Group to open: None for main, \"model_3d\" for 3D model. Default is None (main group).</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing requested group.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If product file does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Open main calibration (full resolution)\n&gt;&gt;&gt; ds_main = cal.open_dataset()\n&gt;&gt;&gt; calibration = ds_main[\"calibration\"]\n</code></pre> <pre><code>&gt;&gt;&gt; # Open model_3d group (coarse resolution)\n&gt;&gt;&gt; ds_model = cal.open_dataset(group=\"model_3d\")\n&gt;&gt;&gt; model_up = ds_model[\"up_down\"]\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def open_dataset(self, group: str | None = None) -&gt; xr.Dataset:\n    \"\"\"Open calibration dataset.\n\n    Parameters\n    ----------\n    group : str or None, optional\n        Group to open: None for main, \"model_3d\" for 3D model.\n        Default is None (main group).\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing requested group.\n\n    Raises\n    ------\n    FileNotFoundError\n        If product file does not exist.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Open main calibration (full resolution)\n    &gt;&gt;&gt; ds_main = cal.open_dataset()\n    &gt;&gt;&gt; calibration = ds_main[\"calibration\"]\n\n    &gt;&gt;&gt; # Open model_3d group (coarse resolution)\n    &gt;&gt;&gt; ds_model = cal.open_dataset(group=\"model_3d\")\n    &gt;&gt;&gt; model_up = ds_model[\"up_down\"]\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n    if group == \"model_3d\":\n        return xr.open_dataset(self.path, group=\"model_3d\", engine=\"h5netcdf\")\n\n    return xr.open_dataset(self.path, engine=\"h5netcdf\")\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.open_model_3d","title":"open_model_3d","text":"<pre><code>open_model_3d() -&gt; xr.Dataset\n</code></pre> <p>Open model_3d group dataset.</p> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing 3D displacement model at coarse resolution.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If product file does not exist.</p> <code>ValueError</code> <p>If model_3d group does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds_model = cal.open_model_3d()\n&gt;&gt;&gt; disp_ns = ds_model[\"north_south\"]\n&gt;&gt;&gt; disp_ew = ds_model[\"east_west\"]\n&gt;&gt;&gt; disp_up = ds_model[\"up_down\"]\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def open_model_3d(self) -&gt; xr.Dataset:\n    \"\"\"Open model_3d group dataset.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing 3D displacement model at coarse resolution.\n\n    Raises\n    ------\n    FileNotFoundError\n        If product file does not exist.\n    ValueError\n        If model_3d group does not exist.\n\n    Examples\n    --------\n    &gt;&gt;&gt; ds_model = cal.open_model_3d()\n    &gt;&gt;&gt; disp_ns = ds_model[\"north_south\"]\n    &gt;&gt;&gt; disp_ew = ds_model[\"east_west\"]\n    &gt;&gt;&gt; disp_up = ds_model[\"up_down\"]\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n    try:\n        return xr.open_dataset(self.path, group=\"model_3d\", engine=\"h5netcdf\")\n    except (OSError, ValueError) as e:\n        raise ValueError(\n            f\"model_3d group not found in {self.filename}. \"\n            \"Product may not contain 3D displacement model.\"\n        ) from e\n</code></pre>"},{"location":"api/#cal_disp.product.CalProduct.to_geotiff","title":"to_geotiff","text":"<pre><code>to_geotiff(layer: str, output_path: Path | str, group: str | None = None, compress: str = 'DEFLATE', **kwargs) -&gt; Path\n</code></pre> <p>Export layer to GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>str</code> <p>Name of layer to export.</p> required <code>output_path</code> <code>Path or str</code> <p>Output GeoTIFF path.</p> required <code>group</code> <code>str or None</code> <p>Group containing layer. Default is None (main group).</p> <code>None</code> <code>compress</code> <code>str</code> <p>Compression method. Default is \"DEFLATE\".</p> <code>'DEFLATE'</code> <code>**kwargs</code> <p>Additional rasterio creation options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to created GeoTIFF.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Export main calibration\n&gt;&gt;&gt; cal.to_geotiff(\"calibration\", \"calibration.tif\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Export 3D model component\n&gt;&gt;&gt; cal.to_geotiff(\"up_down\", \"model_up.tif\", group=\"model_3d\")\n</code></pre> Source code in <code>src/cal_disp/product/_cal.py</code> <pre><code>def to_geotiff(\n    self,\n    layer: str,\n    output_path: Path | str,\n    group: str | None = None,\n    compress: str = \"DEFLATE\",\n    **kwargs,\n) -&gt; Path:\n    \"\"\"Export layer to GeoTIFF.\n\n    Parameters\n    ----------\n    layer : str\n        Name of layer to export.\n    output_path : Path or str\n        Output GeoTIFF path.\n    group : str or None, optional\n        Group containing layer. Default is None (main group).\n    compress : str, optional\n        Compression method. Default is \"DEFLATE\".\n    **kwargs\n        Additional rasterio creation options.\n\n    Returns\n    -------\n    Path\n        Path to created GeoTIFF.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Export main calibration\n    &gt;&gt;&gt; cal.to_geotiff(\"calibration\", \"calibration.tif\")\n\n    &gt;&gt;&gt; # Export 3D model component\n    &gt;&gt;&gt; cal.to_geotiff(\"up_down\", \"model_up.tif\", group=\"model_3d\")\n\n    \"\"\"\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    ds = self.open_dataset(group=group)\n\n    if layer not in ds:\n        available = list(ds.data_vars)\n        group_str = f\" in {group} group\" if group else \"\"\n        raise ValueError(\n            f\"Layer '{layer}' not found{group_str}. Available: {available}\"\n        )\n\n    da = ds[layer]\n    data = da.values\n\n    # Extract spatial information\n    if \"spatial_ref\" in ds:\n        transform = self._get_transform(ds)\n        crs = self._get_crs(ds)\n    else:\n        transform = Affine.translation(\n            float(ds.x.values[0]),\n            float(ds.y.values[0]),\n        ) * Affine.scale(\n            float(ds.x.values[1] - ds.x.values[0]),\n            float(ds.y.values[1] - ds.y.values[0]),\n        )\n        crs = None\n\n    # Write GeoTIFF\n    profile = {\n        \"driver\": \"GTiff\",\n        \"height\": data.shape[0],\n        \"width\": data.shape[1],\n        \"count\": 1,\n        \"dtype\": np.float32,\n        \"transform\": transform,\n        \"compress\": compress,\n        \"tiled\": True,\n        \"blockxsize\": 512,\n        \"blockysize\": 512,\n        **kwargs,\n    }\n\n    if crs:\n        profile[\"crs\"] = crs\n\n    with rasterio.open(output_path, \"w\", **profile) as dst:\n        dst.write(data.astype(np.float32), 1)\n        dst.set_band_description(1, layer)\n\n        # Add OPERA metadata tags\n        dst.update_tags(\n            product_type=f\"OPERA_L4_CAL-DISP-{self.sensor}\",\n            sensor=self.sensor,\n            frame_id=self.frame_id,\n            polarization=self.polarization,\n            primary_date=self.primary_date.isoformat(),\n            secondary_date=self.secondary_date.isoformat(),\n            layer=layer,\n            group=group if group else \"main\",\n        )\n\n    return output_path\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct","title":"DispProduct  <code>dataclass</code>","text":"<p>OPERA DISP-S1 displacement product.</p> <p>Represents a Level-3 interferometric displacement product from the OPERA DISP-S1 archive. Products contain displacement measurements between two Sentinel-1 acquisition dates.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the NetCDF product file.</p> required <code>frame_id</code> <code>int</code> <p>OPERA frame identifier (e.g., 8882).</p> required <code>primary_date</code> <code>datetime</code> <p>Earlier acquisition date (reference).</p> required <code>secondary_date</code> <code>datetime</code> <p>Later acquisition date.</p> required <code>polarization</code> <code>str</code> <p>Radar polarization (e.g., \"VV\", \"VH\").</p> required <code>version</code> <code>str</code> <p>Product version string (e.g., \"1.0\").</p> required <code>production_date</code> <code>datetime</code> <p>Date when product was generated.</p> required <code>mode</code> <code>str</code> <p>Acquisition mode (e.g., \"IW\"). Default is \"IW\".</p> <code>'IW'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; path = Path(\n\"OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc\")\n&gt;&gt;&gt; product = DispProduct.from_path(path)\n&gt;&gt;&gt; product.frame_id\n8882\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct--get-reference-point","title":"Get reference point","text":"<pre><code>&gt;&gt;&gt; row, col = product.get_reference_point_index()\n&gt;&gt;&gt; lat, lon = product.get_reference_point_latlon()\n&gt;&gt;&gt; print(f\"Reference at ({row}, {col}): {lat:.4f}\u00b0N, {lon:.4f}\u00b0E\")\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>@dataclass\nclass DispProduct:\n    \"\"\"OPERA DISP-S1 displacement product.\n\n    Represents a Level-3 interferometric displacement product from\n    the OPERA DISP-S1 archive. Products contain displacement measurements\n    between two Sentinel-1 acquisition dates.\n\n    Parameters\n    ----------\n    path : Path\n        Path to the NetCDF product file.\n    frame_id : int\n        OPERA frame identifier (e.g., 8882).\n    primary_date : datetime\n        Earlier acquisition date (reference).\n    secondary_date : datetime\n        Later acquisition date.\n    polarization : str\n        Radar polarization (e.g., \"VV\", \"VH\").\n    version : str\n        Product version string (e.g., \"1.0\").\n    production_date : datetime\n        Date when product was generated.\n    mode : str, optional\n        Acquisition mode (e.g., \"IW\"). Default is \"IW\".\n\n    Examples\n    --------\n    &gt;&gt;&gt; path = Path(\n    \"OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc\")\n    &gt;&gt;&gt; product = DispProduct.from_path(path)\n    &gt;&gt;&gt; product.frame_id\n    8882\n\n    # Get reference point\n    &gt;&gt;&gt; row, col = product.get_reference_point_index()\n    &gt;&gt;&gt; lat, lon = product.get_reference_point_latlon()\n    &gt;&gt;&gt; print(f\"Reference at ({row}, {col}): {lat:.4f}\u00b0N, {lon:.4f}\u00b0E\")\n\n    \"\"\"\n\n    path: Path\n    frame_id: int\n    primary_date: datetime\n    secondary_date: datetime\n    polarization: str\n    version: str\n    production_date: datetime\n    mode: str = \"IW\"\n\n    # Filename pattern for OPERA DISP-S1 products\n    _PATTERN = re.compile(\n        r\"OPERA_L3_DISP-S1_\"\n        r\"(?P&lt;mode&gt;\\w+)_\"\n        r\"F(?P&lt;frame_id&gt;\\d+)_\"\n        r\"(?P&lt;pol&gt;\\w+)_\"\n        r\"(?P&lt;primary&gt;\\d{8}T\\d{6}Z)_\"\n        r\"(?P&lt;secondary&gt;\\d{8}T\\d{6}Z)_\"\n        r\"v(?P&lt;version&gt;[\\d.]+)_\"\n        r\"(?P&lt;production&gt;\\d{8}T\\d{6}Z)\"\n        r\"\\.nc$\"\n    )\n\n    # Main dataset layers\n    DISPLACEMENT_LAYERS = [\n        \"displacement\",\n        \"short_wavelength_displacement\",\n        \"recommended_mask\",\n        \"connected_component_labels\",\n        \"temporal_coherence\",\n        \"estimated_phase_quality\",\n        \"persistent_scatterer_mask\",\n        \"shp_counts\",\n        \"water_mask\",\n        \"phase_similarity\",\n        \"timeseries_inversion_residuals\",\n    ]\n\n    # Corrections group layers\n    CORRECTION_LAYERS = [\n        \"ionospheric_delay\",\n        \"solid_earth_tide\",\n        \"perpendicular_baseline\",\n    ]\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate product after construction.\"\"\"\n        self.path = Path(self.path)\n\n        if self.frame_id &lt;= 0:\n            raise ValueError(f\"frame_id must be positive, got {self.frame_id}\")\n\n        if self.secondary_date &lt;= self.primary_date:\n            raise ValueError(\n                f\"Secondary date ({self.secondary_date}) must be after \"\n                f\"primary date ({self.primary_date})\"\n            )\n\n        if self.polarization not in {\"VV\", \"VH\", \"HH\", \"HV\"}:\n            raise ValueError(f\"Invalid polarization: {self.polarization}\")\n\n    @classmethod\n    def from_path(cls, path: Path | str) -&gt; \"DispProduct\":\n        \"\"\"Parse product metadata from filename.\n\n        Parameters\n        ----------\n        path : Path or str\n            Path to OPERA DISP-S1 NetCDF file.\n\n        Returns\n        -------\n        DispProduct\n            Parsed product instance.\n\n        Raises\n        ------\n        ValueError\n            If filename doesn't match expected OPERA DISP-S1 format.\n\n        \"\"\"\n        path = Path(path)\n        match = cls._PATTERN.match(path.name)\n\n        if not match:\n            raise ValueError(\n                f\"Filename does not match OPERA DISP-S1 pattern: {path.name}\"\n            )\n\n        return cls(\n            path=path,\n            frame_id=int(match.group(\"frame_id\")),\n            primary_date=datetime.strptime(match.group(\"primary\"), \"%Y%m%dT%H%M%SZ\"),\n            secondary_date=datetime.strptime(\n                match.group(\"secondary\"), \"%Y%m%dT%H%M%SZ\"\n            ),\n            polarization=match.group(\"pol\"),\n            version=match.group(\"version\"),\n            production_date=datetime.strptime(\n                match.group(\"production\"), \"%Y%m%dT%H%M%SZ\"\n            ),\n            mode=match.group(\"mode\"),\n        )\n\n    def open_dataset(\n        self, group: Literal[\"main\", \"corrections\"] | None = None\n    ) -&gt; xr.Dataset:\n        \"\"\"Open dataset.\n\n        Parameters\n        ----------\n        group : {\"main\", \"corrections\"} or None, optional\n            Which group to open. If None, opens main group. Default is None.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing displacement and quality layers.\n\n        Raises\n        ------\n        FileNotFoundError\n            If product file does not exist.\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n        if group == \"corrections\":\n            return xr.open_dataset(self.path, group=\"corrections\", engine=\"h5netcdf\")\n\n        return xr.open_dataset(self.path, engine=\"h5netcdf\")\n\n    def open_corrections(self) -&gt; xr.Dataset:\n        \"\"\"Open corrections group dataset.\n\n        Returns\n        -------\n        xr.Dataset\n            Corrections dataset containing ionospheric delay, solid earth tide, etc.\n\n        Raises\n        ------\n        FileNotFoundError\n            If product file does not exist.\n\n        \"\"\"\n        return self.open_dataset(group=\"corrections\")\n\n    def get_epsg(self) -&gt; int | None:\n        \"\"\"Get EPSG code from spatial reference.\n\n        Returns\n        -------\n        int or None\n            EPSG code if found, None otherwise.\n\n        Examples\n        --------\n        &gt;&gt;&gt; product.get_epsg()\n        32615\n\n        \"\"\"\n        ds = self.open_dataset()\n        crs = self._get_crs(ds)\n\n        # Parse EPSG from CRS\n        raster_crs = CRS.from_wkt(crs)\n        return raster_crs.to_epsg()\n\n    def get_bounds(self) -&gt; dict[str, float]:\n        \"\"\"Get bounds in native projection coordinates.\n\n        Returns\n        -------\n        dict[str, float]\n            Dictionary with keys: left, bottom, right, top.\n\n        Examples\n        --------\n        &gt;&gt;&gt; bounds = product.get_bounds()\n        &gt;&gt;&gt; bounds\n        {'left': 71970.0, 'bottom': 3153930.0, 'right': 355890.0, 'top': 3385920.0}\n\n        \"\"\"\n        ds = self.open_dataset()\n\n        x = ds.x.values\n        y = ds.y.values\n\n        return {\n            \"left\": float(x.min()),\n            \"bottom\": float(y.min()),\n            \"right\": float(x.max()),\n            \"top\": float(y.max()),\n        }\n\n    def get_bounds_wgs84(self) -&gt; dict[str, float]:\n        \"\"\"Get bounds transformed to WGS84 (EPSG:4326).\n\n        Returns\n        -------\n        dict[str, float]\n            Dictionary with keys: west, south, east, north in decimal degrees.\n\n        Examples\n        --------\n        &gt;&gt;&gt; bounds = product.get_bounds_wgs84()\n        &gt;&gt;&gt; bounds\n        {'west': -95.567, 'south': 28.486, 'east': -93.212, 'north': 30.845}\n\n        \"\"\"\n        ds = self.open_dataset()\n\n        # Get native bounds\n        x = ds.x.values\n        y = ds.y.values\n        left = float(x.min())\n        bottom = float(y.min())\n        right = float(x.max())\n        top = float(y.max())\n\n        # Get native CRS\n        crs_wkt = self._get_crs(ds)\n        src_crs = CRS.from_wkt(crs_wkt)\n\n        # Transform to WGS84\n        west, south, east, north = transform_bounds(\n            src_crs,\n            CRS.from_epsg(4326),\n            left,\n            bottom,\n            right,\n            top,\n        )\n\n        return {\n            \"west\": west,\n            \"south\": south,\n            \"east\": east,\n            \"north\": north,\n        }\n\n    def get_reference_point_index(self) -&gt; tuple[int, int]:\n        \"\"\"Get reference point pixel indices.\n\n        The reference point is where the phase was set to zero during\n        processing. This is stored in the corrections group.\n\n        Returns\n        -------\n        tuple[int, int]\n            Row and column indices (row, col) of reference point.\n\n        Raises\n        ------\n        ValueError\n            If reference_point variable not found or missing attributes.\n\n        Examples\n        --------\n        &gt;&gt;&gt; row, col = product.get_reference_point_index()\n        &gt;&gt;&gt; print(f\"Reference point at pixel ({row}, {col})\")\n\n        \"\"\"\n        ds = self.open_corrections()\n\n        if \"reference_point\" not in ds:\n            raise ValueError(\"reference_point variable not found in corrections group\")\n\n        ref_attrs = ds.reference_point.attrs\n\n        if \"rows\" not in ref_attrs or \"cols\" not in ref_attrs:\n            raise ValueError(\"reference_point missing 'rows' or 'cols' attributes\")\n\n        row = int(ref_attrs[\"rows\"])\n        col = int(ref_attrs[\"cols\"])\n\n        return (row, col)\n\n    def get_reference_point_latlon(self) -&gt; tuple[float, float]:\n        \"\"\"Get reference point geographic coordinates.\n\n        Returns latitude and longitude of the reference point in WGS84.\n\n        Returns\n        -------\n        tuple[float, float]\n            Latitude and longitude in decimal degrees (lat, lon).\n\n        Raises\n        ------\n        ValueError\n            If reference_point variable not found or missing attributes.\n\n        Examples\n        --------\n        &gt;&gt;&gt; lat, lon = product.get_reference_point_latlon()\n        &gt;&gt;&gt; print(f\"Reference point: {lat:.6f}\u00b0N, {lon:.6f}\u00b0E\")\n\n        \"\"\"\n        ds = self.open_corrections()\n\n        if \"reference_point\" not in ds:\n            raise ValueError(\"reference_point variable not found in corrections group\")\n\n        ref_attrs = ds.reference_point.attrs\n\n        if \"latitudes\" not in ref_attrs or \"longitudes\" not in ref_attrs:\n            raise ValueError(\n                \"reference_point missing 'latitudes' or 'longitudes' attributes\"\n            )\n\n        lat = float(ref_attrs[\"latitudes\"])\n        lon = float(ref_attrs[\"longitudes\"])\n\n        return (lat, lon)\n\n    def to_geotiff(\n        self,\n        layer: str,\n        output_path: Path | str,\n        group: Literal[\"main\", \"corrections\"] = \"main\",\n        compress: str = \"DEFLATE\",\n        **kwargs,\n    ) -&gt; Path:\n        \"\"\"Export layer to optimized GeoTIFF.\n\n        Parameters\n        ----------\n        layer : str\n            Name of layer to export (e.g., \"displacement\", \"ionospheric_delay\").\n        output_path : Path or str\n            Output GeoTIFF path.\n        group : {\"main\", \"corrections\"}, optional\n            Which group to read from. Default is \"main\".\n        compress : str, optional\n            Compression method. Default is \"DEFLATE\".\n        **kwargs\n            Additional rasterio creation options.\n\n        Returns\n        -------\n        Path\n            Path to created GeoTIFF.\n\n        Raises\n        ------\n        ValueError\n            If layer not found in specified group.\n\n        Examples\n        --------\n        &gt;&gt;&gt; product.to_geotiff(\"displacement\", \"disp.tif\")\n        &gt;&gt;&gt; product.to_geotiff(\"ionospheric_delay\", \"iono.tif\", group=\"corrections\")\n\n        \"\"\"\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Open appropriate dataset\n        ds = self.open_dataset(group=group if group == \"corrections\" else None)\n\n        if layer not in ds:\n            available = list(ds.data_vars)\n            raise ValueError(\n                f\"Layer '{layer}' not found in {group} group. \"\n                f\"Available layers: {available}\"\n            )\n\n        # Get data array\n        da = ds[layer]\n\n        # Extract transform from spatial_ref\n        transform = self._get_transform(ds)\n\n        # Get CRS\n        crs = self._get_crs(ds)\n\n        # Prepare data - handle (y, x) or (time, y, x) shapes\n        if da.ndim == 3:\n            # Take first time slice if 3D\n            data = da.isel(time=0).values\n        else:\n            data = da.values\n\n        # Write GeoTIFF\n        profile = {\n            \"driver\": \"GTiff\",\n            \"height\": data.shape[0],\n            \"width\": data.shape[1],\n            \"count\": 1,\n            \"dtype\": data.dtype,\n            \"crs\": crs,\n            \"transform\": transform,\n            \"compress\": compress,\n            \"tiled\": True,\n            \"blockxsize\": 512,\n            \"blockysize\": 512,\n            **kwargs,\n        }\n\n        with rasterio.open(output_path, \"w\", **profile) as dst:\n            dst.write(data, 1)\n            dst.set_band_description(1, layer)\n\n        return output_path\n\n    def _get_transform(self, ds: xr.Dataset) -&gt; Affine:\n        \"\"\"Extract affine transform from dataset.\"\"\"\n        gt = ds.spatial_ref.attrs.get(\"GeoTransform\")\n        if gt is None:\n            raise ValueError(\"No GeoTransform found in spatial_ref\")\n\n        # Parse string like \"71970.0 30.0 0.0 3385920.0 0.0 -30.0\"\n        vals = [float(x) for x in gt.split()]\n        return Affine(vals[1], vals[2], vals[0], vals[4], vals[5], vals[3])\n\n    def _get_crs(self, ds: xr.Dataset) -&gt; str:\n        \"\"\"Extract CRS from dataset.\"\"\"\n        crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n        if crs_wkt is None:\n            raise ValueError(\"No crs_wkt found in spatial_ref\")\n        return crs_wkt\n\n    @property\n    def baseline_days(self) -&gt; int:\n        \"\"\"Temporal baseline in days between acquisitions.\"\"\"\n        return (self.secondary_date - self.primary_date).days\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Product filename.\"\"\"\n        return self.path.name\n\n    @property\n    def exists(self) -&gt; bool:\n        \"\"\"Check if product file exists on disk.\"\"\"\n        return self.path.exists()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Concise string representation.\"\"\"\n        return (\n            f\"DispProduct(frame={self.frame_id}, \"\n            f\"{self.primary_date.date()} \u2192 {self.secondary_date.date()}, \"\n            f\"{self.polarization})\"\n        )\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.baseline_days","title":"baseline_days  <code>property</code>","text":"<pre><code>baseline_days: int\n</code></pre> <p>Temporal baseline in days between acquisitions.</p>"},{"location":"api/#cal_disp.product.DispProduct.exists","title":"exists  <code>property</code>","text":"<pre><code>exists: bool\n</code></pre> <p>Check if product file exists on disk.</p>"},{"location":"api/#cal_disp.product.DispProduct.filename","title":"filename  <code>property</code>","text":"<pre><code>filename: str\n</code></pre> <p>Product filename.</p>"},{"location":"api/#cal_disp.product.DispProduct.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(path: Path | str) -&gt; DispProduct\n</code></pre> <p>Parse product metadata from filename.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path or str</code> <p>Path to OPERA DISP-S1 NetCDF file.</p> required <p>Returns:</p> Type Description <code>DispProduct</code> <p>Parsed product instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If filename doesn't match expected OPERA DISP-S1 format.</p> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>@classmethod\ndef from_path(cls, path: Path | str) -&gt; \"DispProduct\":\n    \"\"\"Parse product metadata from filename.\n\n    Parameters\n    ----------\n    path : Path or str\n        Path to OPERA DISP-S1 NetCDF file.\n\n    Returns\n    -------\n    DispProduct\n        Parsed product instance.\n\n    Raises\n    ------\n    ValueError\n        If filename doesn't match expected OPERA DISP-S1 format.\n\n    \"\"\"\n    path = Path(path)\n    match = cls._PATTERN.match(path.name)\n\n    if not match:\n        raise ValueError(\n            f\"Filename does not match OPERA DISP-S1 pattern: {path.name}\"\n        )\n\n    return cls(\n        path=path,\n        frame_id=int(match.group(\"frame_id\")),\n        primary_date=datetime.strptime(match.group(\"primary\"), \"%Y%m%dT%H%M%SZ\"),\n        secondary_date=datetime.strptime(\n            match.group(\"secondary\"), \"%Y%m%dT%H%M%SZ\"\n        ),\n        polarization=match.group(\"pol\"),\n        version=match.group(\"version\"),\n        production_date=datetime.strptime(\n            match.group(\"production\"), \"%Y%m%dT%H%M%SZ\"\n        ),\n        mode=match.group(\"mode\"),\n    )\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.get_bounds","title":"get_bounds","text":"<pre><code>get_bounds() -&gt; dict[str, float]\n</code></pre> <p>Get bounds in native projection coordinates.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with keys: left, bottom, right, top.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bounds = product.get_bounds()\n&gt;&gt;&gt; bounds\n{'left': 71970.0, 'bottom': 3153930.0, 'right': 355890.0, 'top': 3385920.0}\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def get_bounds(self) -&gt; dict[str, float]:\n    \"\"\"Get bounds in native projection coordinates.\n\n    Returns\n    -------\n    dict[str, float]\n        Dictionary with keys: left, bottom, right, top.\n\n    Examples\n    --------\n    &gt;&gt;&gt; bounds = product.get_bounds()\n    &gt;&gt;&gt; bounds\n    {'left': 71970.0, 'bottom': 3153930.0, 'right': 355890.0, 'top': 3385920.0}\n\n    \"\"\"\n    ds = self.open_dataset()\n\n    x = ds.x.values\n    y = ds.y.values\n\n    return {\n        \"left\": float(x.min()),\n        \"bottom\": float(y.min()),\n        \"right\": float(x.max()),\n        \"top\": float(y.max()),\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.get_bounds_wgs84","title":"get_bounds_wgs84","text":"<pre><code>get_bounds_wgs84() -&gt; dict[str, float]\n</code></pre> <p>Get bounds transformed to WGS84 (EPSG:4326).</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with keys: west, south, east, north in decimal degrees.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bounds = product.get_bounds_wgs84()\n&gt;&gt;&gt; bounds\n{'west': -95.567, 'south': 28.486, 'east': -93.212, 'north': 30.845}\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def get_bounds_wgs84(self) -&gt; dict[str, float]:\n    \"\"\"Get bounds transformed to WGS84 (EPSG:4326).\n\n    Returns\n    -------\n    dict[str, float]\n        Dictionary with keys: west, south, east, north in decimal degrees.\n\n    Examples\n    --------\n    &gt;&gt;&gt; bounds = product.get_bounds_wgs84()\n    &gt;&gt;&gt; bounds\n    {'west': -95.567, 'south': 28.486, 'east': -93.212, 'north': 30.845}\n\n    \"\"\"\n    ds = self.open_dataset()\n\n    # Get native bounds\n    x = ds.x.values\n    y = ds.y.values\n    left = float(x.min())\n    bottom = float(y.min())\n    right = float(x.max())\n    top = float(y.max())\n\n    # Get native CRS\n    crs_wkt = self._get_crs(ds)\n    src_crs = CRS.from_wkt(crs_wkt)\n\n    # Transform to WGS84\n    west, south, east, north = transform_bounds(\n        src_crs,\n        CRS.from_epsg(4326),\n        left,\n        bottom,\n        right,\n        top,\n    )\n\n    return {\n        \"west\": west,\n        \"south\": south,\n        \"east\": east,\n        \"north\": north,\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.get_epsg","title":"get_epsg","text":"<pre><code>get_epsg() -&gt; int | None\n</code></pre> <p>Get EPSG code from spatial reference.</p> <p>Returns:</p> Type Description <code>int or None</code> <p>EPSG code if found, None otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; product.get_epsg()\n32615\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def get_epsg(self) -&gt; int | None:\n    \"\"\"Get EPSG code from spatial reference.\n\n    Returns\n    -------\n    int or None\n        EPSG code if found, None otherwise.\n\n    Examples\n    --------\n    &gt;&gt;&gt; product.get_epsg()\n    32615\n\n    \"\"\"\n    ds = self.open_dataset()\n    crs = self._get_crs(ds)\n\n    # Parse EPSG from CRS\n    raster_crs = CRS.from_wkt(crs)\n    return raster_crs.to_epsg()\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.get_reference_point_index","title":"get_reference_point_index","text":"<pre><code>get_reference_point_index() -&gt; tuple[int, int]\n</code></pre> <p>Get reference point pixel indices.</p> <p>The reference point is where the phase was set to zero during processing. This is stored in the corrections group.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Row and column indices (row, col) of reference point.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If reference_point variable not found or missing attributes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; row, col = product.get_reference_point_index()\n&gt;&gt;&gt; print(f\"Reference point at pixel ({row}, {col})\")\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def get_reference_point_index(self) -&gt; tuple[int, int]:\n    \"\"\"Get reference point pixel indices.\n\n    The reference point is where the phase was set to zero during\n    processing. This is stored in the corrections group.\n\n    Returns\n    -------\n    tuple[int, int]\n        Row and column indices (row, col) of reference point.\n\n    Raises\n    ------\n    ValueError\n        If reference_point variable not found or missing attributes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; row, col = product.get_reference_point_index()\n    &gt;&gt;&gt; print(f\"Reference point at pixel ({row}, {col})\")\n\n    \"\"\"\n    ds = self.open_corrections()\n\n    if \"reference_point\" not in ds:\n        raise ValueError(\"reference_point variable not found in corrections group\")\n\n    ref_attrs = ds.reference_point.attrs\n\n    if \"rows\" not in ref_attrs or \"cols\" not in ref_attrs:\n        raise ValueError(\"reference_point missing 'rows' or 'cols' attributes\")\n\n    row = int(ref_attrs[\"rows\"])\n    col = int(ref_attrs[\"cols\"])\n\n    return (row, col)\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.get_reference_point_latlon","title":"get_reference_point_latlon","text":"<pre><code>get_reference_point_latlon() -&gt; tuple[float, float]\n</code></pre> <p>Get reference point geographic coordinates.</p> <p>Returns latitude and longitude of the reference point in WGS84.</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>Latitude and longitude in decimal degrees (lat, lon).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If reference_point variable not found or missing attributes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lat, lon = product.get_reference_point_latlon()\n&gt;&gt;&gt; print(f\"Reference point: {lat:.6f}\u00b0N, {lon:.6f}\u00b0E\")\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def get_reference_point_latlon(self) -&gt; tuple[float, float]:\n    \"\"\"Get reference point geographic coordinates.\n\n    Returns latitude and longitude of the reference point in WGS84.\n\n    Returns\n    -------\n    tuple[float, float]\n        Latitude and longitude in decimal degrees (lat, lon).\n\n    Raises\n    ------\n    ValueError\n        If reference_point variable not found or missing attributes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; lat, lon = product.get_reference_point_latlon()\n    &gt;&gt;&gt; print(f\"Reference point: {lat:.6f}\u00b0N, {lon:.6f}\u00b0E\")\n\n    \"\"\"\n    ds = self.open_corrections()\n\n    if \"reference_point\" not in ds:\n        raise ValueError(\"reference_point variable not found in corrections group\")\n\n    ref_attrs = ds.reference_point.attrs\n\n    if \"latitudes\" not in ref_attrs or \"longitudes\" not in ref_attrs:\n        raise ValueError(\n            \"reference_point missing 'latitudes' or 'longitudes' attributes\"\n        )\n\n    lat = float(ref_attrs[\"latitudes\"])\n    lon = float(ref_attrs[\"longitudes\"])\n\n    return (lat, lon)\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.open_corrections","title":"open_corrections","text":"<pre><code>open_corrections() -&gt; xr.Dataset\n</code></pre> <p>Open corrections group dataset.</p> <p>Returns:</p> Type Description <code>Dataset</code> <p>Corrections dataset containing ionospheric delay, solid earth tide, etc.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If product file does not exist.</p> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def open_corrections(self) -&gt; xr.Dataset:\n    \"\"\"Open corrections group dataset.\n\n    Returns\n    -------\n    xr.Dataset\n        Corrections dataset containing ionospheric delay, solid earth tide, etc.\n\n    Raises\n    ------\n    FileNotFoundError\n        If product file does not exist.\n\n    \"\"\"\n    return self.open_dataset(group=\"corrections\")\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.open_dataset","title":"open_dataset","text":"<pre><code>open_dataset(group: Literal['main', 'corrections'] | None = None) -&gt; xr.Dataset\n</code></pre> <p>Open dataset.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>(main, corrections)</code> <p>Which group to open. If None, opens main group. Default is None.</p> <code>\"main\"</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing displacement and quality layers.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If product file does not exist.</p> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def open_dataset(\n    self, group: Literal[\"main\", \"corrections\"] | None = None\n) -&gt; xr.Dataset:\n    \"\"\"Open dataset.\n\n    Parameters\n    ----------\n    group : {\"main\", \"corrections\"} or None, optional\n        Which group to open. If None, opens main group. Default is None.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing displacement and quality layers.\n\n    Raises\n    ------\n    FileNotFoundError\n        If product file does not exist.\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n    if group == \"corrections\":\n        return xr.open_dataset(self.path, group=\"corrections\", engine=\"h5netcdf\")\n\n    return xr.open_dataset(self.path, engine=\"h5netcdf\")\n</code></pre>"},{"location":"api/#cal_disp.product.DispProduct.to_geotiff","title":"to_geotiff","text":"<pre><code>to_geotiff(layer: str, output_path: Path | str, group: Literal['main', 'corrections'] = 'main', compress: str = 'DEFLATE', **kwargs) -&gt; Path\n</code></pre> <p>Export layer to optimized GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>str</code> <p>Name of layer to export (e.g., \"displacement\", \"ionospheric_delay\").</p> required <code>output_path</code> <code>Path or str</code> <p>Output GeoTIFF path.</p> required <code>group</code> <code>(main, corrections)</code> <p>Which group to read from. Default is \"main\".</p> <code>\"main\"</code> <code>compress</code> <code>str</code> <p>Compression method. Default is \"DEFLATE\".</p> <code>'DEFLATE'</code> <code>**kwargs</code> <p>Additional rasterio creation options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to created GeoTIFF.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If layer not found in specified group.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; product.to_geotiff(\"displacement\", \"disp.tif\")\n&gt;&gt;&gt; product.to_geotiff(\"ionospheric_delay\", \"iono.tif\", group=\"corrections\")\n</code></pre> Source code in <code>src/cal_disp/product/_disp.py</code> <pre><code>def to_geotiff(\n    self,\n    layer: str,\n    output_path: Path | str,\n    group: Literal[\"main\", \"corrections\"] = \"main\",\n    compress: str = \"DEFLATE\",\n    **kwargs,\n) -&gt; Path:\n    \"\"\"Export layer to optimized GeoTIFF.\n\n    Parameters\n    ----------\n    layer : str\n        Name of layer to export (e.g., \"displacement\", \"ionospheric_delay\").\n    output_path : Path or str\n        Output GeoTIFF path.\n    group : {\"main\", \"corrections\"}, optional\n        Which group to read from. Default is \"main\".\n    compress : str, optional\n        Compression method. Default is \"DEFLATE\".\n    **kwargs\n        Additional rasterio creation options.\n\n    Returns\n    -------\n    Path\n        Path to created GeoTIFF.\n\n    Raises\n    ------\n    ValueError\n        If layer not found in specified group.\n\n    Examples\n    --------\n    &gt;&gt;&gt; product.to_geotiff(\"displacement\", \"disp.tif\")\n    &gt;&gt;&gt; product.to_geotiff(\"ionospheric_delay\", \"iono.tif\", group=\"corrections\")\n\n    \"\"\"\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Open appropriate dataset\n    ds = self.open_dataset(group=group if group == \"corrections\" else None)\n\n    if layer not in ds:\n        available = list(ds.data_vars)\n        raise ValueError(\n            f\"Layer '{layer}' not found in {group} group. \"\n            f\"Available layers: {available}\"\n        )\n\n    # Get data array\n    da = ds[layer]\n\n    # Extract transform from spatial_ref\n    transform = self._get_transform(ds)\n\n    # Get CRS\n    crs = self._get_crs(ds)\n\n    # Prepare data - handle (y, x) or (time, y, x) shapes\n    if da.ndim == 3:\n        # Take first time slice if 3D\n        data = da.isel(time=0).values\n    else:\n        data = da.values\n\n    # Write GeoTIFF\n    profile = {\n        \"driver\": \"GTiff\",\n        \"height\": data.shape[0],\n        \"width\": data.shape[1],\n        \"count\": 1,\n        \"dtype\": data.dtype,\n        \"crs\": crs,\n        \"transform\": transform,\n        \"compress\": compress,\n        \"tiled\": True,\n        \"blockxsize\": 512,\n        \"blockysize\": 512,\n        **kwargs,\n    }\n\n    with rasterio.open(output_path, \"w\", **profile) as dst:\n        dst.write(data, 1)\n        dst.set_band_description(1, layer)\n\n    return output_path\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer","title":"StaticLayer  <code>dataclass</code>","text":"<p>OPERA DISP-S1-STATIC layer.</p> <p>Represents a single static layer (DEM, incidence angle, LOS vectors, etc.) used as input for DISP-S1 processing. These are frame-specific GeoTIFF files that don't change over time.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; path = Path(\"OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_dem.tif\")\n&gt;&gt;&gt; layer = StaticLayer.from_path(path)\n&gt;&gt;&gt; layer.frame_id\n8882\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer--read-los-components","title":"Read LOS components","text":"<pre><code>&gt;&gt;&gt; los_layer = StaticLayer.from_path(\"..._line_of_sight_enu.tif\")\n&gt;&gt;&gt; bands = los_layer.read_bands()\n&gt;&gt;&gt; east, north, up = bands[0], bands[1], bands[2]\n</code></pre> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>@dataclass\nclass StaticLayer:\n    \"\"\"OPERA DISP-S1-STATIC layer.\n\n    Represents a single static layer (DEM, incidence angle, LOS vectors, etc.)\n    used as input for DISP-S1 processing. These are frame-specific GeoTIFF\n    files that don't change over time.\n\n    Examples\n    --------\n    &gt;&gt;&gt; path = Path(\"OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_dem.tif\")\n    &gt;&gt;&gt; layer = StaticLayer.from_path(path)\n    &gt;&gt;&gt; layer.frame_id\n    8882\n\n    # Read LOS components\n    &gt;&gt;&gt; los_layer = StaticLayer.from_path(\"..._line_of_sight_enu.tif\")\n    &gt;&gt;&gt; bands = los_layer.read_bands()\n    &gt;&gt;&gt; east, north, up = bands[0], bands[1], bands[2]\n\n    \"\"\"\n\n    path: Path\n    frame_id: int\n    reference_date: datetime\n    satellite: str\n    version: str\n    layer_type: str\n\n    _PATTERN = re.compile(\n        r\"OPERA_L3_DISP-S1-STATIC_\"\n        r\"F(?P&lt;frame_id&gt;\\d+)_\"\n        r\"(?P&lt;date&gt;\\d{8})_\"\n        r\"(?P&lt;satellite&gt;S1[AB])_\"\n        r\"v(?P&lt;version&gt;[\\d.]+)_\"\n        r\"(?P&lt;layer&gt;[\\w_]+)\"\n        r\"\\.tif$\"\n    )\n\n    LAYER_TYPES = [\n        \"dem\",\n        \"line_of_sight_enu\",\n        \"layover_shadow_mask\",\n    ]\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate layer after construction.\"\"\"\n        self.path = Path(self.path)\n\n        if self.frame_id &lt;= 0:\n            raise ValueError(f\"frame_id must be positive, got {self.frame_id}\")\n\n    @classmethod\n    def from_path(cls, path: Path | str) -&gt; \"StaticLayer\":\n        \"\"\"Parse layer metadata from filename.\"\"\"\n        path = Path(path)\n        match = cls._PATTERN.match(path.name)\n\n        if not match:\n            raise ValueError(\n                f\"Filename does not match OPERA DISP-S1-STATIC pattern: {path.name}\"\n            )\n\n        return cls(\n            path=path,\n            frame_id=int(match.group(\"frame_id\")),\n            reference_date=datetime.strptime(match.group(\"date\"), \"%Y%m%d\"),\n            satellite=match.group(\"satellite\"),\n            version=match.group(\"version\"),\n            layer_type=match.group(\"layer\"),\n        )\n\n    @property\n    def num_bands(self) -&gt; int:\n        \"\"\"Get number of bands in layer.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            return src.count\n\n    def read(self, band: int = 1, masked: bool = True) -&gt; np.ndarray:\n        \"\"\"Read single band data.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            if band &lt; 1 or band &gt; src.count:\n                raise ValueError(\n                    f\"Band {band} out of range. File has {src.count} bands.\"\n                )\n\n            data = src.read(band)\n\n            if masked and src.nodata is not None:\n                data = np.ma.masked_equal(data, src.nodata)\n\n        return data\n\n    def read_bands(self, masked: bool = True) -&gt; list[np.ndarray]:\n        \"\"\"Read all bands.\n\n        Parameters\n        ----------\n        masked : bool, optional\n            If True, return masked arrays with nodata values masked.\n            Default is True.\n\n        Returns\n        -------\n        list[np.ndarray]\n            List of arrays, one per band.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Read DEM (single band)\n        &gt;&gt;&gt; dem_layer = StaticLayer.from_path(\"..._dem.tif\")\n        &gt;&gt;&gt; bands = dem_layer.read_bands()\n        &gt;&gt;&gt; dem = bands[0]\n\n        &gt;&gt;&gt; # Read LOS vectors (three bands)\n        &gt;&gt;&gt; los_layer = StaticLayer.from_path(\"..._line_of_sight_enu.tif\")\n        &gt;&gt;&gt; bands = los_layer.read_bands()\n        &gt;&gt;&gt; east, north, up = bands[0], bands[1], bands[2]\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            bands = []\n            for band_idx in range(1, src.count + 1):\n                data = src.read(band_idx)\n                if masked and src.nodata is not None:\n                    data = np.ma.masked_equal(data, src.nodata)\n                bands.append(data)\n\n        return bands\n\n    def to_dataset(self) -&gt; xr.Dataset:\n        \"\"\"Convert raster to xarray Dataset.\"\"\"\n        # Get shape and transform using existing methods\n        height, width = self.get_shape()\n        transform = self.get_transform()\n\n        # Generate x and y coordinates from transform\n        x_coords = np.arange(width) * transform[0] + transform[2] + transform[0] / 2\n        y_coords = np.arange(height) * transform[4] + transform[5] + transform[4] / 2\n\n        # Create coordinates dict\n        coords = {\n            \"y\": ([\"y\"], y_coords),\n            \"x\": ([\"x\"], x_coords),\n        }\n\n        # Create data variables\n        data_vars = {}\n        nodata = self.get_nodata()\n\n        if self.num_bands == 1:\n            # Single band - use layer_type as variable name\n            data = self.read(band=1, masked=False)\n            if nodata is not None:\n                data = np.where(data == nodata, np.nan, data)\n            data_vars[self.layer_type] = ([\"y\", \"x\"], data)\n\n        elif self.layer_type == \"line_of_sight_enu\" and self.num_bands == 3:\n            # Special case: LOS vectors - create three variables\n            band_names = [\"los_east\", \"los_north\", \"los_up\"]\n            bands = self.read_bands(masked=False)\n\n            for name, data in zip(band_names, bands):\n                if nodata is not None:\n                    data = np.where(data == nodata, np.nan, data)\n                data_vars[name] = ([\"y\", \"x\"], data)\n\n        else:\n            # Generic multi-band: use band dimension\n            bands = self.read_bands(masked=False)\n            all_data = np.stack(bands)\n\n            if nodata is not None:\n                all_data = np.where(all_data == nodata, np.nan, all_data)\n\n            coords[\"band\"] = ([\"band\"], np.arange(1, self.num_bands + 1))\n            data_vars[self.layer_type] = ([\"band\", \"y\", \"x\"], all_data)\n\n        # Create dataset\n        ds = xr.Dataset(data_vars=data_vars, coords=coords)\n\n        # Add attributes\n        ds.attrs[\"frame_id\"] = self.frame_id\n        ds.attrs[\"satellite\"] = self.satellite\n        ds.attrs[\"version\"] = self.version\n        ds.attrs[\"reference_date\"] = self.reference_date.isoformat()\n        ds.attrs[\"layer_type\"] = self.layer_type\n\n        # Add CRS information using existing method\n        crs = self.get_crs()\n        if crs:\n            ds.attrs[\"crs_wkt\"] = crs.to_wkt()\n            epsg = self.get_epsg()\n            if epsg:\n                ds.attrs[\"epsg\"] = epsg\n\n        # Add transform\n        ds.attrs[\"transform\"] = list(transform)\n        ds.attrs[\"nodata\"] = nodata\n\n        # Add coordinate attributes\n        ds[\"x\"].attrs[\"standard_name\"] = \"projection_x_coordinate\"\n        ds[\"x\"].attrs[\"long_name\"] = \"x coordinate of projection\"\n        ds[\"x\"].attrs[\"units\"] = \"m\"\n\n        ds[\"y\"].attrs[\"standard_name\"] = \"projection_y_coordinate\"\n        ds[\"y\"].attrs[\"long_name\"] = \"y coordinate of projection\"\n        ds[\"y\"].attrs[\"units\"] = \"m\"\n\n        # Add variable-specific attributes\n        if self.layer_type == \"dem\":\n            ds[\"dem\"].attrs[\"units\"] = \"m\"\n            ds[\"dem\"].attrs[\"long_name\"] = \"Digital Elevation Model\"\n        elif self.layer_type == \"line_of_sight_enu\":\n            ds[\"los_east\"].attrs[\"long_name\"] = \"LOS unit vector - East component\"\n            ds[\"los_north\"].attrs[\"long_name\"] = \"LOS unit vector - North component\"\n            ds[\"los_up\"].attrs[\"long_name\"] = \"LOS unit vector - Up component\"\n        elif self.layer_type == \"layover_shadow_mask\":\n            ds[\"layover_shadow_mask\"].attrs[\"long_name\"] = \"Layover and Shadow Mask\"\n            ds[\"layover_shadow_mask\"].attrs[\n                \"description\"\n            ] = \"0=valid, 1=layover, 2=shadow\"\n\n        return ds\n\n    def compute_incidence_angle(\n        self,\n        fill_value: float = 0.0,\n        dtype: np.dtype = np.float32,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute incidence angle from LOS up component.\n\n        Only valid for line_of_sight_enu layers.\n\n        Parameters\n        ----------\n        fill_value : float, optional\n            Value to use for masked/invalid pixels. Default is 0.0.\n        dtype : np.dtype, optional\n            Output data type. Default is np.float32.\n\n        Returns\n        -------\n        np.ndarray\n            Incidence angle in degrees (0-90\u00b0).\n\n        Raises\n        ------\n        ValueError\n            If not a line_of_sight_enu layer or wrong number of bands.\n\n        \"\"\"\n        if self.layer_type != \"line_of_sight_enu\":\n            raise ValueError(\n                \"compute_incidence_angle() only valid for line_of_sight_enu layers, \"\n                f\"got {self.layer_type}\"\n            )\n\n        if self.num_bands != 3:\n            raise ValueError(f\"Expected 3 bands for LOS ENU, got {self.num_bands}\")\n\n        # Get LOS up component (band 3)\n        bands = self.read_bands()\n        los_up = bands[2]\n\n        # Handle masked arrays\n        if isinstance(los_up, np.ma.MaskedArray):\n            los_up_data = los_up.data\n            mask = los_up.mask\n        else:\n            los_up_data = los_up\n            mask = None\n\n        # Clip to valid range [-1, 1] to avoid arccos domain errors\n        los_up_clipped = np.clip(los_up_data, -1.0, 1.0)\n\n        # Compute incidence angle in degrees\n        incidence_angle = np.rad2deg(np.arccos(los_up_clipped))\n\n        # Apply mask if present\n        if mask is not None:\n            incidence_angle = np.where(mask, fill_value, incidence_angle)\n\n        return incidence_angle.astype(dtype)\n\n    def export_incidence_angle(\n        self,\n        output_path: Path | str,\n        fill_value: float = 0.0,\n        nodata: float | None = 0.0,\n        compress: str = \"DEFLATE\",\n        **kwargs,\n    ) -&gt; Path:\n        \"\"\"Compute and export incidence angle to GeoTIFF.\"\"\"\n        if self.layer_type != \"line_of_sight_enu\":\n            raise ValueError(\n                \"export_incidence_angle() only valid for line_of_sight_enu layers, \"\n                f\"got {self.layer_type}\"\n            )\n\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Compute incidence angle\n        incidence_angle = self.compute_incidence_angle(fill_value=fill_value)\n\n        # Write GeoTIFF\n        profile = {\n            \"driver\": \"GTiff\",\n            \"height\": incidence_angle.shape[0],\n            \"width\": incidence_angle.shape[1],\n            \"count\": 1,\n            \"dtype\": incidence_angle.dtype,\n            \"crs\": self.get_crs(),\n            \"transform\": self.get_transform(),\n            \"nodata\": nodata,\n            \"compress\": compress,\n            \"tiled\": True,\n            \"blockxsize\": 512,\n            \"blockysize\": 512,\n            **kwargs,\n        }\n\n        with rasterio.open(output_path, \"w\", **profile) as dst:\n            dst.write(incidence_angle, 1)\n            dst.set_band_description(1, \"Incidence angle (degrees)\")\n            dst.update_tags(\n                1,\n                units=\"degrees\",\n                description=\"Incidence angle computed from LOS up component\",\n            )\n\n        return output_path\n\n    def get_profile(self) -&gt; dict:\n        \"\"\"Get rasterio profile.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            return src.profile\n\n    def get_transform(self) -&gt; Affine:\n        \"\"\"Get affine transform.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            return src.transform\n\n    def get_crs(self) -&gt; CRS:\n        \"\"\"Get coordinate reference system.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            return src.crs\n\n    def get_epsg(self) -&gt; int | None:\n        \"\"\"Get EPSG code.\"\"\"\n        crs = self.get_crs()\n        return crs.to_epsg() if crs else None\n\n    def get_bounds(self) -&gt; dict[str, float]:\n        \"\"\"Get bounds in native projection.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            bounds = src.bounds\n            return {\n                \"left\": bounds.left,\n                \"bottom\": bounds.bottom,\n                \"right\": bounds.right,\n                \"top\": bounds.top,\n            }\n\n    def get_bounds_wgs84(self) -&gt; dict[str, float]:\n        \"\"\"Get bounds transformed to WGS84.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            bounds = src.bounds\n            west, south, east, north = transform_bounds(\n                src.crs,\n                CRS.from_epsg(4326),\n                bounds.left,\n                bounds.bottom,\n                bounds.right,\n                bounds.top,\n            )\n\n        return {\n            \"west\": west,\n            \"south\": south,\n            \"east\": east,\n            \"north\": north,\n        }\n\n    def get_shape(self) -&gt; tuple[int, int]:\n        \"\"\"Get array shape.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            return (src.height, src.width)\n\n    def get_nodata(self) -&gt; float | None:\n        \"\"\"Get nodata value.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n        with rasterio.open(self.path) as src:\n            return src.nodata\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Layer filename.\"\"\"\n        return self.path.name\n\n    @property\n    def exists(self) -&gt; bool:\n        \"\"\"Check if layer file exists.\"\"\"\n        return self.path.exists()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Concise string representation.\"\"\"\n        band_info = f\", bands={self.num_bands}\" if self.exists else \"\"\n        return f\"StaticLayer(frame={self.frame_id}, layer={self.layer_type}{band_info})\"\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.exists","title":"exists  <code>property</code>","text":"<pre><code>exists: bool\n</code></pre> <p>Check if layer file exists.</p>"},{"location":"api/#cal_disp.product.StaticLayer.filename","title":"filename  <code>property</code>","text":"<pre><code>filename: str\n</code></pre> <p>Layer filename.</p>"},{"location":"api/#cal_disp.product.StaticLayer.num_bands","title":"num_bands  <code>property</code>","text":"<pre><code>num_bands: int\n</code></pre> <p>Get number of bands in layer.</p>"},{"location":"api/#cal_disp.product.StaticLayer.compute_incidence_angle","title":"compute_incidence_angle","text":"<pre><code>compute_incidence_angle(fill_value: float = 0.0, dtype: dtype = np.float32) -&gt; np.ndarray\n</code></pre> <p>Compute incidence angle from LOS up component.</p> <p>Only valid for line_of_sight_enu layers.</p> <p>Parameters:</p> Name Type Description Default <code>fill_value</code> <code>float</code> <p>Value to use for masked/invalid pixels. Default is 0.0.</p> <code>0.0</code> <code>dtype</code> <code>dtype</code> <p>Output data type. Default is np.float32.</p> <code>float32</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Incidence angle in degrees (0-90\u00b0).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If not a line_of_sight_enu layer or wrong number of bands.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def compute_incidence_angle(\n    self,\n    fill_value: float = 0.0,\n    dtype: np.dtype = np.float32,\n) -&gt; np.ndarray:\n    \"\"\"Compute incidence angle from LOS up component.\n\n    Only valid for line_of_sight_enu layers.\n\n    Parameters\n    ----------\n    fill_value : float, optional\n        Value to use for masked/invalid pixels. Default is 0.0.\n    dtype : np.dtype, optional\n        Output data type. Default is np.float32.\n\n    Returns\n    -------\n    np.ndarray\n        Incidence angle in degrees (0-90\u00b0).\n\n    Raises\n    ------\n    ValueError\n        If not a line_of_sight_enu layer or wrong number of bands.\n\n    \"\"\"\n    if self.layer_type != \"line_of_sight_enu\":\n        raise ValueError(\n            \"compute_incidence_angle() only valid for line_of_sight_enu layers, \"\n            f\"got {self.layer_type}\"\n        )\n\n    if self.num_bands != 3:\n        raise ValueError(f\"Expected 3 bands for LOS ENU, got {self.num_bands}\")\n\n    # Get LOS up component (band 3)\n    bands = self.read_bands()\n    los_up = bands[2]\n\n    # Handle masked arrays\n    if isinstance(los_up, np.ma.MaskedArray):\n        los_up_data = los_up.data\n        mask = los_up.mask\n    else:\n        los_up_data = los_up\n        mask = None\n\n    # Clip to valid range [-1, 1] to avoid arccos domain errors\n    los_up_clipped = np.clip(los_up_data, -1.0, 1.0)\n\n    # Compute incidence angle in degrees\n    incidence_angle = np.rad2deg(np.arccos(los_up_clipped))\n\n    # Apply mask if present\n    if mask is not None:\n        incidence_angle = np.where(mask, fill_value, incidence_angle)\n\n    return incidence_angle.astype(dtype)\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.export_incidence_angle","title":"export_incidence_angle","text":"<pre><code>export_incidence_angle(output_path: Path | str, fill_value: float = 0.0, nodata: float | None = 0.0, compress: str = 'DEFLATE', **kwargs) -&gt; Path\n</code></pre> <p>Compute and export incidence angle to GeoTIFF.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def export_incidence_angle(\n    self,\n    output_path: Path | str,\n    fill_value: float = 0.0,\n    nodata: float | None = 0.0,\n    compress: str = \"DEFLATE\",\n    **kwargs,\n) -&gt; Path:\n    \"\"\"Compute and export incidence angle to GeoTIFF.\"\"\"\n    if self.layer_type != \"line_of_sight_enu\":\n        raise ValueError(\n            \"export_incidence_angle() only valid for line_of_sight_enu layers, \"\n            f\"got {self.layer_type}\"\n        )\n\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Compute incidence angle\n    incidence_angle = self.compute_incidence_angle(fill_value=fill_value)\n\n    # Write GeoTIFF\n    profile = {\n        \"driver\": \"GTiff\",\n        \"height\": incidence_angle.shape[0],\n        \"width\": incidence_angle.shape[1],\n        \"count\": 1,\n        \"dtype\": incidence_angle.dtype,\n        \"crs\": self.get_crs(),\n        \"transform\": self.get_transform(),\n        \"nodata\": nodata,\n        \"compress\": compress,\n        \"tiled\": True,\n        \"blockxsize\": 512,\n        \"blockysize\": 512,\n        **kwargs,\n    }\n\n    with rasterio.open(output_path, \"w\", **profile) as dst:\n        dst.write(incidence_angle, 1)\n        dst.set_band_description(1, \"Incidence angle (degrees)\")\n        dst.update_tags(\n            1,\n            units=\"degrees\",\n            description=\"Incidence angle computed from LOS up component\",\n        )\n\n    return output_path\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(path: Path | str) -&gt; StaticLayer\n</code></pre> <p>Parse layer metadata from filename.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>@classmethod\ndef from_path(cls, path: Path | str) -&gt; \"StaticLayer\":\n    \"\"\"Parse layer metadata from filename.\"\"\"\n    path = Path(path)\n    match = cls._PATTERN.match(path.name)\n\n    if not match:\n        raise ValueError(\n            f\"Filename does not match OPERA DISP-S1-STATIC pattern: {path.name}\"\n        )\n\n    return cls(\n        path=path,\n        frame_id=int(match.group(\"frame_id\")),\n        reference_date=datetime.strptime(match.group(\"date\"), \"%Y%m%d\"),\n        satellite=match.group(\"satellite\"),\n        version=match.group(\"version\"),\n        layer_type=match.group(\"layer\"),\n    )\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_bounds","title":"get_bounds","text":"<pre><code>get_bounds() -&gt; dict[str, float]\n</code></pre> <p>Get bounds in native projection.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_bounds(self) -&gt; dict[str, float]:\n    \"\"\"Get bounds in native projection.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        bounds = src.bounds\n        return {\n            \"left\": bounds.left,\n            \"bottom\": bounds.bottom,\n            \"right\": bounds.right,\n            \"top\": bounds.top,\n        }\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_bounds_wgs84","title":"get_bounds_wgs84","text":"<pre><code>get_bounds_wgs84() -&gt; dict[str, float]\n</code></pre> <p>Get bounds transformed to WGS84.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_bounds_wgs84(self) -&gt; dict[str, float]:\n    \"\"\"Get bounds transformed to WGS84.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        bounds = src.bounds\n        west, south, east, north = transform_bounds(\n            src.crs,\n            CRS.from_epsg(4326),\n            bounds.left,\n            bounds.bottom,\n            bounds.right,\n            bounds.top,\n        )\n\n    return {\n        \"west\": west,\n        \"south\": south,\n        \"east\": east,\n        \"north\": north,\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_crs","title":"get_crs","text":"<pre><code>get_crs() -&gt; CRS\n</code></pre> <p>Get coordinate reference system.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_crs(self) -&gt; CRS:\n    \"\"\"Get coordinate reference system.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        return src.crs\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_epsg","title":"get_epsg","text":"<pre><code>get_epsg() -&gt; int | None\n</code></pre> <p>Get EPSG code.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_epsg(self) -&gt; int | None:\n    \"\"\"Get EPSG code.\"\"\"\n    crs = self.get_crs()\n    return crs.to_epsg() if crs else None\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_nodata","title":"get_nodata","text":"<pre><code>get_nodata() -&gt; float | None\n</code></pre> <p>Get nodata value.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_nodata(self) -&gt; float | None:\n    \"\"\"Get nodata value.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        return src.nodata\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_profile","title":"get_profile","text":"<pre><code>get_profile() -&gt; dict\n</code></pre> <p>Get rasterio profile.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_profile(self) -&gt; dict:\n    \"\"\"Get rasterio profile.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        return src.profile\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_shape","title":"get_shape","text":"<pre><code>get_shape() -&gt; tuple[int, int]\n</code></pre> <p>Get array shape.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_shape(self) -&gt; tuple[int, int]:\n    \"\"\"Get array shape.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        return (src.height, src.width)\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.get_transform","title":"get_transform","text":"<pre><code>get_transform() -&gt; Affine\n</code></pre> <p>Get affine transform.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def get_transform(self) -&gt; Affine:\n    \"\"\"Get affine transform.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        return src.transform\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.read","title":"read","text":"<pre><code>read(band: int = 1, masked: bool = True) -&gt; np.ndarray\n</code></pre> <p>Read single band data.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def read(self, band: int = 1, masked: bool = True) -&gt; np.ndarray:\n    \"\"\"Read single band data.\"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        if band &lt; 1 or band &gt; src.count:\n            raise ValueError(\n                f\"Band {band} out of range. File has {src.count} bands.\"\n            )\n\n        data = src.read(band)\n\n        if masked and src.nodata is not None:\n            data = np.ma.masked_equal(data, src.nodata)\n\n    return data\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.read_bands","title":"read_bands","text":"<pre><code>read_bands(masked: bool = True) -&gt; list[np.ndarray]\n</code></pre> <p>Read all bands.</p> <p>Parameters:</p> Name Type Description Default <code>masked</code> <code>bool</code> <p>If True, return masked arrays with nodata values masked. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>List of arrays, one per band.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Read DEM (single band)\n&gt;&gt;&gt; dem_layer = StaticLayer.from_path(\"..._dem.tif\")\n&gt;&gt;&gt; bands = dem_layer.read_bands()\n&gt;&gt;&gt; dem = bands[0]\n</code></pre> <pre><code>&gt;&gt;&gt; # Read LOS vectors (three bands)\n&gt;&gt;&gt; los_layer = StaticLayer.from_path(\"..._line_of_sight_enu.tif\")\n&gt;&gt;&gt; bands = los_layer.read_bands()\n&gt;&gt;&gt; east, north, up = bands[0], bands[1], bands[2]\n</code></pre> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def read_bands(self, masked: bool = True) -&gt; list[np.ndarray]:\n    \"\"\"Read all bands.\n\n    Parameters\n    ----------\n    masked : bool, optional\n        If True, return masked arrays with nodata values masked.\n        Default is True.\n\n    Returns\n    -------\n    list[np.ndarray]\n        List of arrays, one per band.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Read DEM (single band)\n    &gt;&gt;&gt; dem_layer = StaticLayer.from_path(\"..._dem.tif\")\n    &gt;&gt;&gt; bands = dem_layer.read_bands()\n    &gt;&gt;&gt; dem = bands[0]\n\n    &gt;&gt;&gt; # Read LOS vectors (three bands)\n    &gt;&gt;&gt; los_layer = StaticLayer.from_path(\"..._line_of_sight_enu.tif\")\n    &gt;&gt;&gt; bands = los_layer.read_bands()\n    &gt;&gt;&gt; east, north, up = bands[0], bands[1], bands[2]\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Layer file not found: {self.path}\")\n\n    with rasterio.open(self.path) as src:\n        bands = []\n        for band_idx in range(1, src.count + 1):\n            data = src.read(band_idx)\n            if masked and src.nodata is not None:\n                data = np.ma.masked_equal(data, src.nodata)\n            bands.append(data)\n\n    return bands\n</code></pre>"},{"location":"api/#cal_disp.product.StaticLayer.to_dataset","title":"to_dataset","text":"<pre><code>to_dataset() -&gt; xr.Dataset\n</code></pre> <p>Convert raster to xarray Dataset.</p> Source code in <code>src/cal_disp/product/_static.py</code> <pre><code>def to_dataset(self) -&gt; xr.Dataset:\n    \"\"\"Convert raster to xarray Dataset.\"\"\"\n    # Get shape and transform using existing methods\n    height, width = self.get_shape()\n    transform = self.get_transform()\n\n    # Generate x and y coordinates from transform\n    x_coords = np.arange(width) * transform[0] + transform[2] + transform[0] / 2\n    y_coords = np.arange(height) * transform[4] + transform[5] + transform[4] / 2\n\n    # Create coordinates dict\n    coords = {\n        \"y\": ([\"y\"], y_coords),\n        \"x\": ([\"x\"], x_coords),\n    }\n\n    # Create data variables\n    data_vars = {}\n    nodata = self.get_nodata()\n\n    if self.num_bands == 1:\n        # Single band - use layer_type as variable name\n        data = self.read(band=1, masked=False)\n        if nodata is not None:\n            data = np.where(data == nodata, np.nan, data)\n        data_vars[self.layer_type] = ([\"y\", \"x\"], data)\n\n    elif self.layer_type == \"line_of_sight_enu\" and self.num_bands == 3:\n        # Special case: LOS vectors - create three variables\n        band_names = [\"los_east\", \"los_north\", \"los_up\"]\n        bands = self.read_bands(masked=False)\n\n        for name, data in zip(band_names, bands):\n            if nodata is not None:\n                data = np.where(data == nodata, np.nan, data)\n            data_vars[name] = ([\"y\", \"x\"], data)\n\n    else:\n        # Generic multi-band: use band dimension\n        bands = self.read_bands(masked=False)\n        all_data = np.stack(bands)\n\n        if nodata is not None:\n            all_data = np.where(all_data == nodata, np.nan, all_data)\n\n        coords[\"band\"] = ([\"band\"], np.arange(1, self.num_bands + 1))\n        data_vars[self.layer_type] = ([\"band\", \"y\", \"x\"], all_data)\n\n    # Create dataset\n    ds = xr.Dataset(data_vars=data_vars, coords=coords)\n\n    # Add attributes\n    ds.attrs[\"frame_id\"] = self.frame_id\n    ds.attrs[\"satellite\"] = self.satellite\n    ds.attrs[\"version\"] = self.version\n    ds.attrs[\"reference_date\"] = self.reference_date.isoformat()\n    ds.attrs[\"layer_type\"] = self.layer_type\n\n    # Add CRS information using existing method\n    crs = self.get_crs()\n    if crs:\n        ds.attrs[\"crs_wkt\"] = crs.to_wkt()\n        epsg = self.get_epsg()\n        if epsg:\n            ds.attrs[\"epsg\"] = epsg\n\n    # Add transform\n    ds.attrs[\"transform\"] = list(transform)\n    ds.attrs[\"nodata\"] = nodata\n\n    # Add coordinate attributes\n    ds[\"x\"].attrs[\"standard_name\"] = \"projection_x_coordinate\"\n    ds[\"x\"].attrs[\"long_name\"] = \"x coordinate of projection\"\n    ds[\"x\"].attrs[\"units\"] = \"m\"\n\n    ds[\"y\"].attrs[\"standard_name\"] = \"projection_y_coordinate\"\n    ds[\"y\"].attrs[\"long_name\"] = \"y coordinate of projection\"\n    ds[\"y\"].attrs[\"units\"] = \"m\"\n\n    # Add variable-specific attributes\n    if self.layer_type == \"dem\":\n        ds[\"dem\"].attrs[\"units\"] = \"m\"\n        ds[\"dem\"].attrs[\"long_name\"] = \"Digital Elevation Model\"\n    elif self.layer_type == \"line_of_sight_enu\":\n        ds[\"los_east\"].attrs[\"long_name\"] = \"LOS unit vector - East component\"\n        ds[\"los_north\"].attrs[\"long_name\"] = \"LOS unit vector - North component\"\n        ds[\"los_up\"].attrs[\"long_name\"] = \"LOS unit vector - Up component\"\n    elif self.layer_type == \"layover_shadow_mask\":\n        ds[\"layover_shadow_mask\"].attrs[\"long_name\"] = \"Layover and Shadow Mask\"\n        ds[\"layover_shadow_mask\"].attrs[\n            \"description\"\n        ] = \"0=valid, 1=layover, 2=shadow\"\n\n    return ds\n</code></pre>"},{"location":"api/#cal_disp.product.TropoProduct","title":"TropoProduct  <code>dataclass</code>","text":"<p>OPERA TROPO-ZENITH tropospheric delay product.</p> <p>Minimal class for managing OPERA tropospheric products. Processing functions are standalone for composability.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; product = TropoProduct.from_path(\"tropo.nc\")\n&gt;&gt;&gt; ds = product.open_dataset()\n&gt;&gt;&gt; total = product.get_total_delay()\n</code></pre> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>@dataclass\nclass TropoProduct:\n    \"\"\"OPERA TROPO-ZENITH tropospheric delay product.\n\n    Minimal class for managing OPERA tropospheric products.\n    Processing functions are standalone for composability.\n\n    Examples\n    --------\n    &gt;&gt;&gt; product = TropoProduct.from_path(\"tropo.nc\")\n    &gt;&gt;&gt; ds = product.open_dataset()\n    &gt;&gt;&gt; total = product.get_total_delay()\n\n    \"\"\"\n\n    path: Path\n    date: datetime\n    production_date: datetime\n    model: str\n    version: str\n\n    _PATTERN = re.compile(\n        r\"OPERA_L4_TROPO-ZENITH_\"\n        r\"(?P&lt;date&gt;\\d{8}T\\d{6}Z)_\"\n        r\"(?P&lt;production&gt;\\d{8}T\\d{6}Z)_\"\n        r\"(?P&lt;model&gt;\\w+)_\"\n        r\"v(?P&lt;version&gt;[\\d.]+)\"\n        r\"\\.nc$\"\n    )\n\n    TROPO_LAYERS = [\n        \"wet_delay\",\n        \"hydrostatic_delay\",\n    ]\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate product after construction.\"\"\"\n        self.path = Path(self.path)\n\n        if self.production_date &lt; self.date:\n            raise ValueError(\n                f\"Production date ({self.production_date}) cannot be before \"\n                f\"model date ({self.date})\"\n            )\n\n    @classmethod\n    def from_path(cls, path: Path | str) -&gt; \"TropoProduct\":\n        \"\"\"Parse product metadata from filename.\"\"\"\n        path = Path(path)\n        match = cls._PATTERN.match(path.name)\n\n        if not match:\n            raise ValueError(\n                f\"Filename does not match OPERA TROPO-ZENITH pattern: {path.name}\"\n            )\n\n        return cls(\n            path=path,\n            date=datetime.strptime(match.group(\"date\"), \"%Y%m%dT%H%M%SZ\"),\n            production_date=datetime.strptime(\n                match.group(\"production\"), \"%Y%m%dT%H%M%SZ\"\n            ),\n            model=match.group(\"model\"),\n            version=match.group(\"version\"),\n        )\n\n    def matches_date(self, target_date: datetime, hours: float = 6.0) -&gt; bool:\n        \"\"\"Check if product date is within time window of target date.\"\"\"\n        delta = abs(self.date - target_date)\n        return delta &lt;= timedelta(hours=hours)\n\n    def open_dataset(\n        self,\n        bounds: tuple[float, float, float, float] | None = None,\n        max_height: float | None = None,\n        bounds_crs: str = \"EPSG:4326\",\n        bounds_buffer: float = 0.0,\n    ) -&gt; xr.Dataset:\n        \"\"\"Open tropospheric delay dataset with optional subsetting.\n\n        Parameters\n        ----------\n        bounds : tuple[float, float, float, float] or None, optional\n            Spatial bounds as (west, south, east, north). Default is None.\n        max_height : float or None, optional\n            Maximum height in meters. Default is None.\n        bounds_crs : str, optional\n            CRS of bounds. Default is \"EPSG:4326\".\n        bounds_buffer : float, optional\n            Buffer to add to bounds in degrees (for lat/lon) or meters\n            (for projected CRS). Default is 0.0. Useful value: 0.2 for lat/lon.\n\n        Returns\n        -------\n        xr.Dataset\n            Tropospheric delay dataset.\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n        ds = xr.open_dataset(self.path, engine=\"h5netcdf\")\n\n        if bounds is not None:\n            # Apply buffer if requested\n            if bounds_buffer &gt; 0:\n                west, south, east, north = bounds\n                bounds = (\n                    west - bounds_buffer,\n                    south - bounds_buffer,\n                    east + bounds_buffer,\n                    north + bounds_buffer,\n                )\n            ds = self._subset_spatial(ds, bounds, bounds_crs)\n\n        if max_height is not None:\n            ds = self._subset_height(ds, max_height)\n\n        return ds\n\n    def get_total_delay(\n        self,\n        time_idx: int = 0,\n        bounds: tuple[float, float, float, float] | None = None,\n        max_height: float | None = None,\n        bounds_crs: str = \"EPSG:4326\",\n        bounds_buffer: float = 0.0,\n    ) -&gt; xr.DataArray:\n        \"\"\"Get total zenith delay (wet + hydrostatic).\n\n        Computes total delay as sum of wet and hydrostatic components.\n\n        Parameters\n        ----------\n        time_idx : int, optional\n            Time index to extract. Default is 0.\n        bounds : tuple[float, float, float, float] or None, optional\n            Spatial bounds for subsetting. Default is None.\n        max_height : float or None, optional\n            Maximum height for subsetting. Default is None.\n        bounds_crs : str, optional\n            CRS of bounds. Default is \"EPSG:4326\".\n        bounds_buffer : float, optional\n            Buffer to add to bounds. Default is 0.0.\n\n        Returns\n        -------\n        xr.DataArray\n            Total zenith delay with dimensions (height, latitude, longitude).\n\n        \"\"\"\n        ds = self.open_dataset(\n            bounds=bounds,\n            max_height=max_height,\n            bounds_crs=bounds_crs,\n            bounds_buffer=bounds_buffer,\n        )\n\n        # Compute total delay from wet + hydrostatic\n        if \"wet_delay\" not in ds:\n            raise ValueError(\"wet_delay not found in dataset\")\n        if \"hydrostatic_delay\" not in ds:\n            raise ValueError(\"hydrostatic_delay not found in dataset\")\n\n        da = ds[\"wet_delay\"] + ds[\"hydrostatic_delay\"]\n        da.name = \"zenith_total_delay\"\n        da.attrs.update(\n            {\n                \"long_name\": \"Total zenith tropospheric delay\",\n                \"units\": \"meters\",\n                \"description\": \"Sum of wet and hydrostatic components\",\n            }\n        )\n\n        # Preserve spatial reference\n        if \"spatial_ref\" in ds:\n            da = da.assign_coords({\"spatial_ref\": ds[\"spatial_ref\"]})\n\n        # Handle time dimension\n        if \"time\" in da.dims:\n            n_times = len(da.time)\n            if abs(time_idx) &gt;= n_times:\n                raise ValueError(\n                    f\"time_idx {time_idx} out of range for {n_times} timesteps\"\n                )\n            da = da.isel(time=time_idx)\n\n        return da\n\n    def _subset_spatial(\n        self,\n        ds: xr.Dataset,\n        bounds: tuple[float, float, float, float],\n        bounds_crs: str,\n    ) -&gt; xr.Dataset:\n        \"\"\"Subset dataset to spatial bounds.\"\"\"\n        west, south, east, north = bounds\n\n        ds_crs_wkt = ds.spatial_ref.attrs.get(\"crs_wkt\")\n        if ds_crs_wkt is None:\n            raise ValueError(\"Dataset missing CRS information\")\n\n        ds_crs = CRS.from_wkt(ds_crs_wkt)\n\n        # Transform bounds to dataset CRS if needed\n        if bounds_crs != ds_crs.to_string():\n            left, bottom, right, top = rio_transform_bounds(\n                CRS.from_string(bounds_crs),\n                ds_crs,\n                west,\n                south,\n                east,\n                north,\n            )\n        else:\n            left, bottom, right, top = west, south, east, north\n\n        # Use latitude/longitude for subsetting\n        ds_subset = ds.sel(\n            longitude=slice(left, right),\n            latitude=slice(top, bottom),\n        )\n\n        return ds_subset\n\n    def _subset_height(self, ds: xr.Dataset, max_height: float) -&gt; xr.Dataset:\n        \"\"\"Subset dataset by maximum height.\"\"\"\n        if \"height\" not in ds.dims:\n            return ds\n\n        return ds.where(ds[\"height\"] &lt;= max_height, drop=True)\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Product filename.\"\"\"\n        return self.path.name\n\n    @property\n    def exists(self) -&gt; bool:\n        \"\"\"Check if product file exists.\"\"\"\n        return self.path.exists()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation.\"\"\"\n        return f\"TropoProduct(date={self.date.isoformat()}, model={self.model})\"\n</code></pre>"},{"location":"api/#cal_disp.product.TropoProduct.exists","title":"exists  <code>property</code>","text":"<pre><code>exists: bool\n</code></pre> <p>Check if product file exists.</p>"},{"location":"api/#cal_disp.product.TropoProduct.filename","title":"filename  <code>property</code>","text":"<pre><code>filename: str\n</code></pre> <p>Product filename.</p>"},{"location":"api/#cal_disp.product.TropoProduct.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(path: Path | str) -&gt; TropoProduct\n</code></pre> <p>Parse product metadata from filename.</p> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>@classmethod\ndef from_path(cls, path: Path | str) -&gt; \"TropoProduct\":\n    \"\"\"Parse product metadata from filename.\"\"\"\n    path = Path(path)\n    match = cls._PATTERN.match(path.name)\n\n    if not match:\n        raise ValueError(\n            f\"Filename does not match OPERA TROPO-ZENITH pattern: {path.name}\"\n        )\n\n    return cls(\n        path=path,\n        date=datetime.strptime(match.group(\"date\"), \"%Y%m%dT%H%M%SZ\"),\n        production_date=datetime.strptime(\n            match.group(\"production\"), \"%Y%m%dT%H%M%SZ\"\n        ),\n        model=match.group(\"model\"),\n        version=match.group(\"version\"),\n    )\n</code></pre>"},{"location":"api/#cal_disp.product.TropoProduct.get_total_delay","title":"get_total_delay","text":"<pre><code>get_total_delay(time_idx: int = 0, bounds: tuple[float, float, float, float] | None = None, max_height: float | None = None, bounds_crs: str = 'EPSG:4326', bounds_buffer: float = 0.0) -&gt; xr.DataArray\n</code></pre> <p>Get total zenith delay (wet + hydrostatic).</p> <p>Computes total delay as sum of wet and hydrostatic components.</p> <p>Parameters:</p> Name Type Description Default <code>time_idx</code> <code>int</code> <p>Time index to extract. Default is 0.</p> <code>0</code> <code>bounds</code> <code>tuple[float, float, float, float] or None</code> <p>Spatial bounds for subsetting. Default is None.</p> <code>None</code> <code>max_height</code> <code>float or None</code> <p>Maximum height for subsetting. Default is None.</p> <code>None</code> <code>bounds_crs</code> <code>str</code> <p>CRS of bounds. Default is \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>bounds_buffer</code> <code>float</code> <p>Buffer to add to bounds. Default is 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>Total zenith delay with dimensions (height, latitude, longitude).</p> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>def get_total_delay(\n    self,\n    time_idx: int = 0,\n    bounds: tuple[float, float, float, float] | None = None,\n    max_height: float | None = None,\n    bounds_crs: str = \"EPSG:4326\",\n    bounds_buffer: float = 0.0,\n) -&gt; xr.DataArray:\n    \"\"\"Get total zenith delay (wet + hydrostatic).\n\n    Computes total delay as sum of wet and hydrostatic components.\n\n    Parameters\n    ----------\n    time_idx : int, optional\n        Time index to extract. Default is 0.\n    bounds : tuple[float, float, float, float] or None, optional\n        Spatial bounds for subsetting. Default is None.\n    max_height : float or None, optional\n        Maximum height for subsetting. Default is None.\n    bounds_crs : str, optional\n        CRS of bounds. Default is \"EPSG:4326\".\n    bounds_buffer : float, optional\n        Buffer to add to bounds. Default is 0.0.\n\n    Returns\n    -------\n    xr.DataArray\n        Total zenith delay with dimensions (height, latitude, longitude).\n\n    \"\"\"\n    ds = self.open_dataset(\n        bounds=bounds,\n        max_height=max_height,\n        bounds_crs=bounds_crs,\n        bounds_buffer=bounds_buffer,\n    )\n\n    # Compute total delay from wet + hydrostatic\n    if \"wet_delay\" not in ds:\n        raise ValueError(\"wet_delay not found in dataset\")\n    if \"hydrostatic_delay\" not in ds:\n        raise ValueError(\"hydrostatic_delay not found in dataset\")\n\n    da = ds[\"wet_delay\"] + ds[\"hydrostatic_delay\"]\n    da.name = \"zenith_total_delay\"\n    da.attrs.update(\n        {\n            \"long_name\": \"Total zenith tropospheric delay\",\n            \"units\": \"meters\",\n            \"description\": \"Sum of wet and hydrostatic components\",\n        }\n    )\n\n    # Preserve spatial reference\n    if \"spatial_ref\" in ds:\n        da = da.assign_coords({\"spatial_ref\": ds[\"spatial_ref\"]})\n\n    # Handle time dimension\n    if \"time\" in da.dims:\n        n_times = len(da.time)\n        if abs(time_idx) &gt;= n_times:\n            raise ValueError(\n                f\"time_idx {time_idx} out of range for {n_times} timesteps\"\n            )\n        da = da.isel(time=time_idx)\n\n    return da\n</code></pre>"},{"location":"api/#cal_disp.product.TropoProduct.matches_date","title":"matches_date","text":"<pre><code>matches_date(target_date: datetime, hours: float = 6.0) -&gt; bool\n</code></pre> <p>Check if product date is within time window of target date.</p> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>def matches_date(self, target_date: datetime, hours: float = 6.0) -&gt; bool:\n    \"\"\"Check if product date is within time window of target date.\"\"\"\n    delta = abs(self.date - target_date)\n    return delta &lt;= timedelta(hours=hours)\n</code></pre>"},{"location":"api/#cal_disp.product.TropoProduct.open_dataset","title":"open_dataset","text":"<pre><code>open_dataset(bounds: tuple[float, float, float, float] | None = None, max_height: float | None = None, bounds_crs: str = 'EPSG:4326', bounds_buffer: float = 0.0) -&gt; xr.Dataset\n</code></pre> <p>Open tropospheric delay dataset with optional subsetting.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>tuple[float, float, float, float] or None</code> <p>Spatial bounds as (west, south, east, north). Default is None.</p> <code>None</code> <code>max_height</code> <code>float or None</code> <p>Maximum height in meters. Default is None.</p> <code>None</code> <code>bounds_crs</code> <code>str</code> <p>CRS of bounds. Default is \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>bounds_buffer</code> <code>float</code> <p>Buffer to add to bounds in degrees (for lat/lon) or meters (for projected CRS). Default is 0.0. Useful value: 0.2 for lat/lon.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Tropospheric delay dataset.</p> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>def open_dataset(\n    self,\n    bounds: tuple[float, float, float, float] | None = None,\n    max_height: float | None = None,\n    bounds_crs: str = \"EPSG:4326\",\n    bounds_buffer: float = 0.0,\n) -&gt; xr.Dataset:\n    \"\"\"Open tropospheric delay dataset with optional subsetting.\n\n    Parameters\n    ----------\n    bounds : tuple[float, float, float, float] or None, optional\n        Spatial bounds as (west, south, east, north). Default is None.\n    max_height : float or None, optional\n        Maximum height in meters. Default is None.\n    bounds_crs : str, optional\n        CRS of bounds. Default is \"EPSG:4326\".\n    bounds_buffer : float, optional\n        Buffer to add to bounds in degrees (for lat/lon) or meters\n        (for projected CRS). Default is 0.0. Useful value: 0.2 for lat/lon.\n\n    Returns\n    -------\n    xr.Dataset\n        Tropospheric delay dataset.\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"Product file not found: {self.path}\")\n\n    ds = xr.open_dataset(self.path, engine=\"h5netcdf\")\n\n    if bounds is not None:\n        # Apply buffer if requested\n        if bounds_buffer &gt; 0:\n            west, south, east, north = bounds\n            bounds = (\n                west - bounds_buffer,\n                south - bounds_buffer,\n                east + bounds_buffer,\n                north + bounds_buffer,\n            )\n        ds = self._subset_spatial(ds, bounds, bounds_crs)\n\n    if max_height is not None:\n        ds = self._subset_height(ds, max_height)\n\n    return ds\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid","title":"UnrGrid  <code>dataclass</code>","text":"<p>UNR GNSS grid data.</p> <p>Represents gridded GNSS velocity data from University of Nevada Reno. Data is stored as parquet with point geometries and metadata.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load from path (frame_id parsed if in filename)\n&gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n&gt;&gt;&gt; grid.frame_id\n8882\n</code></pre> <pre><code>&gt;&gt;&gt; # Load GeoDataFrame\n&gt;&gt;&gt; gdf = grid.load()\n&gt;&gt;&gt; gdf.columns\n['lon', 'lat', 'east', 'north', 'up', 'geometry', ...]\n</code></pre> <pre><code>&gt;&gt;&gt; # Get metadata\n&gt;&gt;&gt; meta = grid.get_metadata()\n&gt;&gt;&gt; meta['source']\n'UNR'\n</code></pre> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>@dataclass\nclass UnrGrid:\n    \"\"\"UNR GNSS grid data.\n\n    Represents gridded GNSS velocity data from University of Nevada Reno.\n    Data is stored as parquet with point geometries and metadata.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Load from path (frame_id parsed if in filename)\n    &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n    &gt;&gt;&gt; grid.frame_id\n    8882\n\n    &gt;&gt;&gt; # Load GeoDataFrame\n    &gt;&gt;&gt; gdf = grid.load()\n    &gt;&gt;&gt; gdf.columns\n    ['lon', 'lat', 'east', 'north', 'up', 'geometry', ...]\n\n    &gt;&gt;&gt; # Get metadata\n    &gt;&gt;&gt; meta = grid.get_metadata()\n    &gt;&gt;&gt; meta['source']\n    'UNR'\n\n    \"\"\"\n\n    path: Path\n    frame_id: int | None = None\n\n    # Optional pattern to extract frame_id from filename\n    _PATTERN = re.compile(r\"frame[\\s_-]?(\\d+)\", re.IGNORECASE)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate grid after construction.\"\"\"\n        self.path = Path(self.path)\n\n    @classmethod\n    def from_path(cls, path: Path | str, frame_id: int | None = None) -&gt; \"UnrGrid\":\n        \"\"\"Create UnrGrid from parquet file path.\n\n        Parameters\n        ----------\n        path : Path or str\n            Path to UNR parquet file.\n        frame_id : int or None, optional\n            Frame ID. If None, attempts to parse from filename.\n            Default is None.\n\n        Returns\n        -------\n        UnrGrid\n            Grid instance.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Frame ID from filename\n        &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n        &gt;&gt;&gt; grid.frame_id\n        8882\n\n        &gt;&gt;&gt; # Explicit frame ID\n        &gt;&gt;&gt; grid = UnrGrid.from_path(\"custom_unr_data.parquet\", frame_id=8882)\n        &gt;&gt;&gt; grid.frame_id\n        8882\n\n        &gt;&gt;&gt; # No frame ID\n        &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_data.parquet\")\n        &gt;&gt;&gt; grid.frame_id is None\n        True\n\n        \"\"\"\n        path = Path(path)\n\n        # Try to parse frame_id from filename if not provided\n        if frame_id is None:\n            match = cls._PATTERN.search(path.name)\n            if match:\n                frame_id = int(match.group(1))\n\n        return cls(path=path, frame_id=frame_id)\n\n    def load(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Load UNR grid as GeoDataFrame.\n\n        Returns\n        -------\n        gpd.GeoDataFrame\n            GeoDataFrame with point geometries and velocity data.\n\n        Raises\n        ------\n        FileNotFoundError\n            If parquet file does not exist.\n\n        Examples\n        --------\n        &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n        &gt;&gt;&gt; gdf = grid.load()\n        &gt;&gt;&gt; gdf.crs\n        'EPSG:4326'\n        &gt;&gt;&gt; gdf[['lon', 'lat', 'east', 'north', 'up']].head()\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"UNR grid file not found: {self.path}\")\n\n        # Load parquet as DataFrame\n        df = pd.read_parquet(self.path)\n\n        # Create GeoDataFrame with point geometries\n        gdf = gpd.GeoDataFrame(\n            df,\n            geometry=gpd.points_from_xy(x=df.lon, y=df.lat),\n            crs=\"EPSG:4326\",\n        )\n\n        return gdf\n\n    def get_metadata(self) -&gt; dict[str, str]:\n        \"\"\"Extract metadata from parquet file.\n\n        Returns\n        -------\n        dict[str, str]\n            Metadata dictionary.\n\n        Examples\n        --------\n        &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n        &gt;&gt;&gt; meta = grid.get_metadata()\n        &gt;&gt;&gt; meta.keys()\n        dict_keys(['source', 'date_created', 'frame_id', ...])\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"UNR grid file not found: {self.path}\")\n\n        meta = pq.read_metadata(self.path).metadata\n\n        if meta is None:\n            return {}\n\n        metadata_dict = {k.decode(): v.decode() for k, v in meta.items()}\n\n        return metadata_dict\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Load as regular DataFrame without geometry.\n\n        Returns\n        -------\n        pd.DataFrame\n            DataFrame with lon, lat, and velocity columns.\n\n        \"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"UNR grid file not found: {self.path}\")\n\n        return pd.read_parquet(self.path)\n\n    def get_bounds(self) -&gt; dict[str, float]:\n        \"\"\"Get spatial bounds of grid.\n\n        Returns\n        -------\n        dict[str, float]\n            Dictionary with keys: west, south, east, north.\n\n        \"\"\"\n        gdf = self.load()\n        bounds = gdf.total_bounds  # (minx, miny, maxx, maxy)\n\n        return {\n            \"west\": bounds[0],\n            \"south\": bounds[1],\n            \"east\": bounds[2],\n            \"north\": bounds[3],\n        }\n\n    def get_grid_count(self) -&gt; int:\n        \"\"\"Get number of GNSS points in grid.\n\n        Returns\n        -------\n        int\n            Number of stations.\n\n        \"\"\"\n        df = self.to_dataframe()\n        grid_points = df.groupby(\"id\", as_index=False).first()\n        return len(grid_points)\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Grid filename.\"\"\"\n        return self.path.name\n\n    @property\n    def exists(self) -&gt; bool:\n        \"\"\"Check if grid file exists.\"\"\"\n        return self.path.exists()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation.\"\"\"\n        frame_str = f\"frame={self.frame_id}\" if self.frame_id else \"frame=None\"\n        return (\n            f\"UnrGrid({frame_str},\"\n            f\" points={self.get_grid_count() if self.exists else '?'})\"\n        )\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid.exists","title":"exists  <code>property</code>","text":"<pre><code>exists: bool\n</code></pre> <p>Check if grid file exists.</p>"},{"location":"api/#cal_disp.product.UnrGrid.filename","title":"filename  <code>property</code>","text":"<pre><code>filename: str\n</code></pre> <p>Grid filename.</p>"},{"location":"api/#cal_disp.product.UnrGrid.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(path: Path | str, frame_id: int | None = None) -&gt; UnrGrid\n</code></pre> <p>Create UnrGrid from parquet file path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path or str</code> <p>Path to UNR parquet file.</p> required <code>frame_id</code> <code>int or None</code> <p>Frame ID. If None, attempts to parse from filename. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>UnrGrid</code> <p>Grid instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Frame ID from filename\n&gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n&gt;&gt;&gt; grid.frame_id\n8882\n</code></pre> <pre><code>&gt;&gt;&gt; # Explicit frame ID\n&gt;&gt;&gt; grid = UnrGrid.from_path(\"custom_unr_data.parquet\", frame_id=8882)\n&gt;&gt;&gt; grid.frame_id\n8882\n</code></pre> <pre><code>&gt;&gt;&gt; # No frame ID\n&gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_data.parquet\")\n&gt;&gt;&gt; grid.frame_id is None\nTrue\n</code></pre> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>@classmethod\ndef from_path(cls, path: Path | str, frame_id: int | None = None) -&gt; \"UnrGrid\":\n    \"\"\"Create UnrGrid from parquet file path.\n\n    Parameters\n    ----------\n    path : Path or str\n        Path to UNR parquet file.\n    frame_id : int or None, optional\n        Frame ID. If None, attempts to parse from filename.\n        Default is None.\n\n    Returns\n    -------\n    UnrGrid\n        Grid instance.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Frame ID from filename\n    &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n    &gt;&gt;&gt; grid.frame_id\n    8882\n\n    &gt;&gt;&gt; # Explicit frame ID\n    &gt;&gt;&gt; grid = UnrGrid.from_path(\"custom_unr_data.parquet\", frame_id=8882)\n    &gt;&gt;&gt; grid.frame_id\n    8882\n\n    &gt;&gt;&gt; # No frame ID\n    &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_data.parquet\")\n    &gt;&gt;&gt; grid.frame_id is None\n    True\n\n    \"\"\"\n    path = Path(path)\n\n    # Try to parse frame_id from filename if not provided\n    if frame_id is None:\n        match = cls._PATTERN.search(path.name)\n        if match:\n            frame_id = int(match.group(1))\n\n    return cls(path=path, frame_id=frame_id)\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid.get_bounds","title":"get_bounds","text":"<pre><code>get_bounds() -&gt; dict[str, float]\n</code></pre> <p>Get spatial bounds of grid.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with keys: west, south, east, north.</p> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>def get_bounds(self) -&gt; dict[str, float]:\n    \"\"\"Get spatial bounds of grid.\n\n    Returns\n    -------\n    dict[str, float]\n        Dictionary with keys: west, south, east, north.\n\n    \"\"\"\n    gdf = self.load()\n    bounds = gdf.total_bounds  # (minx, miny, maxx, maxy)\n\n    return {\n        \"west\": bounds[0],\n        \"south\": bounds[1],\n        \"east\": bounds[2],\n        \"north\": bounds[3],\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid.get_grid_count","title":"get_grid_count","text":"<pre><code>get_grid_count() -&gt; int\n</code></pre> <p>Get number of GNSS points in grid.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of stations.</p> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>def get_grid_count(self) -&gt; int:\n    \"\"\"Get number of GNSS points in grid.\n\n    Returns\n    -------\n    int\n        Number of stations.\n\n    \"\"\"\n    df = self.to_dataframe()\n    grid_points = df.groupby(\"id\", as_index=False).first()\n    return len(grid_points)\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata() -&gt; dict[str, str]\n</code></pre> <p>Extract metadata from parquet file.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Metadata dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n&gt;&gt;&gt; meta = grid.get_metadata()\n&gt;&gt;&gt; meta.keys()\ndict_keys(['source', 'date_created', 'frame_id', ...])\n</code></pre> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>def get_metadata(self) -&gt; dict[str, str]:\n    \"\"\"Extract metadata from parquet file.\n\n    Returns\n    -------\n    dict[str, str]\n        Metadata dictionary.\n\n    Examples\n    --------\n    &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n    &gt;&gt;&gt; meta = grid.get_metadata()\n    &gt;&gt;&gt; meta.keys()\n    dict_keys(['source', 'date_created', 'frame_id', ...])\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"UNR grid file not found: {self.path}\")\n\n    meta = pq.read_metadata(self.path).metadata\n\n    if meta is None:\n        return {}\n\n    metadata_dict = {k.decode(): v.decode() for k, v in meta.items()}\n\n    return metadata_dict\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid.load","title":"load","text":"<pre><code>load() -&gt; gpd.GeoDataFrame\n</code></pre> <p>Load UNR grid as GeoDataFrame.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with point geometries and velocity data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If parquet file does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n&gt;&gt;&gt; gdf = grid.load()\n&gt;&gt;&gt; gdf.crs\n'EPSG:4326'\n&gt;&gt;&gt; gdf[['lon', 'lat', 'east', 'north', 'up']].head()\n</code></pre> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>def load(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Load UNR grid as GeoDataFrame.\n\n    Returns\n    -------\n    gpd.GeoDataFrame\n        GeoDataFrame with point geometries and velocity data.\n\n    Raises\n    ------\n    FileNotFoundError\n        If parquet file does not exist.\n\n    Examples\n    --------\n    &gt;&gt;&gt; grid = UnrGrid.from_path(\"unr_grid_frame8882.parquet\")\n    &gt;&gt;&gt; gdf = grid.load()\n    &gt;&gt;&gt; gdf.crs\n    'EPSG:4326'\n    &gt;&gt;&gt; gdf[['lon', 'lat', 'east', 'north', 'up']].head()\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"UNR grid file not found: {self.path}\")\n\n    # Load parquet as DataFrame\n    df = pd.read_parquet(self.path)\n\n    # Create GeoDataFrame with point geometries\n    gdf = gpd.GeoDataFrame(\n        df,\n        geometry=gpd.points_from_xy(x=df.lon, y=df.lat),\n        crs=\"EPSG:4326\",\n    )\n\n    return gdf\n</code></pre>"},{"location":"api/#cal_disp.product.UnrGrid.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe() -&gt; pd.DataFrame\n</code></pre> <p>Load as regular DataFrame without geometry.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with lon, lat, and velocity columns.</p> Source code in <code>src/cal_disp/product/_unr.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Load as regular DataFrame without geometry.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with lon, lat, and velocity columns.\n\n    \"\"\"\n    if not self.path.exists():\n        raise FileNotFoundError(f\"UNR grid file not found: {self.path}\")\n\n    return pd.read_parquet(self.path)\n</code></pre>"},{"location":"api/#cal_disp.product.bounds_contains","title":"bounds_contains","text":"<pre><code>bounds_contains(outer_bounds: tuple[float, float, float, float] | dict[str, float], inner_bounds: tuple[float, float, float, float] | dict[str, float], buffer: float = 0.0) -&gt; bool\n</code></pre> <p>Check if outer bounds completely contain inner bounds.</p> <p>Parameters:</p> Name Type Description Default <code>outer_bounds</code> <code>tuple or dict</code> <p>Outer bounds as (west, south, east, north) or dict with those keys.</p> required <code>inner_bounds</code> <code>tuple or dict</code> <p>Inner bounds as (west, south, east, north) or dict with those keys.</p> required <code>buffer</code> <code>float</code> <p>Buffer distance to require around inner bounds (in same units). Default is 0.0 (exact containment).</p> <code>0.0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if outer bounds completely contain inner bounds (with buffer).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Check if UNR grid covers DISP frame\n&gt;&gt;&gt; unr_bounds = (-97, 28, -93, 32)\n&gt;&gt;&gt; disp_bounds = (-96, 29, -94, 31)\n&gt;&gt;&gt; bounds_contains(unr_bounds, disp_bounds)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # With buffer requirement\n&gt;&gt;&gt; bounds_contains(unr_bounds, disp_bounds, buffer=0.5)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Dict format\n&gt;&gt;&gt; unr_bounds = {\"west\": -97, \"south\": 28, \"east\": -93, \"north\": 32}\n&gt;&gt;&gt; disp_bounds = {\"west\": -96, \"south\": 29, \"east\": -94, \"north\": 31}\n&gt;&gt;&gt; bounds_contains(unr_bounds, disp_bounds)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; # Does not contain\n&gt;&gt;&gt; small_bounds = (-95, 29, -94, 30)\n&gt;&gt;&gt; large_bounds = (-96, 28, -93, 31)\n&gt;&gt;&gt; bounds_contains(small_bounds, large_bounds)\nFalse\n</code></pre> Source code in <code>src/cal_disp/product/_utils.py</code> <pre><code>def bounds_contains(\n    outer_bounds: tuple[float, float, float, float] | dict[str, float],\n    inner_bounds: tuple[float, float, float, float] | dict[str, float],\n    buffer: float = 0.0,\n) -&gt; bool:\n    \"\"\"Check if outer bounds completely contain inner bounds.\n\n    Parameters\n    ----------\n    outer_bounds : tuple or dict\n        Outer bounds as (west, south, east, north) or dict with those keys.\n    inner_bounds : tuple or dict\n        Inner bounds as (west, south, east, north) or dict with those keys.\n    buffer : float, optional\n        Buffer distance to require around inner bounds (in same units).\n        Default is 0.0 (exact containment).\n\n    Returns\n    -------\n    bool\n        True if outer bounds completely contain inner bounds (with buffer).\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Check if UNR grid covers DISP frame\n    &gt;&gt;&gt; unr_bounds = (-97, 28, -93, 32)\n    &gt;&gt;&gt; disp_bounds = (-96, 29, -94, 31)\n    &gt;&gt;&gt; bounds_contains(unr_bounds, disp_bounds)\n    True\n\n    &gt;&gt;&gt; # With buffer requirement\n    &gt;&gt;&gt; bounds_contains(unr_bounds, disp_bounds, buffer=0.5)\n    True\n\n    &gt;&gt;&gt; # Dict format\n    &gt;&gt;&gt; unr_bounds = {\"west\": -97, \"south\": 28, \"east\": -93, \"north\": 32}\n    &gt;&gt;&gt; disp_bounds = {\"west\": -96, \"south\": 29, \"east\": -94, \"north\": 31}\n    &gt;&gt;&gt; bounds_contains(unr_bounds, disp_bounds)\n    True\n\n    &gt;&gt;&gt; # Does not contain\n    &gt;&gt;&gt; small_bounds = (-95, 29, -94, 30)\n    &gt;&gt;&gt; large_bounds = (-96, 28, -93, 31)\n    &gt;&gt;&gt; bounds_contains(small_bounds, large_bounds)\n    False\n\n    \"\"\"\n    # Parse outer bounds\n    if isinstance(outer_bounds, dict):\n        o_west = outer_bounds[\"west\"]\n        o_south = outer_bounds[\"south\"]\n        o_east = outer_bounds[\"east\"]\n        o_north = outer_bounds[\"north\"]\n    else:\n        o_west, o_south, o_east, o_north = outer_bounds\n\n    # Parse inner bounds\n    if isinstance(inner_bounds, dict):\n        i_west = inner_bounds[\"west\"]\n        i_south = inner_bounds[\"south\"]\n        i_east = inner_bounds[\"east\"]\n        i_north = inner_bounds[\"north\"]\n    else:\n        i_west, i_south, i_east, i_north = inner_bounds\n\n    # Check containment with buffer\n    return (\n        o_west &lt;= i_west - buffer\n        and o_south &lt;= i_south - buffer\n        and o_east &gt;= i_east + buffer\n        and o_north &gt;= i_north + buffer\n    )\n</code></pre>"},{"location":"api/#cal_disp.product.check_bounds_coverage","title":"check_bounds_coverage","text":"<pre><code>check_bounds_coverage(outer_bounds: tuple[float, float, float, float] | dict[str, float], inner_bounds: tuple[float, float, float, float] | dict[str, float], buffer: float = 0.0) -&gt; dict[str, bool | dict[str, float]]\n</code></pre> <p>Check bounds coverage with detailed gap information.</p> <p>Parameters:</p> Name Type Description Default <code>outer_bounds</code> <code>tuple or dict</code> <p>Outer bounds as (west, south, east, north) or dict.</p> required <code>inner_bounds</code> <code>tuple or dict</code> <p>Inner bounds as (west, south, east, north) or dict.</p> required <code>buffer</code> <code>float</code> <p>Buffer distance required. Default is 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with: - \"contains\": bool, True if outer contains inner (with buffer) - \"gaps\": dict, Gap distances by direction (negative = covered)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; unr_bounds = (-97, 28, -93, 32)\n&gt;&gt;&gt; disp_bounds = (-96, 29, -94, 31)\n&gt;&gt;&gt; result = check_bounds_coverage(unr_bounds, disp_bounds, buffer=0.5)\n&gt;&gt;&gt; result[\"contains\"]\nTrue\n&gt;&gt;&gt; result[\"gaps\"]\n{'west': -0.5, 'south': -0.5, 'east': -0.5, 'north': -0.5}\n</code></pre> <pre><code>&gt;&gt;&gt; # Insufficient coverage\n&gt;&gt;&gt; small_bounds = (-95.5, 29, -94, 30)\n&gt;&gt;&gt; result = check_bounds_coverage(small_bounds, disp_bounds)\n&gt;&gt;&gt; result[\"contains\"]\nFalse\n&gt;&gt;&gt; result[\"gaps\"]\n{'west': 0.5, 'south': 0.0, 'east': 0.0, 'north': 1.0}\n</code></pre> Source code in <code>src/cal_disp/product/_utils.py</code> <pre><code>def check_bounds_coverage(\n    outer_bounds: tuple[float, float, float, float] | dict[str, float],\n    inner_bounds: tuple[float, float, float, float] | dict[str, float],\n    buffer: float = 0.0,\n) -&gt; dict[str, bool | dict[str, float]]:\n    \"\"\"Check bounds coverage with detailed gap information.\n\n    Parameters\n    ----------\n    outer_bounds : tuple or dict\n        Outer bounds as (west, south, east, north) or dict.\n    inner_bounds : tuple or dict\n        Inner bounds as (west, south, east, north) or dict.\n    buffer : float, optional\n        Buffer distance required. Default is 0.0.\n\n    Returns\n    -------\n    dict\n        Dictionary with:\n        - \"contains\": bool, True if outer contains inner (with buffer)\n        - \"gaps\": dict, Gap distances by direction (negative = covered)\n\n    Examples\n    --------\n    &gt;&gt;&gt; unr_bounds = (-97, 28, -93, 32)\n    &gt;&gt;&gt; disp_bounds = (-96, 29, -94, 31)\n    &gt;&gt;&gt; result = check_bounds_coverage(unr_bounds, disp_bounds, buffer=0.5)\n    &gt;&gt;&gt; result[\"contains\"]\n    True\n    &gt;&gt;&gt; result[\"gaps\"]\n    {'west': -0.5, 'south': -0.5, 'east': -0.5, 'north': -0.5}\n\n    &gt;&gt;&gt; # Insufficient coverage\n    &gt;&gt;&gt; small_bounds = (-95.5, 29, -94, 30)\n    &gt;&gt;&gt; result = check_bounds_coverage(small_bounds, disp_bounds)\n    &gt;&gt;&gt; result[\"contains\"]\n    False\n    &gt;&gt;&gt; result[\"gaps\"]\n    {'west': 0.5, 'south': 0.0, 'east': 0.0, 'north': 1.0}\n\n    \"\"\"\n    # Parse bounds\n    if isinstance(outer_bounds, dict):\n        o_west = outer_bounds[\"west\"]\n        o_south = outer_bounds[\"south\"]\n        o_east = outer_bounds[\"east\"]\n        o_north = outer_bounds[\"north\"]\n    else:\n        o_west, o_south, o_east, o_north = outer_bounds\n\n    if isinstance(inner_bounds, dict):\n        i_west = inner_bounds[\"west\"]\n        i_south = inner_bounds[\"south\"]\n        i_east = inner_bounds[\"east\"]\n        i_north = inner_bounds[\"north\"]\n    else:\n        i_west, i_south, i_east, i_north = inner_bounds\n\n    # Calculate gaps (positive = missing coverage, negative = extra coverage)\n    gaps = {\n        \"west\": (i_west - buffer) - o_west,\n        \"south\": (i_south - buffer) - o_south,\n        \"east\": o_east - (i_east + buffer),\n        \"north\": o_north - (i_north + buffer),\n    }\n\n    # Check if all gaps are negative or zero (fully contained)\n    contains = all(gap &lt;= 0 for gap in gaps.values())\n\n    return {\n        \"contains\": contains,\n        \"gaps\": gaps,\n    }\n</code></pre>"},{"location":"api/#cal_disp.product.compute_los_correction","title":"compute_los_correction","text":"<pre><code>compute_los_correction(zenith_delay_2d: DataArray, los_up: DataArray, reference_correction: DataArray | None = None, target_crs: str | None = None, output_path: Path | str | None = None, output_format: str = 'geotiff') -&gt; xr.DataArray\n</code></pre> <p>Convert zenith delay to line-of-sight correction.</p> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>def compute_los_correction(\n    zenith_delay_2d: xr.DataArray,\n    los_up: xr.DataArray,\n    reference_correction: xr.DataArray | None = None,\n    target_crs: str | None = None,\n    output_path: Path | str | None = None,\n    output_format: str = \"geotiff\",\n) -&gt; xr.DataArray:\n    \"\"\"Convert zenith delay to line-of-sight correction.\"\"\"\n    # Ensure same grid\n    if los_up.shape != zenith_delay_2d.shape:\n        if hasattr(los_up, \"rio\") and hasattr(zenith_delay_2d, \"rio\"):\n            los_up = los_up.rio.reproject_match(zenith_delay_2d)\n        else:\n            raise ValueError(\n                f\"Shape mismatch: los_up {los_up.shape} vs \"\n                f\"zenith_delay {zenith_delay_2d.shape}\"\n            )\n\n    # Convert zenith to LOS\n    # Note: -1 matches DISP convention (positive = apparent uplift)\n    los_correction = -1 * (zenith_delay_2d / los_up)\n\n    # Subtract reference if provided\n    if reference_correction is not None:\n        if reference_correction.shape != los_correction.shape:\n            if hasattr(reference_correction, \"rio\"):\n                reference_correction = reference_correction.rio.reproject_match(\n                    los_correction\n                )\n\n        los_correction = (los_correction - reference_correction).astype(np.float32)\n\n    # Reproject to target CRS if requested\n    if target_crs is not None:\n        if hasattr(los_correction, \"rio\"):\n            los_correction = los_correction.rio.reproject(target_crs)\n        else:\n            raise ValueError(\"Cannot reproject: DataArray missing CRS information\")\n\n    # Add metadata\n    los_correction.name = \"los_correction\"\n    los_correction.attrs.update(\n        {\n            \"units\": \"meters\",\n            \"long_name\": \"Line-of-sight atmospheric correction\",\n            \"line_of_sight_convention\": (\n                \"Positive means decrease in delay (apparent uplift towards satellite)\"\n            ),\n        }\n    )\n\n    if reference_correction is not None:\n        los_correction.attrs[\"reference_subtracted\"] = \"yes\"\n\n    if target_crs is not None:\n        los_correction.attrs[\"target_crs\"] = target_crs\n\n    # Save if requested\n    if output_path is not None:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        if output_format == \"netcdf\":\n            los_correction.to_netcdf(output_path, engine=\"h5netcdf\")\n        elif output_format == \"geotiff\":\n            los_correction.rio.to_raster(\n                output_path,\n                compress=\"deflate\",\n                tiled=True,\n                dtype=\"float32\",\n            )\n        else:\n            raise ValueError(f\"Unknown format: {output_format}\")\n\n    return los_correction\n</code></pre>"},{"location":"api/#cal_disp.product.interpolate_in_time","title":"interpolate_in_time","text":"<pre><code>interpolate_in_time(tropo_early: TropoProduct, tropo_late: TropoProduct, target_datetime: datetime, bounds: tuple[float, float, float, float] | None = None, max_height: float = 11000.0, bounds_buffer: float = 0.2, output_path: Path | str | None = None) -&gt; xr.DataArray\n</code></pre> <p>Interpolate tropospheric delay between two products in time.</p> <p>Parameters:</p> Name Type Description Default <code>tropo_early</code> <code>TropoProduct</code> <p>Earlier tropospheric product.</p> required <code>tropo_late</code> <code>TropoProduct</code> <p>Later tropospheric product.</p> required <code>target_datetime</code> <code>datetime</code> <p>Target datetime for interpolation.</p> required <code>bounds</code> <code>tuple[float, float, float, float] or None</code> <p>Spatial bounds as (west, south, east, north). Default is None.</p> <code>None</code> <code>max_height</code> <code>float</code> <p>Maximum height in meters. Default is 11000 m.</p> <code>11000.0</code> <code>bounds_buffer</code> <code>float</code> <p>Buffer to add to bounds in degrees. Default is 0.2.</p> <code>0.2</code> <code>output_path</code> <code>Path, str, or None</code> <p>If provided, save result to NetCDF. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>Interpolated tropospheric delay at target datetime.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; from product import TropoProduct\n&gt;&gt;&gt; from product._tropo import interpolate_in_time\n&gt;&gt;&gt;\n&gt;&gt;&gt; early = TropoProduct.from_path(\"tropo_00Z.nc\")\n&gt;&gt;&gt; late = TropoProduct.from_path(\"tropo_06Z.nc\")\n&gt;&gt;&gt; target = datetime(2022, 1, 11, 3, 0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Basic interpolation\n&gt;&gt;&gt; delay = interpolate_in_time(early, late, target)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With spatial subsetting\n&gt;&gt;&gt; bounds = (-96, 29, -94, 31)\n&gt;&gt;&gt; delay = interpolate_in_time(\n...     early, late, target,\n...     bounds=bounds,\n...     max_height=11000,\n...     bounds_buffer=0.2,\n... )\n</code></pre> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>def interpolate_in_time(\n    tropo_early: TropoProduct,\n    tropo_late: TropoProduct,\n    target_datetime: datetime,\n    bounds: tuple[float, float, float, float] | None = None,\n    max_height: float = 11e3,\n    bounds_buffer: float = 0.2,\n    output_path: Path | str | None = None,\n) -&gt; xr.DataArray:\n    \"\"\"Interpolate tropospheric delay between two products in time.\n\n    Parameters\n    ----------\n    tropo_early : TropoProduct\n        Earlier tropospheric product.\n    tropo_late : TropoProduct\n        Later tropospheric product.\n    target_datetime : datetime\n        Target datetime for interpolation.\n    bounds : tuple[float, float, float, float] or None, optional\n        Spatial bounds as (west, south, east, north). Default is None.\n    max_height : float, optional\n        Maximum height in meters. Default is 11000 m.\n    bounds_buffer : float, optional\n        Buffer to add to bounds in degrees. Default is 0.2.\n    output_path : Path, str, or None, optional\n        If provided, save result to NetCDF. Default is None.\n\n    Returns\n    -------\n    xr.DataArray\n        Interpolated tropospheric delay at target datetime.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from datetime import datetime\n    &gt;&gt;&gt; from product import TropoProduct\n    &gt;&gt;&gt; from product._tropo import interpolate_in_time\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; early = TropoProduct.from_path(\"tropo_00Z.nc\")\n    &gt;&gt;&gt; late = TropoProduct.from_path(\"tropo_06Z.nc\")\n    &gt;&gt;&gt; target = datetime(2022, 1, 11, 3, 0)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Basic interpolation\n    &gt;&gt;&gt; delay = interpolate_in_time(early, late, target)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # With spatial subsetting\n    &gt;&gt;&gt; bounds = (-96, 29, -94, 31)\n    &gt;&gt;&gt; delay = interpolate_in_time(\n    ...     early, late, target,\n    ...     bounds=bounds,\n    ...     max_height=11000,\n    ...     bounds_buffer=0.2,\n    ... )\n\n    \"\"\"\n    if tropo_early.date &gt; tropo_late.date:\n        raise ValueError(\n            f\"Early product date ({tropo_early.date}) must be before \"\n            f\"late product date ({tropo_late.date})\"\n        )\n\n    if target_datetime &lt; tropo_early.date or target_datetime &gt; tropo_late.date:\n        raise ValueError(\n            f\"Target datetime ({target_datetime}) must be between \"\n            f\"early ({tropo_early.date}) and late ({tropo_late.date}) dates\"\n        )\n\n    # Get total delay with consistent kwargs\n    da_early = tropo_early.get_total_delay(\n        bounds=bounds,\n        max_height=max_height,\n        bounds_buffer=bounds_buffer,\n    )\n\n    da_late = tropo_late.get_total_delay(\n        bounds=bounds,\n        max_height=max_height,\n        bounds_buffer=bounds_buffer,\n    )\n\n    # Compute interpolation weight\n    delta_total = (tropo_late.date - tropo_early.date).total_seconds()\n    delta_target = (target_datetime - tropo_early.date).total_seconds()\n    weight = delta_target / delta_total\n\n    # Linear interpolation\n    da_interp = (1 - weight) * da_early + weight * da_late\n\n    # Add metadata\n    da_interp.name = \"zenith_total_delay\"\n    da_interp.attrs.update(\n        {\n            \"interpolation_method\": \"linear\",\n            \"early_product\": tropo_early.filename,\n            \"late_product\": tropo_late.filename,\n            \"early_date\": tropo_early.date.isoformat(),\n            \"late_date\": tropo_late.date.isoformat(),\n            \"target_date\": target_datetime.isoformat(),\n            \"interpolation_weight\": float(weight),\n            \"long_name\": \"Total zenith tropospheric delay\",\n            \"units\": \"meters\",\n        }\n    )\n\n    # Save if requested\n    if output_path is not None:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        da_interp.to_netcdf(output_path, engine=\"h5netcdf\")\n\n    return da_interp\n</code></pre>"},{"location":"api/#cal_disp.product.interpolate_to_dem_surface","title":"interpolate_to_dem_surface","text":"<pre><code>interpolate_to_dem_surface(da_tropo_cube: DataArray, dem: DataArray, method: str = 'linear', output_path: Path | str | None = None, output_format: str = 'netcdf') -&gt; xr.DataArray\n</code></pre> <p>Interpolate 3D tropospheric delay to DEM surface heights.</p> <p>Parameters:</p> Name Type Description Default <code>da_tropo_cube</code> <code>DataArray</code> <p>3D tropospheric delay with dims (height, y, x). Assumed to be in EPSG:4326 (WGS84) if CRS not specified.</p> required <code>dem</code> <code>DataArray</code> <p>DEM with surface heights in meters. Must have CRS information.</p> required <code>method</code> <code>str</code> <p>Interpolation method (\"linear\" or \"nearest\"). Default is \"linear\".</p> <code>'linear'</code> <code>output_path</code> <code>Path, str, or None</code> <p>If provided, save result. Default is None.</p> <code>None</code> <code>output_format</code> <code>str</code> <p>Output format (\"netcdf\" or \"geotiff\"). Default is \"netcdf\".</p> <code>'netcdf'</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>2D tropospheric delay at DEM surface.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If DEM is missing CRS information.</p> Source code in <code>src/cal_disp/product/_tropo.py</code> <pre><code>def interpolate_to_dem_surface(\n    da_tropo_cube: xr.DataArray,\n    dem: xr.DataArray,\n    method: str = \"linear\",\n    output_path: Path | str | None = None,\n    output_format: str = \"netcdf\",\n) -&gt; xr.DataArray:\n    \"\"\"Interpolate 3D tropospheric delay to DEM surface heights.\n\n    Parameters\n    ----------\n    da_tropo_cube : xr.DataArray\n        3D tropospheric delay with dims (height, y, x).\n        Assumed to be in EPSG:4326 (WGS84) if CRS not specified.\n    dem : xr.DataArray\n        DEM with surface heights in meters. Must have CRS information.\n    method : str, optional\n        Interpolation method (\"linear\" or \"nearest\"). Default is \"linear\".\n    output_path : Path, str, or None, optional\n        If provided, save result. Default is None.\n    output_format : str, optional\n        Output format (\"netcdf\" or \"geotiff\"). Default is \"netcdf\".\n\n    Returns\n    -------\n    xr.DataArray\n        2D tropospheric delay at DEM surface.\n\n    Raises\n    ------\n    ValueError\n        If DEM is missing CRS information.\n\n    \"\"\"\n    # Ensure consistent coordinate naming\n    if \"latitude\" in da_tropo_cube.dims:\n        da_tropo_cube = da_tropo_cube.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n\n    # Check DEM has CRS\n    if not hasattr(dem, \"rio\") or dem.rio.crs is None:\n        raise ValueError(\n            \"DEM is missing CRS information. \"\n            \"Use dem.rio.write_crs() to set the CRS before calling this function.\"\n        )\n\n    dem_crs = dem.rio.crs\n\n    # Write CRS to tropo if missing (assume EPSG:4326)\n    if not hasattr(da_tropo_cube, \"rio\") or da_tropo_cube.rio.crs is None:\n        da_tropo_cube = da_tropo_cube.rio.write_crs(\"EPSG:4326\")\n\n    # Reproject if different CRS\n    if da_tropo_cube.rio.crs != dem_crs:\n        td_utm = da_tropo_cube.rio.reproject(\n            dem_crs,\n            resampling=Resampling.cubic,\n        )\n    else:\n        td_utm = da_tropo_cube\n\n    # Find height dimension\n    if \"height\" not in td_utm.dims:\n        raise ValueError(\n            f\"No height dimension found. Available dims: {list(td_utm.dims)}\"\n        )\n\n    # Build interpolator\n    rgi = RegularGridInterpolator(\n        (td_utm[\"height\"].values, td_utm.y.values, td_utm.x.values),\n        np.nan_to_num(td_utm.values),\n        method=method,\n        bounds_error=False,\n        fill_value=np.nan,\n    )\n\n    # Create coordinate meshgrid\n    yy, xx = np.meshgrid(dem.y.values, dem.x.values, indexing=\"ij\")\n    pts = np.column_stack([dem.values.ravel(), yy.ravel(), xx.ravel()])\n\n    # Interpolate\n    vals = rgi(pts)\n\n    # Create output DataArray\n    out = dem.copy()\n    out.values[:] = vals.reshape(dem.shape).astype(np.float32)\n    out.name = da_tropo_cube.name or \"tropospheric_delay\"\n\n    # Update attributes\n    out.attrs.update(\n        {\n            \"interpolation_method\": method,\n            \"interpolated_from\": \"3D tropospheric model\",\n            \"units\": \"meters\",\n            \"long_name\": \"Tropospheric delay at DEM surface\",\n        }\n    )\n\n    # Add time if present in input\n    if \"time\" in td_utm.coords:\n        out.attrs[\"time\"] = str(td_utm.time.values)\n\n    # Preserve any existing tropo metadata\n    if \"target_date\" in da_tropo_cube.attrs:\n        out.attrs[\"target_date\"] = da_tropo_cube.attrs[\"target_date\"]\n    if \"interpolation_weight\" in da_tropo_cube.attrs:\n        out.attrs[\"interpolation_weight\"] = da_tropo_cube.attrs[\"interpolation_weight\"]\n\n    # Save if requested\n    if output_path is not None:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        if output_format == \"netcdf\":\n            out.to_netcdf(output_path, engine=\"h5netcdf\")\n        elif output_format == \"geotiff\":\n            out.rio.to_raster(\n                output_path,\n                compress=\"deflate\",\n                tiled=True,\n                dtype=\"float32\",\n            )\n        else:\n            raise ValueError(f\"Unknown format: {output_format}\")\n\n    return out\n</code></pre>"},{"location":"api/#config","title":"Config","text":""},{"location":"api/#cal_disp.config","title":"cal_disp.config","text":""},{"location":"api/#cal_disp.config.DynamicAncillaryFileGroup","title":"DynamicAncillaryFileGroup","text":"<p>               Bases: <code>YamlModel</code></p> <p>Dynamic ancillary files for the SAS.</p> <p>Attributes:</p> Name Type Description <code>algorithm_parameters_file</code> <code>Path</code> <p>Path to file containing SAS algorithm parameters.</p> <code>los_file</code> <code>Path</code> <p>Path to the DISP static LOS layer file (line-of-sight unit vectors). Alias: static_los_file</p> <code>dem_file</code> <code>Path</code> <p>Path to the DISP static DEM layer file (digital elevation model). Alias: static_dem_file</p> <code>mask_file</code> <code>(Path or None, optional)</code> <p>Optional byte mask file to ignore low correlation/bad data (e.g., water mask). Convention: 0 = invalid/no data, 1 = good data. Dtype must be uint8. Default is None.</p> <code>reference_tropo_files</code> <code>(list[Path] or None, optional)</code> <p>Paths to TROPO files for the reference (primary) date. If not provided, tropospheric correction for reference is skipped. Alias: ref_tropo_files. Default is None.</p> <code>secondary_tropo_files</code> <code>(list[Path] or None, optional)</code> <p>Paths to TROPO files for the secondary date. If not provided, tropospheric correction for secondary is skipped. Alias: sec_tropo_files. Default is None.</p> <code>iono_files</code> <code>(list[Path] or None, optional)</code> <p>Paths to ionospheric correction files. If not provided, ionospheric correction is skipped. Default is None.</p> <code>tiles_files</code> <code>(list[Path] or None, optional)</code> <p>Paths to calibration tile bounds files (e.g., S1 burst bounds) covering the full frame. If not provided, per-tile calibration is skipped. Default is None.</p> Source code in <code>src/cal_disp/config/_base.py</code> <pre><code>class DynamicAncillaryFileGroup(YamlModel):\n    \"\"\"Dynamic ancillary files for the SAS.\n\n    Attributes\n    ----------\n    algorithm_parameters_file : Path\n        Path to file containing SAS algorithm parameters.\n    los_file : Path\n        Path to the DISP static LOS layer file (line-of-sight unit vectors).\n        Alias: static_los_file\n    dem_file : Path\n        Path to the DISP static DEM layer file (digital elevation model).\n        Alias: static_dem_file\n    mask_file : Path or None, optional\n        Optional byte mask file to ignore low correlation/bad data (e.g., water mask).\n        Convention: 0 = invalid/no data, 1 = good data. Dtype must be uint8.\n        Default is None.\n    reference_tropo_files : list[Path] or None, optional\n        Paths to TROPO files for the reference (primary) date.\n        If not provided, tropospheric correction for reference is skipped.\n        Alias: ref_tropo_files. Default is None.\n    secondary_tropo_files : list[Path] or None, optional\n        Paths to TROPO files for the secondary date.\n        If not provided, tropospheric correction for secondary is skipped.\n        Alias: sec_tropo_files. Default is None.\n    iono_files : list[Path] or None, optional\n        Paths to ionospheric correction files.\n        If not provided, ionospheric correction is skipped.\n        Default is None.\n    tiles_files : list[Path] or None, optional\n        Paths to calibration tile bounds files (e.g., S1 burst bounds) covering\n        the full frame. If not provided, per-tile calibration is skipped.\n        Default is None.\n\n    \"\"\"\n\n    algorithm_parameters_file: RequiredPath = Field(\n        ...,\n        description=\"Path to file containing SAS algorithm parameters.\",\n    )\n\n    los_file: RequiredPath = Field(\n        ...,\n        alias=\"static_los_file\",\n        description=(\n            \"Path to the DISP static los layer file (1 per frame) with line-of-sight\"\n            \" unit vectors.\"\n        ),\n    )\n\n    dem_file: RequiredPath = Field(\n        ...,\n        alias=\"static_dem_file\",\n        description=(\n            \"Path to the DISP static dem layer file (1 per frame) with line-of-sight\"\n            \" unit vectors.\"\n        ),\n    )\n    # NOTE should I add also shadow_layover static file as input\n\n    mask_file: OptionalPath = Field(\n        default=None,\n        description=(\n            \"Optional Byte mask file used to ignore low correlation/bad data (e.g water\"\n            \" mask). Convention is 0 for no data/invalid, and 1 for good data. Dtype\"\n            \" must be uint8.\"\n        ),\n    )\n\n    reference_tropo_files: Optional[List[Path]] = Field(\n        default=None,\n        alias=\"ref_tropo_files\",\n        description=(\n            \"Path to the TROPO file for the reference date.\"\n            \" If not provided, tropospheric correction for reference is skipped.\"\n        ),\n    )\n\n    secondary_tropo_files: Optional[List[Path]] = Field(\n        default=None,\n        alias=\"sec_tropo_files\",\n        description=(\n            \"Path to the TROPO file for the secondary date.\"\n            \" If not provided, tropospheric correction for secondary is skipped.\"\n        ),\n    )\n\n    iono_files: Optional[List[Path]] = Field(\n        default=None,\n        alias=\"iono_files\",\n        description=(\n            \"Path to the IONO files\"\n            \" If not provided, ionosphere correction for reference is skipped.\"\n        ),\n    )\n\n    tiles_files: Optional[List[Path]] = Field(\n        default=None,\n        description=(\n            \"Paths to the calibration tile bounds files (e.g. S1 burst bounds) covering\"\n            \" full frame. If none provided, calibration per tile is skipped.\"\n        ),\n    )\n\n    @field_validator(\n        \"reference_tropo_files\",\n        \"secondary_tropo_files\",\n        \"iono_files\",\n        \"tiles_files\",\n        mode=\"before\",\n    )\n    @classmethod\n    def _validate_file_lists(cls, v):\n        \"\"\"Validate and process file lists or glob patterns.\"\"\"\n        return _read_file_list_or_glob(cls, v)\n\n    def get_all_files(self) -&gt; Dict[str, Path | list[Path]]:\n        return self.get_all_file_paths(flatten_lists=True)\n\n    model_config = STRICT_CONFIG_WITH_ALIASES\n</code></pre>"},{"location":"api/#cal_disp.config.InputFileGroup","title":"InputFileGroup","text":"<p>               Bases: <code>YamlModel</code></p> <p>Input file group for the SAS.</p> <p>Attributes:</p> Name Type Description <code>disp_file</code> <code>Path</code> <p>Path to DISP file.</p> <code>calibration_reference_grid</code> <code>Path</code> <p>Path to UNR calibration reference file (parquet format).</p> Source code in <code>src/cal_disp/config/_base.py</code> <pre><code>class InputFileGroup(YamlModel):\n    \"\"\"Input file group for the SAS.\n\n    Attributes\n    ----------\n    disp_file : Path\n        Path to DISP file.\n    calibration_reference_grid : Path\n        Path to UNR calibration reference file (parquet format).\n\n    \"\"\"\n\n    disp_file: RequiredPath = Field(\n        ...,\n        description=\"Path to DISP file.\",\n    )\n\n    calibration_reference_grid: RequiredPath = Field(\n        ...,\n        description=\"Path to UNR calibration reference file [parquet].\",\n    )\n\n    frame_id: int = Field(\n        ...,\n        description=\"Frame ID of the DISP frame.\",\n    )\n\n    model_config = ConfigDict(\n        extra=\"forbid\",\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.StaticAncillaryFileGroup","title":"StaticAncillaryFileGroup","text":"<p>               Bases: <code>YamlModel</code></p> <p>Static ancillary files for the SAS.</p> <p>These files contain configuration and reference data that don't change between processing runs for a given frame.</p> <p>Attributes:</p> Name Type Description <code>algorithm_parameters_overrides_json</code> <code>Optional[Path]</code> <p>JSON file with frame-specific algorithm parameter overrides.</p> <code>deformation_area_database_json</code> <code>Optional[Path]</code> <p>GeoJSON file with deforming areas to exclude from calibration.</p> <code>event_database_json</code> <code>Optional[Path]</code> <p>GeoJSON file with earthquake/volcanic activity events for each frame.</p> Source code in <code>src/cal_disp/config/_base.py</code> <pre><code>class StaticAncillaryFileGroup(YamlModel):\n    \"\"\"Static ancillary files for the SAS.\n\n    These files contain configuration and reference data that don't change\n    between processing runs for a given frame.\n\n    Attributes\n    ----------\n    algorithm_parameters_overrides_json : Optional[Path]\n        JSON file with frame-specific algorithm parameter overrides.\n    deformation_area_database_json : Optional[Path]\n        GeoJSON file with deforming areas to exclude from calibration.\n    event_database_json : Optional[Path]\n        GeoJSON file with earthquake/volcanic activity events for each frame.\n\n    \"\"\"\n\n    algorithm_parameters_overrides_json: OptionalPath = Field(\n        default=None,\n        description=(\n            \"JSON file containing frame-specific algorithm parameters to override the\"\n            \" defaults passed in the `algorithm_parameters.yaml`.\"\n        ),\n    )\n\n    deformation_area_database_json: OptionalPath = Field(\n        default=None,\n        alias=\"defo_area_db_json\",\n        description=(\n            \"GeoJSON file containing list of deforming areas to exclude from\"\n            \" calibration (e.g. Central Valley subsidence).\"\n        ),\n    )\n\n    event_database_json: OptionalPath = Field(\n        default=None,\n        alias=\"event_db_json\",\n        description=(\n            \"GeoJSON file containing list of events (earthquakes, volcanic activity)\"\n            \" for each frame.\"\n        ),\n    )\n\n    def has_algorithm_overrides(self) -&gt; bool:\n        \"\"\"Check if algorithm parameter overrides are provided.\"\"\"\n        return self.algorithm_parameters_overrides_json is not None\n\n    def has_deformation_database(self) -&gt; bool:\n        \"\"\"Check if deformation area database is provided.\"\"\"\n        return self.deformation_area_database_json is not None\n\n    def has_event_database(self) -&gt; bool:\n        \"\"\"Check if event database is provided.\"\"\"\n        return self.event_database_json is not None\n\n    def get_all_files(self) -&gt; Dict[str, Path | list[Path]]:\n        return self.get_all_file_paths(flatten_lists=True)\n\n    model_config = STRICT_CONFIG_WITH_ALIASES\n</code></pre>"},{"location":"api/#cal_disp.config.StaticAncillaryFileGroup.has_algorithm_overrides","title":"has_algorithm_overrides","text":"<pre><code>has_algorithm_overrides() -&gt; bool\n</code></pre> <p>Check if algorithm parameter overrides are provided.</p> Source code in <code>src/cal_disp/config/_base.py</code> <pre><code>def has_algorithm_overrides(self) -&gt; bool:\n    \"\"\"Check if algorithm parameter overrides are provided.\"\"\"\n    return self.algorithm_parameters_overrides_json is not None\n</code></pre>"},{"location":"api/#cal_disp.config.StaticAncillaryFileGroup.has_deformation_database","title":"has_deformation_database","text":"<pre><code>has_deformation_database() -&gt; bool\n</code></pre> <p>Check if deformation area database is provided.</p> Source code in <code>src/cal_disp/config/_base.py</code> <pre><code>def has_deformation_database(self) -&gt; bool:\n    \"\"\"Check if deformation area database is provided.\"\"\"\n    return self.deformation_area_database_json is not None\n</code></pre>"},{"location":"api/#cal_disp.config.StaticAncillaryFileGroup.has_event_database","title":"has_event_database","text":"<pre><code>has_event_database() -&gt; bool\n</code></pre> <p>Check if event database is provided.</p> Source code in <code>src/cal_disp/config/_base.py</code> <pre><code>def has_event_database(self) -&gt; bool:\n    \"\"\"Check if event database is provided.\"\"\"\n    return self.event_database_json is not None\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings","title":"WorkerSettings","text":"<p>               Bases: <code>YamlModel</code></p> <p>Settings for controlling CPU/threading and parallelism.</p> <p>This class configures Dask distributed computing settings including worker count, threads per worker, and data block sizes for chunked processing.</p> <p>Attributes:</p> Name Type Description <code>n_workers</code> <code>int</code> <p>Number of Dask workers to spawn. Default is 4.</p> <code>threads_per_worker</code> <code>int</code> <p>Number of threads per worker. Default is 2.</p> <code>block_shape</code> <code>Tuple[int, int]</code> <p>Block size (rows, columns) for chunked data loading.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Default settings\n&gt;&gt;&gt; settings = WorkerSettings()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Custom configuration\n&gt;&gt;&gt; settings = WorkerSettings(\n...     n_workers=8,\n...     threads_per_worker=4,\n...     block_shape=(256, 256)\n... )\n&gt;&gt;&gt; print(f\"Total threads: {settings.total_threads}\")\n</code></pre> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>class WorkerSettings(YamlModel):\n    \"\"\"Settings for controlling CPU/threading and parallelism.\n\n    This class configures Dask distributed computing settings including\n    worker count, threads per worker, and data block sizes for chunked\n    processing.\n\n    Attributes\n    ----------\n    n_workers : int\n        Number of Dask workers to spawn. Default is 4.\n    threads_per_worker : int\n        Number of threads per worker. Default is 2.\n    block_shape : Tuple[int, int]\n        Block size (rows, columns) for chunked data loading.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Default settings\n    &gt;&gt;&gt; settings = WorkerSettings()\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Custom configuration\n    &gt;&gt;&gt; settings = WorkerSettings(\n    ...     n_workers=8,\n    ...     threads_per_worker=4,\n    ...     block_shape=(256, 256)\n    ... )\n    &gt;&gt;&gt; print(f\"Total threads: {settings.total_threads}\")\n\n    \"\"\"\n\n    n_workers: int = Field(\n        default=4,\n        ge=1,\n        le=256,  # Reasonable upper limit\n        description=(\n            \"Number of workers to use in dask.Client. Typically set to the number \"\n            \"of physical CPU cores or less. Default: 4\"\n        ),\n    )\n\n    threads_per_worker: int = Field(\n        default=2,\n        ge=1,\n        le=32,  # Reasonable upper limit\n        description=(\n            \"Number of threads per worker in dask.Client. Controls parallelism \"\n            \"within each worker. Default: 2\"\n        ),\n    )\n\n    block_shape: Tuple[int, int] = Field(\n        default=(128, 128),\n        description=(\n            \"Size (rows, columns) of blocks of data to load at a time. \"\n            \"Larger blocks use more memory but may be more efficient. \"\n            \"Must be positive integers. Default: (128, 128)\"\n        ),\n    )\n\n    @field_validator(\"block_shape\", mode=\"before\")\n    @classmethod\n    def _validate_block_shape(cls, v) -&gt; Tuple[int, int]:\n        \"\"\"Validate block shape has positive dimensions.\n\n        Parameters\n        ----------\n        v : tuple | list\n            Block shape to validate.\n\n        Returns\n        -------\n        Tuple[int, int]\n            Validated block shape.\n\n        Raises\n        ------\n        ValueError\n            If block dimensions are not positive or not exactly 2 values.\n\n        \"\"\"\n        # Convert to tuple if list\n        if isinstance(v, list):\n            v = tuple(v)\n\n        # Check it's a tuple/list with 2 elements\n        if not isinstance(v, tuple) or len(v) != 2:\n            raise ValueError(\n                f\"block_shape must have exactly 2 dimensions (rows, cols), got {v}\"\n            )\n\n        # Check both are integers\n        if not all(isinstance(x, int) for x in v):\n            raise ValueError(f\"block_shape dimensions must be integers, got {v}\")\n\n        # Check both are positive\n        if not all(x &gt; 0 for x in v):\n            raise ValueError(f\"block_shape dimensions must be positive, got {v}\")\n\n        return v\n\n    # CHANGED: Use @property instead of @computed_field to avoid mypy issues\n    @property\n    def total_threads(self) -&gt; int:\n        \"\"\"Total number of threads across all workers.\n\n        Returns\n        -------\n        int\n            n_workers * threads_per_worker\n\n        \"\"\"\n        return self.n_workers * self.threads_per_worker\n\n    @property\n    def block_size(self) -&gt; int:\n        \"\"\"Total number of elements per block.\n\n        Returns\n        -------\n        int\n            Product of block_shape dimensions (rows * columns).\n\n        \"\"\"\n        return self.block_shape[0] * self.block_shape[1]\n\n    def estimate_memory_per_block(self, dtype_size: int = 8, n_bands: int = 1) -&gt; float:\n        \"\"\"Estimate memory usage per block in MB.\n\n        Parameters\n        ----------\n        dtype_size : int, default=8\n            Size of data type in bytes (e.g., 8 for float64, 4 for float32).\n        n_bands : int, default=1\n            Number of bands/layers in the data.\n\n        Returns\n        -------\n        float\n            Estimated memory in megabytes.\n\n        Examples\n        --------\n        &gt;&gt;&gt; settings = WorkerSettings(block_shape=(256, 256))\n        &gt;&gt;&gt; mem_mb = settings.estimate_memory_per_block(dtype_size=8, n_bands=2)\n        &gt;&gt;&gt; print(f\"Estimated memory: {mem_mb:.2f} MB\")\n\n        \"\"\"\n        bytes_per_block = self.block_size * dtype_size * n_bands\n        return bytes_per_block / (1024 * 1024)  # Convert to MB\n\n    def estimate_total_memory(\n        self, dtype_size: int = 8, n_bands: int = 1, overhead_factor: float = 1.5\n    ) -&gt; float:\n        \"\"\"Estimate total memory usage across all workers in GB.\n\n        Parameters\n        ----------\n        dtype_size : int, default=8\n            Size of data type in bytes.\n        n_bands : int, default=1\n            Number of bands/layers in the data.\n        overhead_factor : float, default=1.5\n            Multiplier for overhead (copies, intermediate results).\n\n        Returns\n        -------\n        float\n            Estimated total memory in gigabytes.\n\n        \"\"\"\n        mb_per_block = self.estimate_memory_per_block(dtype_size, n_bands)\n        # Assume each worker might hold multiple blocks\n        total_mb = mb_per_block * self.n_workers * overhead_factor\n        return total_mb / 1024  # Convert to GB\n\n    def summary(self) -&gt; str:\n        \"\"\"Generate a human-readable summary of settings.\n\n        Returns\n        -------\n        str\n            Multi-line summary string.\n\n        \"\"\"\n        lines = [\n            \"WorkerSettings Configuration:\",\n            \"=\" * 50,\n            f\"Workers:              {self.n_workers}\",\n            f\"Threads per worker:   {self.threads_per_worker}\",\n            f\"Total threads:        {self.total_threads}\",\n            f\"Block shape:          {self.block_shape[0]} x {self.block_shape[1]}\",\n            f\"Block size:           {self.block_size:,} elements\",\n            f\"Estimated mem/block:  {self.estimate_memory_per_block():.2f} MB\",\n            f\"Estimated total mem:  {self.estimate_total_memory():.2f} GB\",\n            \"=\" * 50,\n        ]\n        return \"\\n\".join(lines)\n\n    @classmethod\n    def create_lightweight(cls) -&gt; \"WorkerSettings\":\n        \"\"\"Create lightweight settings for small datasets or testing.\n\n        Returns\n        -------\n        WorkerSettings\n            Configuration with 2 workers, 1 thread each, small blocks.\n\n        \"\"\"\n        return cls(n_workers=2, threads_per_worker=1, block_shape=(64, 64))\n\n    @classmethod\n    def create_standard(cls) -&gt; \"WorkerSettings\":\n        \"\"\"Create standard settings for typical workloads.\n\n        Returns\n        -------\n        WorkerSettings\n            Configuration with 4 workers, 2 threads each, medium blocks.\n\n        \"\"\"\n        return cls(n_workers=4, threads_per_worker=2, block_shape=(128, 128))\n\n    @classmethod\n    def create_heavy(cls) -&gt; \"WorkerSettings\":\n        \"\"\"Create heavy-duty settings for large datasets.\n\n        Returns\n        -------\n        WorkerSettings\n            Configuration with 8 workers, 4 threads each, large blocks.\n\n        \"\"\"\n        return cls(n_workers=8, threads_per_worker=4, block_shape=(256, 256))\n\n    @classmethod\n    def create_from_cpu_count(\n        cls,\n        use_fraction: float = 0.75,\n        threads_per_worker: int = 2,\n        block_shape: Tuple[int, int] = (128, 128),\n    ) -&gt; \"WorkerSettings\":\n        \"\"\"Create settings based on available CPU count.\n\n        Parameters\n        ----------\n        use_fraction : float, default=0.75\n            Fraction of available CPUs to use (0.0 to 1.0).\n        threads_per_worker : int, default=2\n            Threads per worker.\n        block_shape : Tuple[int, int], default=(128, 128)\n            Block shape for data loading.\n\n        Returns\n        -------\n        WorkerSettings\n            Configuration tuned to system CPU count.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Use 75% of available CPUs\n        &gt;&gt;&gt; settings = WorkerSettings.create_from_cpu_count(use_fraction=0.75)\n\n        \"\"\"\n        cpu_count = multiprocessing.cpu_count()\n        n_workers = max(1, int(cpu_count * use_fraction / threads_per_worker))\n\n        return cls(\n            n_workers=n_workers,\n            threads_per_worker=threads_per_worker,\n            block_shape=block_shape,\n        )\n\n    def validate_against_system(self) -&gt; Dict[str, Any]:\n        \"\"\"Validate settings against system resources.\n\n        Returns\n        -------\n        dict\n            Dictionary with validation results and warnings.\n\n        Examples\n        --------\n        &gt;&gt;&gt; settings = WorkerSettings(n_workers=16, threads_per_worker=8)\n        &gt;&gt;&gt; validation = settings.validate_against_system()\n        &gt;&gt;&gt; if validation['warnings']:\n        ...     for warning in validation['warnings']:\n        ...         print(f\"Warning: {warning}\")\n\n        \"\"\"\n        cpu_count = multiprocessing.cpu_count()\n        warnings = []\n\n        # Check if total threads exceed CPU count\n        if self.total_threads &gt; cpu_count:\n            warnings.append(\n                f\"Total threads ({self.total_threads}) exceeds available \"\n                f\"CPU cores ({cpu_count}). May cause oversubscription.\"\n            )\n\n        # Check if block size is very large\n        if self.block_size &gt; 1_000_000:\n            warnings.append(\n                f\"Block size ({self.block_size:,}) is very large. \"\n                \"May cause high memory usage.\"\n            )\n\n        # Check if block size is very small\n        if self.block_size &lt; 1_000:\n            warnings.append(\n                f\"Block size ({self.block_size:,}) is very small. \"\n                \"May cause poor performance due to overhead.\"\n            )\n\n        return {\n            \"valid\": len(warnings) == 0,\n            \"warnings\": warnings,\n            \"system_cpu_count\": cpu_count,\n            \"configured_total_threads\": self.total_threads,\n            \"cpu_utilization\": self.total_threads / cpu_count if cpu_count &gt; 0 else 0,\n        }\n\n    model_config = ConfigDict(\n        extra=\"forbid\",\n        validate_assignment=True,\n        json_schema_extra={\n            \"examples\": [\n                {\"n_workers\": 4, \"threads_per_worker\": 2, \"block_shape\": [128, 128]},\n                {\"n_workers\": 8, \"threads_per_worker\": 4, \"block_shape\": [256, 256]},\n            ]\n        },\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.block_size","title":"block_size  <code>property</code>","text":"<pre><code>block_size: int\n</code></pre> <p>Total number of elements per block.</p> <p>Returns:</p> Type Description <code>int</code> <p>Product of block_shape dimensions (rows * columns).</p>"},{"location":"api/#cal_disp.config.WorkerSettings.total_threads","title":"total_threads  <code>property</code>","text":"<pre><code>total_threads: int\n</code></pre> <p>Total number of threads across all workers.</p> <p>Returns:</p> Type Description <code>int</code> <p>n_workers * threads_per_worker</p>"},{"location":"api/#cal_disp.config.WorkerSettings.create_from_cpu_count","title":"create_from_cpu_count  <code>classmethod</code>","text":"<pre><code>create_from_cpu_count(use_fraction: float = 0.75, threads_per_worker: int = 2, block_shape: Tuple[int, int] = (128, 128)) -&gt; WorkerSettings\n</code></pre> <p>Create settings based on available CPU count.</p> <p>Parameters:</p> Name Type Description Default <code>use_fraction</code> <code>float</code> <p>Fraction of available CPUs to use (0.0 to 1.0).</p> <code>0.75</code> <code>threads_per_worker</code> <code>int</code> <p>Threads per worker.</p> <code>2</code> <code>block_shape</code> <code>Tuple[int, int]</code> <p>Block shape for data loading.</p> <code>(128, 128)</code> <p>Returns:</p> Type Description <code>WorkerSettings</code> <p>Configuration tuned to system CPU count.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Use 75% of available CPUs\n&gt;&gt;&gt; settings = WorkerSettings.create_from_cpu_count(use_fraction=0.75)\n</code></pre> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>@classmethod\ndef create_from_cpu_count(\n    cls,\n    use_fraction: float = 0.75,\n    threads_per_worker: int = 2,\n    block_shape: Tuple[int, int] = (128, 128),\n) -&gt; \"WorkerSettings\":\n    \"\"\"Create settings based on available CPU count.\n\n    Parameters\n    ----------\n    use_fraction : float, default=0.75\n        Fraction of available CPUs to use (0.0 to 1.0).\n    threads_per_worker : int, default=2\n        Threads per worker.\n    block_shape : Tuple[int, int], default=(128, 128)\n        Block shape for data loading.\n\n    Returns\n    -------\n    WorkerSettings\n        Configuration tuned to system CPU count.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Use 75% of available CPUs\n    &gt;&gt;&gt; settings = WorkerSettings.create_from_cpu_count(use_fraction=0.75)\n\n    \"\"\"\n    cpu_count = multiprocessing.cpu_count()\n    n_workers = max(1, int(cpu_count * use_fraction / threads_per_worker))\n\n    return cls(\n        n_workers=n_workers,\n        threads_per_worker=threads_per_worker,\n        block_shape=block_shape,\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.create_heavy","title":"create_heavy  <code>classmethod</code>","text":"<pre><code>create_heavy() -&gt; WorkerSettings\n</code></pre> <p>Create heavy-duty settings for large datasets.</p> <p>Returns:</p> Type Description <code>WorkerSettings</code> <p>Configuration with 8 workers, 4 threads each, large blocks.</p> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>@classmethod\ndef create_heavy(cls) -&gt; \"WorkerSettings\":\n    \"\"\"Create heavy-duty settings for large datasets.\n\n    Returns\n    -------\n    WorkerSettings\n        Configuration with 8 workers, 4 threads each, large blocks.\n\n    \"\"\"\n    return cls(n_workers=8, threads_per_worker=4, block_shape=(256, 256))\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.create_lightweight","title":"create_lightweight  <code>classmethod</code>","text":"<pre><code>create_lightweight() -&gt; WorkerSettings\n</code></pre> <p>Create lightweight settings for small datasets or testing.</p> <p>Returns:</p> Type Description <code>WorkerSettings</code> <p>Configuration with 2 workers, 1 thread each, small blocks.</p> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>@classmethod\ndef create_lightweight(cls) -&gt; \"WorkerSettings\":\n    \"\"\"Create lightweight settings for small datasets or testing.\n\n    Returns\n    -------\n    WorkerSettings\n        Configuration with 2 workers, 1 thread each, small blocks.\n\n    \"\"\"\n    return cls(n_workers=2, threads_per_worker=1, block_shape=(64, 64))\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.create_standard","title":"create_standard  <code>classmethod</code>","text":"<pre><code>create_standard() -&gt; WorkerSettings\n</code></pre> <p>Create standard settings for typical workloads.</p> <p>Returns:</p> Type Description <code>WorkerSettings</code> <p>Configuration with 4 workers, 2 threads each, medium blocks.</p> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>@classmethod\ndef create_standard(cls) -&gt; \"WorkerSettings\":\n    \"\"\"Create standard settings for typical workloads.\n\n    Returns\n    -------\n    WorkerSettings\n        Configuration with 4 workers, 2 threads each, medium blocks.\n\n    \"\"\"\n    return cls(n_workers=4, threads_per_worker=2, block_shape=(128, 128))\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.estimate_memory_per_block","title":"estimate_memory_per_block","text":"<pre><code>estimate_memory_per_block(dtype_size: int = 8, n_bands: int = 1) -&gt; float\n</code></pre> <p>Estimate memory usage per block in MB.</p> <p>Parameters:</p> Name Type Description Default <code>dtype_size</code> <code>int</code> <p>Size of data type in bytes (e.g., 8 for float64, 4 for float32).</p> <code>8</code> <code>n_bands</code> <code>int</code> <p>Number of bands/layers in the data.</p> <code>1</code> <p>Returns:</p> Type Description <code>float</code> <p>Estimated memory in megabytes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; settings = WorkerSettings(block_shape=(256, 256))\n&gt;&gt;&gt; mem_mb = settings.estimate_memory_per_block(dtype_size=8, n_bands=2)\n&gt;&gt;&gt; print(f\"Estimated memory: {mem_mb:.2f} MB\")\n</code></pre> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>def estimate_memory_per_block(self, dtype_size: int = 8, n_bands: int = 1) -&gt; float:\n    \"\"\"Estimate memory usage per block in MB.\n\n    Parameters\n    ----------\n    dtype_size : int, default=8\n        Size of data type in bytes (e.g., 8 for float64, 4 for float32).\n    n_bands : int, default=1\n        Number of bands/layers in the data.\n\n    Returns\n    -------\n    float\n        Estimated memory in megabytes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; settings = WorkerSettings(block_shape=(256, 256))\n    &gt;&gt;&gt; mem_mb = settings.estimate_memory_per_block(dtype_size=8, n_bands=2)\n    &gt;&gt;&gt; print(f\"Estimated memory: {mem_mb:.2f} MB\")\n\n    \"\"\"\n    bytes_per_block = self.block_size * dtype_size * n_bands\n    return bytes_per_block / (1024 * 1024)  # Convert to MB\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.estimate_total_memory","title":"estimate_total_memory","text":"<pre><code>estimate_total_memory(dtype_size: int = 8, n_bands: int = 1, overhead_factor: float = 1.5) -&gt; float\n</code></pre> <p>Estimate total memory usage across all workers in GB.</p> <p>Parameters:</p> Name Type Description Default <code>dtype_size</code> <code>int</code> <p>Size of data type in bytes.</p> <code>8</code> <code>n_bands</code> <code>int</code> <p>Number of bands/layers in the data.</p> <code>1</code> <code>overhead_factor</code> <code>float</code> <p>Multiplier for overhead (copies, intermediate results).</p> <code>1.5</code> <p>Returns:</p> Type Description <code>float</code> <p>Estimated total memory in gigabytes.</p> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>def estimate_total_memory(\n    self, dtype_size: int = 8, n_bands: int = 1, overhead_factor: float = 1.5\n) -&gt; float:\n    \"\"\"Estimate total memory usage across all workers in GB.\n\n    Parameters\n    ----------\n    dtype_size : int, default=8\n        Size of data type in bytes.\n    n_bands : int, default=1\n        Number of bands/layers in the data.\n    overhead_factor : float, default=1.5\n        Multiplier for overhead (copies, intermediate results).\n\n    Returns\n    -------\n    float\n        Estimated total memory in gigabytes.\n\n    \"\"\"\n    mb_per_block = self.estimate_memory_per_block(dtype_size, n_bands)\n    # Assume each worker might hold multiple blocks\n    total_mb = mb_per_block * self.n_workers * overhead_factor\n    return total_mb / 1024  # Convert to GB\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.summary","title":"summary","text":"<pre><code>summary() -&gt; str\n</code></pre> <p>Generate a human-readable summary of settings.</p> <p>Returns:</p> Type Description <code>str</code> <p>Multi-line summary string.</p> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>def summary(self) -&gt; str:\n    \"\"\"Generate a human-readable summary of settings.\n\n    Returns\n    -------\n    str\n        Multi-line summary string.\n\n    \"\"\"\n    lines = [\n        \"WorkerSettings Configuration:\",\n        \"=\" * 50,\n        f\"Workers:              {self.n_workers}\",\n        f\"Threads per worker:   {self.threads_per_worker}\",\n        f\"Total threads:        {self.total_threads}\",\n        f\"Block shape:          {self.block_shape[0]} x {self.block_shape[1]}\",\n        f\"Block size:           {self.block_size:,} elements\",\n        f\"Estimated mem/block:  {self.estimate_memory_per_block():.2f} MB\",\n        f\"Estimated total mem:  {self.estimate_total_memory():.2f} GB\",\n        \"=\" * 50,\n    ]\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/#cal_disp.config.WorkerSettings.validate_against_system","title":"validate_against_system","text":"<pre><code>validate_against_system() -&gt; Dict[str, Any]\n</code></pre> <p>Validate settings against system resources.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with validation results and warnings.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; settings = WorkerSettings(n_workers=16, threads_per_worker=8)\n&gt;&gt;&gt; validation = settings.validate_against_system()\n&gt;&gt;&gt; if validation['warnings']:\n...     for warning in validation['warnings']:\n...         print(f\"Warning: {warning}\")\n</code></pre> Source code in <code>src/cal_disp/config/_workers.py</code> <pre><code>def validate_against_system(self) -&gt; Dict[str, Any]:\n    \"\"\"Validate settings against system resources.\n\n    Returns\n    -------\n    dict\n        Dictionary with validation results and warnings.\n\n    Examples\n    --------\n    &gt;&gt;&gt; settings = WorkerSettings(n_workers=16, threads_per_worker=8)\n    &gt;&gt;&gt; validation = settings.validate_against_system()\n    &gt;&gt;&gt; if validation['warnings']:\n    ...     for warning in validation['warnings']:\n    ...         print(f\"Warning: {warning}\")\n\n    \"\"\"\n    cpu_count = multiprocessing.cpu_count()\n    warnings = []\n\n    # Check if total threads exceed CPU count\n    if self.total_threads &gt; cpu_count:\n        warnings.append(\n            f\"Total threads ({self.total_threads}) exceeds available \"\n            f\"CPU cores ({cpu_count}). May cause oversubscription.\"\n        )\n\n    # Check if block size is very large\n    if self.block_size &gt; 1_000_000:\n        warnings.append(\n            f\"Block size ({self.block_size:,}) is very large. \"\n            \"May cause high memory usage.\"\n        )\n\n    # Check if block size is very small\n    if self.block_size &lt; 1_000:\n        warnings.append(\n            f\"Block size ({self.block_size:,}) is very small. \"\n            \"May cause poor performance due to overhead.\"\n        )\n\n    return {\n        \"valid\": len(warnings) == 0,\n        \"warnings\": warnings,\n        \"system_cpu_count\": cpu_count,\n        \"configured_total_threads\": self.total_threads,\n        \"cpu_utilization\": self.total_threads / cpu_count if cpu_count &gt; 0 else 0,\n    }\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel","title":"YamlModel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model that can be exported to yaml.</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>class YamlModel(BaseModel):\n    \"\"\"Pydantic model that can be exported to yaml.\"\"\"\n\n    model_config = STRICT_CONFIG\n\n    def to_yaml(\n        self,\n        output_path: Union[Filename, TextIO],\n        with_comments: bool = True,\n        by_alias: bool = True,\n        indent_per_level: int = 2,\n    ):\n        \"\"\"Save configuration as a yaml file.\n\n        Used to record the default-filled version of a supplied yaml.\n\n        Parameters\n        ----------\n        output_path : Pathlike\n            Path to the yaml file to save.\n        with_comments : bool, default = False.\n            Whether to add comments containing the type/descriptions to all fields.\n        by_alias : bool, default = False.\n            Whether to use the alias names for the fields.\n            Passed to pydantic's ``to_json`` method.\n            https://docs.pydantic.dev/usage/exporting_models/#modeljson\n        indent_per_level : int, default = 2\n            Number of spaces to indent per level.\n\n        \"\"\"\n        yaml_obj = self._to_yaml_obj(by_alias=by_alias)\n\n        if with_comments:\n            _add_comments(\n                yaml_obj,\n                self.model_json_schema(by_alias=by_alias),\n                indent_per_level=indent_per_level,\n            )\n\n        y = YAML()\n        # https://yaml.readthedocs.io/en/latest/detail.html#indentation-of-block-sequences\n        y.indent(\n            offset=indent_per_level,\n            mapping=indent_per_level,\n            # It is best to always have sequence &gt;= offset + 2 but this is not enforced\n            # not following this advice might lead to invalid output.\n            sequence=indent_per_level + 2,\n        )\n        if hasattr(output_path, \"write\"):\n            y.dump(yaml_obj, output_path)\n        else:\n            with open(output_path, \"w\") as f:\n                y.dump(yaml_obj, f)\n\n    @classmethod\n    def from_yaml(cls, yaml_path: Filename):\n        \"\"\"Load a configuration from a yaml file.\n\n        Parameters\n        ----------\n        yaml_path : Pathlike\n            Path to the yaml file to load.\n\n        Returns\n        -------\n        Config\n            Workflow configuration\n\n        \"\"\"\n        y = YAML(typ=\"safe\")\n        with open(yaml_path) as f:\n            data = y.load(f)\n\n        return cls(**data)\n\n    @classmethod\n    def print_yaml_schema(\n        cls,\n        output_path: Union[Filename, TextIO] = sys.stdout,\n        indent_per_level: int = 2,\n    ):\n        \"\"\"Print/save an empty configuration with defaults filled in.\n\n        Ignores the required `input_file_list` input, so a user can\n        inspect all fields.\n\n        Parameters\n        ----------\n        output_path : Pathlike\n            Path or stream to save to the yaml file to.\n            By default, prints to stdout.\n        indent_per_level : int, default = 2\n            Number of spaces to indent per level.\n\n        \"\"\"\n        cls.model_construct().to_yaml(\n            output_path, with_comments=True, indent_per_level=indent_per_level\n        )\n\n    def _to_yaml_obj(self, by_alias: bool = True) -&gt; CommentedMap:\n        # Make the YAML object to add comments to\n        # We can't just do `dumps` for some reason, need a stream\n        y = YAML()\n        ss = StringIO()\n        y.dump(json.loads(self.model_dump_json(by_alias=by_alias)), ss)\n        return y.load(ss.getvalue())\n\n    def get_all_file_paths(\n        self, include_none: bool = False, flatten_lists: bool = True\n    ) -&gt; Dict[str, Union[Path, List[Path]]]:\n        \"\"\"Get all Path fields from the model.\n\n        Parameters\n        ----------\n        include_none : bool, default=False\n            Include fields with None values.\n        flatten_lists : bool, default=True\n            Flatten list fields to individual entries with indices.\n\n        Returns\n        -------\n        dict\n            Mapping of field names to Path objects.\n\n        \"\"\"\n        files: Dict[str, Path | list[Path]] = {}\n\n        for field_name, field_info in self.model_fields.items():\n            value = getattr(self, field_name)\n\n            # Skip None values if requested\n            if value is None and not include_none:\n                continue\n\n            # Check if field is Path or Optional[Path]\n            if self._is_path_field(field_info):\n                if value is not None:\n                    files[field_name] = value\n\n            # Check if field is List[Path]\n            elif self._is_path_list_field(field_info):\n                if value:\n                    if flatten_lists:\n                        for i, path in enumerate(value):\n                            files[f\"{field_name}[{i}]\"] = path\n                    else:\n                        files[field_name] = value\n\n        return files\n\n    @staticmethod\n    def _is_path_field(field_info) -&gt; bool:\n        \"\"\"Check if field is a Path type.\"\"\"\n        from pathlib import Path\n        from typing import Union, get_args, get_origin\n\n        annotation = field_info.annotation\n\n        # Handle Annotated[Path, ...]\n        if get_origin(annotation) is not None:\n            # Check if it's Annotated\n            if hasattr(annotation, \"__metadata__\"):\n                # Get the actual type from Annotated\n                args = get_args(annotation)\n                if args:\n                    annotation = args[0]\n\n        # Direct Path type\n        if annotation is Path:\n            return True\n\n        # Optional[Path] or Union[Path, None]\n        origin = get_origin(annotation)\n        if origin is Union:\n            args = get_args(annotation)\n            return Path in args or any(arg is Path for arg in args)\n\n        return False\n\n    @staticmethod\n    def _is_path_list_field(field_info) -&gt; bool:\n        \"\"\"Check if field is a List[Path] type.\"\"\"\n        from pathlib import Path\n        from typing import Union, get_args, get_origin\n\n        annotation = field_info.annotation\n        origin = get_origin(annotation)\n\n        # Check if it's Optional[List[Path]]\n        if origin is Union:\n            args = get_args(annotation)\n            for arg in args:\n                if arg is type(None):\n                    continue\n                if get_origin(arg) in (list, List):\n                    list_args = get_args(arg)\n                    if list_args:\n                        first_arg = list_args[0]\n                        if first_arg is Path:\n                            return True\n\n        # Check if it's List[Path]\n        if origin in (list, List):\n            args = get_args(annotation)\n            if not args:\n                return False\n            first_arg = args[0]\n            is_path_type: bool = first_arg is Path\n            return is_path_type\n\n        return False\n\n    def validate_files_exist(\n        self, raise_on_missing: bool = False\n    ) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Validate all file paths exist on disk.\n\n        Parameters\n        ----------\n        raise_on_missing : bool, default=False\n            If True, raise FileNotFoundError on first missing file.\n\n        Returns\n        -------\n        dict\n            Detailed status for each file including existence, size, etc.\n\n        Raises\n        ------\n        FileNotFoundError\n            If raise_on_missing=True and any file is missing.\n\n        \"\"\"\n        results: Dict[str, Dict[str, Any]] = {}\n\n        # flatten_lists=True guarantees Dict[str, Path]\n        file_paths: Dict[str, Path] = cast(\n            Dict[str, Path], self.get_all_file_paths(flatten_lists=True)\n        )\n\n        for field_name, path in file_paths.items():\n            exists = path.exists()\n\n            if not exists and raise_on_missing:\n                raise FileNotFoundError(\n                    f\"Required file not found: {field_name} = {path}\"\n                )\n\n            results[field_name] = {\n                \"exists\": exists,\n                \"is_file\": path.is_file() if exists else None,\n                \"is_dir\": path.is_dir() if exists else None,\n                \"size_bytes\": path.stat().st_size if exists else None,\n                \"absolute_path\": str(path.absolute()),\n            }\n\n        return results\n\n    def validate_ready_to_run(self) -&gt; ValidationResult:\n        \"\"\"Check if configuration is ready to run.\"\"\"\n        return ValidationResult(ready=True, errors=[], warnings=[])\n\n    def get_missing_files(self) -&gt; List[str]:\n        \"\"\"Get list of missing file paths.\"\"\"\n        return [\n            f\"{name}: {info['absolute_path']}\"\n            for name, info in self.validate_files_exist().items()\n            if not info[\"exists\"]\n        ]\n\n    def all_files_exist(self) -&gt; bool:\n        \"\"\"Check if all files exist.\"\"\"\n        return len(self.get_missing_files()) == 0\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.all_files_exist","title":"all_files_exist","text":"<pre><code>all_files_exist() -&gt; bool\n</code></pre> <p>Check if all files exist.</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>def all_files_exist(self) -&gt; bool:\n    \"\"\"Check if all files exist.\"\"\"\n    return len(self.get_missing_files()) == 0\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(yaml_path: Filename)\n</code></pre> <p>Load a configuration from a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>Pathlike</code> <p>Path to the yaml file to load.</p> required <p>Returns:</p> Type Description <code>Config</code> <p>Workflow configuration</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>@classmethod\ndef from_yaml(cls, yaml_path: Filename):\n    \"\"\"Load a configuration from a yaml file.\n\n    Parameters\n    ----------\n    yaml_path : Pathlike\n        Path to the yaml file to load.\n\n    Returns\n    -------\n    Config\n        Workflow configuration\n\n    \"\"\"\n    y = YAML(typ=\"safe\")\n    with open(yaml_path) as f:\n        data = y.load(f)\n\n    return cls(**data)\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.get_all_file_paths","title":"get_all_file_paths","text":"<pre><code>get_all_file_paths(include_none: bool = False, flatten_lists: bool = True) -&gt; Dict[str, Union[Path, List[Path]]]\n</code></pre> <p>Get all Path fields from the model.</p> <p>Parameters:</p> Name Type Description Default <code>include_none</code> <code>bool</code> <p>Include fields with None values.</p> <code>False</code> <code>flatten_lists</code> <code>bool</code> <p>Flatten list fields to individual entries with indices.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Mapping of field names to Path objects.</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>def get_all_file_paths(\n    self, include_none: bool = False, flatten_lists: bool = True\n) -&gt; Dict[str, Union[Path, List[Path]]]:\n    \"\"\"Get all Path fields from the model.\n\n    Parameters\n    ----------\n    include_none : bool, default=False\n        Include fields with None values.\n    flatten_lists : bool, default=True\n        Flatten list fields to individual entries with indices.\n\n    Returns\n    -------\n    dict\n        Mapping of field names to Path objects.\n\n    \"\"\"\n    files: Dict[str, Path | list[Path]] = {}\n\n    for field_name, field_info in self.model_fields.items():\n        value = getattr(self, field_name)\n\n        # Skip None values if requested\n        if value is None and not include_none:\n            continue\n\n        # Check if field is Path or Optional[Path]\n        if self._is_path_field(field_info):\n            if value is not None:\n                files[field_name] = value\n\n        # Check if field is List[Path]\n        elif self._is_path_list_field(field_info):\n            if value:\n                if flatten_lists:\n                    for i, path in enumerate(value):\n                        files[f\"{field_name}[{i}]\"] = path\n                else:\n                    files[field_name] = value\n\n    return files\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.get_missing_files","title":"get_missing_files","text":"<pre><code>get_missing_files() -&gt; List[str]\n</code></pre> <p>Get list of missing file paths.</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>def get_missing_files(self) -&gt; List[str]:\n    \"\"\"Get list of missing file paths.\"\"\"\n    return [\n        f\"{name}: {info['absolute_path']}\"\n        for name, info in self.validate_files_exist().items()\n        if not info[\"exists\"]\n    ]\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.print_yaml_schema","title":"print_yaml_schema  <code>classmethod</code>","text":"<pre><code>print_yaml_schema(output_path: Union[Filename, TextIO] = sys.stdout, indent_per_level: int = 2)\n</code></pre> <p>Print/save an empty configuration with defaults filled in.</p> <p>Ignores the required <code>input_file_list</code> input, so a user can inspect all fields.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Pathlike</code> <p>Path or stream to save to the yaml file to. By default, prints to stdout.</p> <code>stdout</code> <code>indent_per_level</code> <code>int</code> <p>Number of spaces to indent per level.</p> <code>= 2</code> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>@classmethod\ndef print_yaml_schema(\n    cls,\n    output_path: Union[Filename, TextIO] = sys.stdout,\n    indent_per_level: int = 2,\n):\n    \"\"\"Print/save an empty configuration with defaults filled in.\n\n    Ignores the required `input_file_list` input, so a user can\n    inspect all fields.\n\n    Parameters\n    ----------\n    output_path : Pathlike\n        Path or stream to save to the yaml file to.\n        By default, prints to stdout.\n    indent_per_level : int, default = 2\n        Number of spaces to indent per level.\n\n    \"\"\"\n    cls.model_construct().to_yaml(\n        output_path, with_comments=True, indent_per_level=indent_per_level\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.to_yaml","title":"to_yaml","text":"<pre><code>to_yaml(output_path: Union[Filename, TextIO], with_comments: bool = True, by_alias: bool = True, indent_per_level: int = 2)\n</code></pre> <p>Save configuration as a yaml file.</p> <p>Used to record the default-filled version of a supplied yaml.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Pathlike</code> <p>Path to the yaml file to save.</p> required <code>with_comments</code> <code>bool</code> <p>Whether to add comments containing the type/descriptions to all fields.</p> <code>= False.</code> <code>by_alias</code> <code>bool</code> <p>Whether to use the alias names for the fields. Passed to pydantic's <code>to_json</code> method. https://docs.pydantic.dev/usage/exporting_models/#modeljson</p> <code>= False.</code> <code>indent_per_level</code> <code>int</code> <p>Number of spaces to indent per level.</p> <code>= 2</code> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>def to_yaml(\n    self,\n    output_path: Union[Filename, TextIO],\n    with_comments: bool = True,\n    by_alias: bool = True,\n    indent_per_level: int = 2,\n):\n    \"\"\"Save configuration as a yaml file.\n\n    Used to record the default-filled version of a supplied yaml.\n\n    Parameters\n    ----------\n    output_path : Pathlike\n        Path to the yaml file to save.\n    with_comments : bool, default = False.\n        Whether to add comments containing the type/descriptions to all fields.\n    by_alias : bool, default = False.\n        Whether to use the alias names for the fields.\n        Passed to pydantic's ``to_json`` method.\n        https://docs.pydantic.dev/usage/exporting_models/#modeljson\n    indent_per_level : int, default = 2\n        Number of spaces to indent per level.\n\n    \"\"\"\n    yaml_obj = self._to_yaml_obj(by_alias=by_alias)\n\n    if with_comments:\n        _add_comments(\n            yaml_obj,\n            self.model_json_schema(by_alias=by_alias),\n            indent_per_level=indent_per_level,\n        )\n\n    y = YAML()\n    # https://yaml.readthedocs.io/en/latest/detail.html#indentation-of-block-sequences\n    y.indent(\n        offset=indent_per_level,\n        mapping=indent_per_level,\n        # It is best to always have sequence &gt;= offset + 2 but this is not enforced\n        # not following this advice might lead to invalid output.\n        sequence=indent_per_level + 2,\n    )\n    if hasattr(output_path, \"write\"):\n        y.dump(yaml_obj, output_path)\n    else:\n        with open(output_path, \"w\") as f:\n            y.dump(yaml_obj, f)\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.validate_files_exist","title":"validate_files_exist","text":"<pre><code>validate_files_exist(raise_on_missing: bool = False) -&gt; Dict[str, Dict[str, Any]]\n</code></pre> <p>Validate all file paths exist on disk.</p> <p>Parameters:</p> Name Type Description Default <code>raise_on_missing</code> <code>bool</code> <p>If True, raise FileNotFoundError on first missing file.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Detailed status for each file including existence, size, etc.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If raise_on_missing=True and any file is missing.</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>def validate_files_exist(\n    self, raise_on_missing: bool = False\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Validate all file paths exist on disk.\n\n    Parameters\n    ----------\n    raise_on_missing : bool, default=False\n        If True, raise FileNotFoundError on first missing file.\n\n    Returns\n    -------\n    dict\n        Detailed status for each file including existence, size, etc.\n\n    Raises\n    ------\n    FileNotFoundError\n        If raise_on_missing=True and any file is missing.\n\n    \"\"\"\n    results: Dict[str, Dict[str, Any]] = {}\n\n    # flatten_lists=True guarantees Dict[str, Path]\n    file_paths: Dict[str, Path] = cast(\n        Dict[str, Path], self.get_all_file_paths(flatten_lists=True)\n    )\n\n    for field_name, path in file_paths.items():\n        exists = path.exists()\n\n        if not exists and raise_on_missing:\n            raise FileNotFoundError(\n                f\"Required file not found: {field_name} = {path}\"\n            )\n\n        results[field_name] = {\n            \"exists\": exists,\n            \"is_file\": path.is_file() if exists else None,\n            \"is_dir\": path.is_dir() if exists else None,\n            \"size_bytes\": path.stat().st_size if exists else None,\n            \"absolute_path\": str(path.absolute()),\n        }\n\n    return results\n</code></pre>"},{"location":"api/#cal_disp.config.YamlModel.validate_ready_to_run","title":"validate_ready_to_run","text":"<pre><code>validate_ready_to_run() -&gt; ValidationResult\n</code></pre> <p>Check if configuration is ready to run.</p> Source code in <code>src/cal_disp/config/_yaml.py</code> <pre><code>def validate_ready_to_run(self) -&gt; ValidationResult:\n    \"\"\"Check if configuration is ready to run.\"\"\"\n    return ValidationResult(ready=True, errors=[], warnings=[])\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig","title":"pge_runconfig","text":""},{"location":"api/#cal_disp.config.pge_runconfig.OutputOptions","title":"OutputOptions","text":"<p>               Bases: <code>YamlModel</code></p> <p>Output configuration options.</p> <p>Attributes:</p> Name Type Description <code>product_version</code> <code>str</code> <p>Version of the product in . format. <code>output_format</code> <code>str</code> <p>Format for output files (e.g., 'netcdf', 'hdf5').</p> <code>compression</code> <code>bool</code> <p>Whether to compress output files.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>class OutputOptions(YamlModel):\n    \"\"\"Output configuration options.\n\n    Attributes\n    ----------\n    product_version : str\n        Version of the product in &lt;major&gt;.&lt;minor&gt; format.\n    output_format : str\n        Format for output files (e.g., 'netcdf', 'hdf5').\n    compression : bool\n        Whether to compress output files.\n\n    \"\"\"\n\n    product_version: str = Field(\n        default=\"1.0\",\n        description=\"Version of the product, in &lt;major&gt;.&lt;minor&gt; format.\",\n    )\n\n    output_format: str = Field(\n        default=\"netcdf\",\n        description=\"Output file format.\",\n    )\n\n    compression: bool = Field(\n        default=True,\n        description=\"Whether to compress output files.\",\n    )\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.PrimaryExecutable","title":"PrimaryExecutable","text":"<p>               Bases: <code>YamlModel</code></p> <p>Group describing the primary executable.</p> <p>Attributes:</p> Name Type Description <code>product_type</code> <code>str</code> <p>Product type identifier for the PGE.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>class PrimaryExecutable(YamlModel):\n    \"\"\"Group describing the primary executable.\n\n    Attributes\n    ----------\n    product_type : str\n        Product type identifier for the PGE.\n\n    \"\"\"\n\n    product_type: str = Field(\n        default=\"CAL_DISP\",\n        description=\"Product type of the PGE.\",\n    )\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.ProductPathGroup","title":"ProductPathGroup","text":"<p>               Bases: <code>YamlModel</code></p> <p>Group describing the product paths.</p> <p>Attributes:</p> Name Type Description <code>product_path</code> <code>Path</code> <p>Directory where PGE will place results.</p> <code>scratch_path</code> <code>Path</code> <p>Path to the scratch directory for intermediate files.</p> <code>output_path</code> <code>Path</code> <p>Path to the SAS output directory.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>class ProductPathGroup(YamlModel):\n    \"\"\"Group describing the product paths.\n\n    Attributes\n    ----------\n    product_path : Path\n        Directory where PGE will place results.\n    scratch_path : Path\n        Path to the scratch directory for intermediate files.\n    output_path : Path\n        Path to the SAS output directory.\n\n    \"\"\"\n\n    product_path: DirectoryPath = Field(\n        default=Path(),\n        description=\"Directory where PGE will place results.\",\n    )\n\n    scratch_path: DirectoryPath = Field(\n        default=Path(\"./scratch\"),\n        description=\"Path to the scratch directory for intermediate files.\",\n    )\n\n    output_path: DirectoryPath = Field(\n        default=Path(\"./output\"),\n        description=\"Path to the SAS output directory.\",\n        alias=\"sas_output_path\",\n    )\n\n    model_config = ConfigDict(extra=\"forbid\", populate_by_name=True)\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig","title":"RunConfig","text":"<p>               Bases: <code>YamlModel</code></p> <p>A PGE (Product Generation Executive) run configuration.</p> <p>This class represents the top-level configuration for running the calibration workflow as a PGE. It includes input files, output options, paths, and worker settings.</p> <p>Attributes:</p> Name Type Description <code>input_file_group</code> <code>InputFileGroup</code> <p>Configuration for input files.</p> <code>dynamic_ancillary_file_group</code> <code>Optional[DynamicAncillaryFileGroup]</code> <p>Dynamic ancillary files configuration.</p> <code>static_ancillary_file_group</code> <code>Optional[StaticAncillaryFileGroup]</code> <p>Static ancillary files configuration.</p> <code>output_options</code> <code>OutputOptions</code> <p>Output configuration options.</p> <code>primary_executable</code> <code>PrimaryExecutable</code> <p>Primary executable configuration.</p> <code>product_path_group</code> <code>ProductPathGroup</code> <p>Product path configuration.</p> <code>worker_settings</code> <code>WorkerSettings</code> <p>Dask worker and parallelism configuration.</p> <code>log_file</code> <code>Optional[Path]</code> <p>Path to the output log file.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>class RunConfig(YamlModel):\n    \"\"\"A PGE (Product Generation Executive) run configuration.\n\n    This class represents the top-level configuration for running the\n    calibration workflow as a PGE. It includes input files, output options,\n    paths, and worker settings.\n\n    Attributes\n    ----------\n    input_file_group : InputFileGroup\n        Configuration for input files.\n    dynamic_ancillary_file_group : Optional[DynamicAncillaryFileGroup]\n        Dynamic ancillary files configuration.\n    static_ancillary_file_group : Optional[StaticAncillaryFileGroup]\n        Static ancillary files configuration.\n    output_options : OutputOptions\n        Output configuration options.\n    primary_executable : PrimaryExecutable\n        Primary executable configuration.\n    product_path_group : ProductPathGroup\n        Product path configuration.\n    worker_settings : WorkerSettings\n        Dask worker and parallelism configuration.\n    log_file : Optional[Path]\n        Path to the output log file.\n\n    \"\"\"\n\n    # Used for the top-level key in YAML\n    name: ClassVar[str] = \"cal_disp_workflow\"\n\n    # Input configuration\n    input_file_group: InputFileGroup = Field(\n        ...,\n        description=\"Configuration for required input files.\",\n    )\n\n    dynamic_ancillary_file_group: Optional[DynamicAncillaryFileGroup] = Field(\n        default=None,\n        description=\"Dynamic ancillary files configuration.\",\n    )\n\n    static_ancillary_file_group: Optional[StaticAncillaryFileGroup] = Field(\n        default=None,\n        description=\"Static ancillary files configuration.\",\n    )\n\n    # Output and execution configuration\n    output_options: OutputOptions = Field(\n        default_factory=OutputOptions,\n        description=\"Output configuration options.\",\n    )\n\n    primary_executable: PrimaryExecutable = Field(\n        default_factory=PrimaryExecutable,\n        description=\"Primary executable configuration.\",\n    )\n\n    product_path_group: ProductPathGroup = Field(\n        default_factory=ProductPathGroup,\n        description=\"Product path configuration.\",\n    )\n\n    # Worker settings\n    worker_settings: WorkerSettings = Field(\n        default_factory=WorkerSettings,\n        description=\"Dask worker and parallelism configuration.\",\n    )\n\n    # Logging\n    log_file: Optional[Path] = Field(\n        default=None,\n        description=(\n            \"Path to the output log file in addition to logging to stderr. \"\n            \"If None, will be set based on output path.\"\n        ),\n    )\n\n    def to_workflow(self) -&gt; CalibrationWorkflow:\n        \"\"\"Convert PGE RunConfig to a CalibrationWorkflow object.\n\n        This method translates the PGE-style configuration into the format\n        expected by CalibrationWorkflow.\n\n        Returns\n        -------\n        CalibrationWorkflow\n            Converted workflow configuration.\n\n        Examples\n        --------\n        &gt;&gt;&gt; run_config = RunConfig.from_yaml_file(\"pge_config.yaml\")\n        &gt;&gt;&gt; workflow = run_config.to_workflow()\n        &gt;&gt;&gt; workflow.create_directories()\n        &gt;&gt;&gt; workflow.run()\n\n        \"\"\"\n        # Set up directories\n        scratch_directory = self.product_path_group.scratch_path\n        output_directory = self.product_path_group.output_path\n\n        # Set up log file\n        log_file = self.log_file\n        if log_file is None:\n            log_file = output_directory / \"cal_disp_workflow.log\"\n\n        # Create the workflow\n        workflow = CalibrationWorkflow(\n            # Input files\n            input_options=self.input_file_group,\n            # Ancillary files\n            dynamic_ancillary_file_options=self.dynamic_ancillary_file_group,\n            static_ancillary_file_options=self.static_ancillary_file_group,\n            # Directories\n            work_directory=scratch_directory,\n            output_directory=output_directory,\n            # Settings\n            worker_settings=self.worker_settings,\n            log_file=log_file,\n            # Don't resolve paths yet - let the workflow handle it\n            keep_paths_relative=False,\n        )\n\n        return workflow\n\n    def create_directories(self, exist_ok: bool = True) -&gt; None:\n        \"\"\"Create all necessary directories for the PGE run.\n\n        Parameters\n        ----------\n        exist_ok : bool, default=True\n            If True, don't raise error if directories already exist.\n\n        \"\"\"\n        self.product_path_group.product_path.mkdir(parents=True, exist_ok=exist_ok)\n        self.product_path_group.scratch_path.mkdir(parents=True, exist_ok=exist_ok)\n        self.product_path_group.output_path.mkdir(parents=True, exist_ok=exist_ok)\n\n        # Create tmp directory in scratch\n        tmp_dir = self.product_path_group.scratch_path / \"tmp\"\n        tmp_dir.mkdir(parents=True, exist_ok=exist_ok)\n\n    def validate_ready_to_run(self) -&gt; ValidationResult:\n        \"\"\"Check if run configuration is ready for execution.\"\"\"\n        errors: List[str] = []\n        warnings: List[str] = []\n\n        # Check if None instead\n        if self.input_file_group.disp_file is None:\n            errors.append(\"disp_file must be provided\")\n\n        if self.input_file_group.calibration_reference_grid is None:\n            errors.append(\"calibration_reference_grid must be provided\")\n\n        # Check dynamic ancillary files if provided\n        if self.dynamic_ancillary_file_group:\n            if self.dynamic_ancillary_file_group.algorithm_parameters_file is None:\n                warnings.append(\"Missing algorithm_parameters_file\")\n            if self.dynamic_ancillary_file_group.geometry_file is None:\n                warnings.append(\"Missing geometry_file\")\n\n        return {\n            \"ready\": len(errors) == 0,\n            \"errors\": errors,\n            \"warnings\": warnings,\n        }\n\n    def summary(self) -&gt; str:\n        \"\"\"Generate a human-readable summary of the run configuration.\n\n        Returns\n        -------\n        str\n            Multi-line summary string.\n\n        \"\"\"\n        lines = [\n            \"PGE Run Configuration\",\n            \"=\" * 70,\n            \"\",\n            \"Product Information:\",\n            f\"  Product type:     {self.primary_executable.product_type}\",\n            f\"  Product version:  {self.output_options.product_version}\",\n            f\"  Output format:    {self.output_options.output_format}\",\n            \"\",\n            \"Paths:\",\n            f\"  Product path:     {self.product_path_group.product_path}\",\n            f\"  Scratch path:     {self.product_path_group.scratch_path}\",\n            f\"  Output path:      {self.product_path_group.output_path}\",\n            f\"  Log file:         {self.log_file or 'Default'}\",\n            \"\",\n            \"Input Files:\",\n            f\"  DISP file:        {self.input_file_group.disp_file}\",\n            f\"  Calibration grid: {self.input_file_group.calibration_reference_grid}\",\n            \"\",\n            \"Worker Settings:\",\n            f\"  Workers:          {self.worker_settings.n_workers}\",\n            f\"  Threads/worker:   {self.worker_settings.threads_per_worker}\",\n            f\"  Total threads:    {self.worker_settings.total_threads}\",\n        ]\n\n        # Dynamic ancillary files\n        if self.dynamic_ancillary_file_group:\n            lines.extend(\n                [\n                    \"\",\n                    \"Dynamic Ancillary Files:\",\n                ]\n            )\n            dynamic_files = self.dynamic_ancillary_file_group.get_all_files()\n            for name, path in list(dynamic_files.items())[:5]:\n                lines.append(f\"  {name}: {path}\")\n            if len(dynamic_files) &gt; 5:\n                lines.append(f\"  ... and {len(dynamic_files) - 5} more\")\n\n        # Static ancillary files\n        if self.static_ancillary_file_group:\n            static_files = self.static_ancillary_file_group.get_all_files()\n            if static_files:\n                lines.extend(\n                    [\n                        \"\",\n                        \"Static Ancillary Files:\",\n                    ]\n                )\n                for name, path in static_files.items():\n                    lines.append(f\"  {name}: {path}\")\n\n        # Validation status\n        status = self.validate_ready_to_run()\n        lines.append(\"\")\n        if not status[\"ready\"]:\n            lines.append(\"\u26a0\ufe0f  Status: NOT READY\")\n            lines.append(\"Errors:\")\n            for error in status[\"errors\"]:\n                lines.append(f\"  - {error}\")\n        else:\n            lines.append(\"\u2713 Status: READY\")\n\n        if status[\"warnings\"]:\n            lines.append(\"\")\n            lines.append(\"Warnings:\")\n            for warning in status[\"warnings\"]:\n                lines.append(f\"  \u26a0\ufe0f  {warning}\")\n\n        lines.append(\"=\" * 70)\n\n        return \"\\n\".join(lines)\n\n    @classmethod\n    def create_example(cls) -&gt; \"RunConfig\":\n        \"\"\"Create an example PGE run configuration.\n\n        Returns\n        -------\n        RunConfig\n            Example configuration with placeholder values.\n\n        \"\"\"\n        return cls(\n            input_file_group=InputFileGroup(\n                disp_file=Path(\"input/disp.h5\"),\n                calibration_reference_grid=Path(\"input/cal_grid.parquet\"),\n                frame_id=1,\n            ),\n            dynamic_ancillary_file_group=DynamicAncillaryFileGroup(\n                algorithm_parameters_file=Path(\"config/algorithm_params.yaml\"),\n                static_layers_file=Path(\"input/geometry.h5\"),\n            ),\n            product_path_group=ProductPathGroup(\n                scratch_path=Path(\"./scratch\"), output_path=Path(\"./output\")\n            ),\n            worker_settings=WorkerSettings.create_standard(),\n        )\n\n    @classmethod\n    def from_yaml_file(cls, yaml_path: Path) -&gt; \"RunConfig\":\n        \"\"\"Load run configuration from YAML file.\n\n        Handles optional name wrapper key.\n\n        Parameters\n        ----------\n        yaml_path : Path\n            Path to YAML configuration file.\n\n        Returns\n        -------\n        RunConfig\n            Loaded and validated run configuration.\n\n        \"\"\"\n        data = cls._load_yaml_data(yaml_path)\n\n        # Handle optional wrapper key\n        if cls.name in data:\n            data = data[cls.name]\n\n        return cls.model_validate(data)\n\n    @staticmethod\n    def _load_yaml_data(yaml_path: Path) -&gt; dict:\n        \"\"\"Load YAML data from file.\n\n        Parameters\n        ----------\n        yaml_path : Path\n            Path to YAML file.\n\n        Returns\n        -------\n        dict\n            Loaded YAML data.\n\n        \"\"\"\n        from ruamel.yaml import YAML\n\n        y = YAML(typ=\"safe\")\n        with open(yaml_path) as f:\n            return y.load(f)\n\n    def to_yaml(\n        self,\n        output_path: Union[str, PathLike, TextIO],\n        with_comments: bool = True,  # noqa: ARG002\n        by_alias: bool = True,\n        indent_per_level: int = 2,\n    ) -&gt; None:  # Note: return type can be None or Any, both work\n        \"\"\"Save configuration to YAML file with name wrapper.\n\n        Parameters\n        ----------\n        output_path : str | PathLike | TextIO\n            Path where YAML should be saved.\n        with_comments : bool, default=True\n            Whether to include field descriptions as comments.\n        by_alias : bool, default=True\n            Whether to use field aliases in output.\n        indent_per_level : int, default=2\n            Indentation spacing.\n\n        Notes\n        -----\n        This method always wraps output in {cal_disp_workflow: ...} structure.\n        The with_comments parameter is accepted for signature compatibility but\n        not currently used in the wrapper output.\n\n        \"\"\"\n        from ruamel.yaml import YAML\n\n        # Handle file-like objects\n        if hasattr(output_path, \"write\"):\n            raise ValueError(\n                \"RunConfig.to_yaml doesn't support file-like objects. \"\n                \"Save to a file path instead.\"\n            )\n\n        # Convert to Path\n        output_path_obj = Path(output_path)\n\n        # Get dict and convert paths\n        data = self.model_dump(mode=\"python\", by_alias=by_alias)\n        data = convert_paths_to_strings(data)\n        wrapped = {self.name: data}\n\n        # Write with proper formatting\n        output_path_obj.parent.mkdir(parents=True, exist_ok=True)\n        y = YAML()\n        y.indent(\n            mapping=indent_per_level,\n            sequence=indent_per_level + 2,\n            offset=indent_per_level,\n        )\n        with open(output_path_obj, \"w\") as f:\n            y.dump(wrapped, f)\n\n    model_config = STRICT_CONFIG_WITH_ALIASES\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.create_directories","title":"create_directories","text":"<pre><code>create_directories(exist_ok: bool = True) -&gt; None\n</code></pre> <p>Create all necessary directories for the PGE run.</p> <p>Parameters:</p> Name Type Description Default <code>exist_ok</code> <code>bool</code> <p>If True, don't raise error if directories already exist.</p> <code>True</code> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>def create_directories(self, exist_ok: bool = True) -&gt; None:\n    \"\"\"Create all necessary directories for the PGE run.\n\n    Parameters\n    ----------\n    exist_ok : bool, default=True\n        If True, don't raise error if directories already exist.\n\n    \"\"\"\n    self.product_path_group.product_path.mkdir(parents=True, exist_ok=exist_ok)\n    self.product_path_group.scratch_path.mkdir(parents=True, exist_ok=exist_ok)\n    self.product_path_group.output_path.mkdir(parents=True, exist_ok=exist_ok)\n\n    # Create tmp directory in scratch\n    tmp_dir = self.product_path_group.scratch_path / \"tmp\"\n    tmp_dir.mkdir(parents=True, exist_ok=exist_ok)\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.create_example","title":"create_example  <code>classmethod</code>","text":"<pre><code>create_example() -&gt; 'RunConfig'\n</code></pre> <p>Create an example PGE run configuration.</p> <p>Returns:</p> Type Description <code>RunConfig</code> <p>Example configuration with placeholder values.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>@classmethod\ndef create_example(cls) -&gt; \"RunConfig\":\n    \"\"\"Create an example PGE run configuration.\n\n    Returns\n    -------\n    RunConfig\n        Example configuration with placeholder values.\n\n    \"\"\"\n    return cls(\n        input_file_group=InputFileGroup(\n            disp_file=Path(\"input/disp.h5\"),\n            calibration_reference_grid=Path(\"input/cal_grid.parquet\"),\n            frame_id=1,\n        ),\n        dynamic_ancillary_file_group=DynamicAncillaryFileGroup(\n            algorithm_parameters_file=Path(\"config/algorithm_params.yaml\"),\n            static_layers_file=Path(\"input/geometry.h5\"),\n        ),\n        product_path_group=ProductPathGroup(\n            scratch_path=Path(\"./scratch\"), output_path=Path(\"./output\")\n        ),\n        worker_settings=WorkerSettings.create_standard(),\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.from_yaml_file","title":"from_yaml_file  <code>classmethod</code>","text":"<pre><code>from_yaml_file(yaml_path: Path) -&gt; 'RunConfig'\n</code></pre> <p>Load run configuration from YAML file.</p> <p>Handles optional name wrapper key.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>Path</code> <p>Path to YAML configuration file.</p> required <p>Returns:</p> Type Description <code>RunConfig</code> <p>Loaded and validated run configuration.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>@classmethod\ndef from_yaml_file(cls, yaml_path: Path) -&gt; \"RunConfig\":\n    \"\"\"Load run configuration from YAML file.\n\n    Handles optional name wrapper key.\n\n    Parameters\n    ----------\n    yaml_path : Path\n        Path to YAML configuration file.\n\n    Returns\n    -------\n    RunConfig\n        Loaded and validated run configuration.\n\n    \"\"\"\n    data = cls._load_yaml_data(yaml_path)\n\n    # Handle optional wrapper key\n    if cls.name in data:\n        data = data[cls.name]\n\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.summary","title":"summary","text":"<pre><code>summary() -&gt; str\n</code></pre> <p>Generate a human-readable summary of the run configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Multi-line summary string.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>def summary(self) -&gt; str:\n    \"\"\"Generate a human-readable summary of the run configuration.\n\n    Returns\n    -------\n    str\n        Multi-line summary string.\n\n    \"\"\"\n    lines = [\n        \"PGE Run Configuration\",\n        \"=\" * 70,\n        \"\",\n        \"Product Information:\",\n        f\"  Product type:     {self.primary_executable.product_type}\",\n        f\"  Product version:  {self.output_options.product_version}\",\n        f\"  Output format:    {self.output_options.output_format}\",\n        \"\",\n        \"Paths:\",\n        f\"  Product path:     {self.product_path_group.product_path}\",\n        f\"  Scratch path:     {self.product_path_group.scratch_path}\",\n        f\"  Output path:      {self.product_path_group.output_path}\",\n        f\"  Log file:         {self.log_file or 'Default'}\",\n        \"\",\n        \"Input Files:\",\n        f\"  DISP file:        {self.input_file_group.disp_file}\",\n        f\"  Calibration grid: {self.input_file_group.calibration_reference_grid}\",\n        \"\",\n        \"Worker Settings:\",\n        f\"  Workers:          {self.worker_settings.n_workers}\",\n        f\"  Threads/worker:   {self.worker_settings.threads_per_worker}\",\n        f\"  Total threads:    {self.worker_settings.total_threads}\",\n    ]\n\n    # Dynamic ancillary files\n    if self.dynamic_ancillary_file_group:\n        lines.extend(\n            [\n                \"\",\n                \"Dynamic Ancillary Files:\",\n            ]\n        )\n        dynamic_files = self.dynamic_ancillary_file_group.get_all_files()\n        for name, path in list(dynamic_files.items())[:5]:\n            lines.append(f\"  {name}: {path}\")\n        if len(dynamic_files) &gt; 5:\n            lines.append(f\"  ... and {len(dynamic_files) - 5} more\")\n\n    # Static ancillary files\n    if self.static_ancillary_file_group:\n        static_files = self.static_ancillary_file_group.get_all_files()\n        if static_files:\n            lines.extend(\n                [\n                    \"\",\n                    \"Static Ancillary Files:\",\n                ]\n            )\n            for name, path in static_files.items():\n                lines.append(f\"  {name}: {path}\")\n\n    # Validation status\n    status = self.validate_ready_to_run()\n    lines.append(\"\")\n    if not status[\"ready\"]:\n        lines.append(\"\u26a0\ufe0f  Status: NOT READY\")\n        lines.append(\"Errors:\")\n        for error in status[\"errors\"]:\n            lines.append(f\"  - {error}\")\n    else:\n        lines.append(\"\u2713 Status: READY\")\n\n    if status[\"warnings\"]:\n        lines.append(\"\")\n        lines.append(\"Warnings:\")\n        for warning in status[\"warnings\"]:\n            lines.append(f\"  \u26a0\ufe0f  {warning}\")\n\n    lines.append(\"=\" * 70)\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.to_workflow","title":"to_workflow","text":"<pre><code>to_workflow() -&gt; CalibrationWorkflow\n</code></pre> <p>Convert PGE RunConfig to a CalibrationWorkflow object.</p> <p>This method translates the PGE-style configuration into the format expected by CalibrationWorkflow.</p> <p>Returns:</p> Type Description <code>CalibrationWorkflow</code> <p>Converted workflow configuration.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; run_config = RunConfig.from_yaml_file(\"pge_config.yaml\")\n&gt;&gt;&gt; workflow = run_config.to_workflow()\n&gt;&gt;&gt; workflow.create_directories()\n&gt;&gt;&gt; workflow.run()\n</code></pre> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>def to_workflow(self) -&gt; CalibrationWorkflow:\n    \"\"\"Convert PGE RunConfig to a CalibrationWorkflow object.\n\n    This method translates the PGE-style configuration into the format\n    expected by CalibrationWorkflow.\n\n    Returns\n    -------\n    CalibrationWorkflow\n        Converted workflow configuration.\n\n    Examples\n    --------\n    &gt;&gt;&gt; run_config = RunConfig.from_yaml_file(\"pge_config.yaml\")\n    &gt;&gt;&gt; workflow = run_config.to_workflow()\n    &gt;&gt;&gt; workflow.create_directories()\n    &gt;&gt;&gt; workflow.run()\n\n    \"\"\"\n    # Set up directories\n    scratch_directory = self.product_path_group.scratch_path\n    output_directory = self.product_path_group.output_path\n\n    # Set up log file\n    log_file = self.log_file\n    if log_file is None:\n        log_file = output_directory / \"cal_disp_workflow.log\"\n\n    # Create the workflow\n    workflow = CalibrationWorkflow(\n        # Input files\n        input_options=self.input_file_group,\n        # Ancillary files\n        dynamic_ancillary_file_options=self.dynamic_ancillary_file_group,\n        static_ancillary_file_options=self.static_ancillary_file_group,\n        # Directories\n        work_directory=scratch_directory,\n        output_directory=output_directory,\n        # Settings\n        worker_settings=self.worker_settings,\n        log_file=log_file,\n        # Don't resolve paths yet - let the workflow handle it\n        keep_paths_relative=False,\n    )\n\n    return workflow\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.to_yaml","title":"to_yaml","text":"<pre><code>to_yaml(output_path: Union[str, PathLike, TextIO], with_comments: bool = True, by_alias: bool = True, indent_per_level: int = 2) -&gt; None\n</code></pre> <p>Save configuration to YAML file with name wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str | PathLike | TextIO</code> <p>Path where YAML should be saved.</p> required <code>with_comments</code> <code>bool</code> <p>Whether to include field descriptions as comments.</p> <code>True</code> <code>by_alias</code> <code>bool</code> <p>Whether to use field aliases in output.</p> <code>True</code> <code>indent_per_level</code> <code>int</code> <p>Indentation spacing.</p> <code>2</code> Notes <p>This method always wraps output in {cal_disp_workflow: ...} structure. The with_comments parameter is accepted for signature compatibility but not currently used in the wrapper output.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>def to_yaml(\n    self,\n    output_path: Union[str, PathLike, TextIO],\n    with_comments: bool = True,  # noqa: ARG002\n    by_alias: bool = True,\n    indent_per_level: int = 2,\n) -&gt; None:  # Note: return type can be None or Any, both work\n    \"\"\"Save configuration to YAML file with name wrapper.\n\n    Parameters\n    ----------\n    output_path : str | PathLike | TextIO\n        Path where YAML should be saved.\n    with_comments : bool, default=True\n        Whether to include field descriptions as comments.\n    by_alias : bool, default=True\n        Whether to use field aliases in output.\n    indent_per_level : int, default=2\n        Indentation spacing.\n\n    Notes\n    -----\n    This method always wraps output in {cal_disp_workflow: ...} structure.\n    The with_comments parameter is accepted for signature compatibility but\n    not currently used in the wrapper output.\n\n    \"\"\"\n    from ruamel.yaml import YAML\n\n    # Handle file-like objects\n    if hasattr(output_path, \"write\"):\n        raise ValueError(\n            \"RunConfig.to_yaml doesn't support file-like objects. \"\n            \"Save to a file path instead.\"\n        )\n\n    # Convert to Path\n    output_path_obj = Path(output_path)\n\n    # Get dict and convert paths\n    data = self.model_dump(mode=\"python\", by_alias=by_alias)\n    data = convert_paths_to_strings(data)\n    wrapped = {self.name: data}\n\n    # Write with proper formatting\n    output_path_obj.parent.mkdir(parents=True, exist_ok=True)\n    y = YAML()\n    y.indent(\n        mapping=indent_per_level,\n        sequence=indent_per_level + 2,\n        offset=indent_per_level,\n    )\n    with open(output_path_obj, \"w\") as f:\n        y.dump(wrapped, f)\n</code></pre>"},{"location":"api/#cal_disp.config.pge_runconfig.RunConfig.validate_ready_to_run","title":"validate_ready_to_run","text":"<pre><code>validate_ready_to_run() -&gt; ValidationResult\n</code></pre> <p>Check if run configuration is ready for execution.</p> Source code in <code>src/cal_disp/config/pge_runconfig.py</code> <pre><code>def validate_ready_to_run(self) -&gt; ValidationResult:\n    \"\"\"Check if run configuration is ready for execution.\"\"\"\n    errors: List[str] = []\n    warnings: List[str] = []\n\n    # Check if None instead\n    if self.input_file_group.disp_file is None:\n        errors.append(\"disp_file must be provided\")\n\n    if self.input_file_group.calibration_reference_grid is None:\n        errors.append(\"calibration_reference_grid must be provided\")\n\n    # Check dynamic ancillary files if provided\n    if self.dynamic_ancillary_file_group:\n        if self.dynamic_ancillary_file_group.algorithm_parameters_file is None:\n            warnings.append(\"Missing algorithm_parameters_file\")\n        if self.dynamic_ancillary_file_group.geometry_file is None:\n            warnings.append(\"Missing geometry_file\")\n\n    return {\n        \"ready\": len(errors) == 0,\n        \"errors\": errors,\n        \"warnings\": warnings,\n    }\n</code></pre>"},{"location":"api/#cal_disp.config.workflow","title":"workflow","text":""},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow","title":"CalibrationWorkflow","text":"<p>               Bases: <code>YamlModel</code></p> <p>Calibration workflow configuration.</p> <p>This class manages the complete configuration for the displacement calibration workflow, including input files, output directories, worker settings, and logging configuration.</p> <p>Attributes:</p> Name Type Description <code>input_options</code> <code>Optional[InputFileGroup]</code> <p>Configuration for required input files.</p> <code>work_directory</code> <code>Path</code> <p>Directory for intermediate processing files.</p> <code>output_directory</code> <code>Path</code> <p>Directory for final output files.</p> <code>keep_paths_relative</code> <code>bool</code> <p>If False, resolve all paths to absolute paths.</p> <code>dynamic_ancillary_file_options</code> <code>Optional[DynamicAncillaryFileGroup]</code> <p>Optional dynamic ancillary files for processing.</p> <code>static_ancillary_file_options</code> <code>Optional[StaticAncillaryFileGroup]</code> <p>Optional static ancillary files for processing.</p> <code>worker_settings</code> <code>WorkerSettings</code> <p>Dask worker and threading configuration.</p> <code>log_file</code> <code>Optional[Path]</code> <p>Custom log file path.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>class CalibrationWorkflow(YamlModel):\n    \"\"\"Calibration workflow configuration.\n\n    This class manages the complete configuration for the displacement calibration\n    workflow, including input files, output directories, worker settings, and\n    logging configuration.\n\n    Attributes\n    ----------\n    input_options : Optional[InputFileGroup]\n        Configuration for required input files.\n    work_directory : Path\n        Directory for intermediate processing files.\n    output_directory : Path\n        Directory for final output files.\n    keep_paths_relative : bool\n        If False, resolve all paths to absolute paths.\n    dynamic_ancillary_file_options : Optional[DynamicAncillaryFileGroup]\n        Optional dynamic ancillary files for processing.\n    static_ancillary_file_options : Optional[StaticAncillaryFileGroup]\n        Optional static ancillary files for processing.\n    worker_settings : WorkerSettings\n        Dask worker and threading configuration.\n    log_file : Optional[Path]\n        Custom log file path.\n\n    \"\"\"\n\n    # Input/output file configuration\n    input_options: Optional[InputFileGroup] = Field(\n        default=None,\n        description=(\n            \"Configuration for required input files. Must be provided before running\"\n            \" workflow.\"\n        ),\n    )\n\n    work_directory: DirectoryPath = Field(\n        default=Path(),\n        description=(\n            \"Directory for intermediate processing files. Created if it doesn't exist.\"\n        ),\n    )\n\n    output_directory: DirectoryPath = Field(\n        default=Path(),\n        description=\"Directory for final output files. Created if it doesn't exist.\",\n    )\n\n    keep_paths_relative: bool = Field(\n        default=False,\n        description=(\n            \"If False, resolve all relative paths to absolute paths. \"\n            \"If True, keep paths as provided.\"\n        ),\n    )\n\n    # Optional ancillary file groups\n    dynamic_ancillary_options: Optional[DynamicAncillaryFileGroup] = Field(\n        default=None,\n        description=\"Dynamic ancillary files (dem, los, masks, troposphere, etc.).\",\n    )\n\n    static_ancillary_options: Optional[StaticAncillaryFileGroup] = Field(\n        default=None,\n        description=\"Static ancillary files (algorithm overrides, databases, etc.).\",\n    )\n\n    # Worker and logging configuration\n    worker_settings: WorkerSettings = Field(\n        default_factory=WorkerSettings,\n        description=\"Worker and parallelism configuration.\",\n    )\n\n    log_file: Optional[Path] = Field(\n        default=None,\n        description=(\n            \"Path to output log file (in addition to logging to stderr). \"\n            \"If None, defaults to 'cal_disp.log' in work_directory.\"\n        ),\n    )\n\n    @model_validator(mode=\"after\")\n    def _resolve_paths(self) -&gt; \"CalibrationWorkflow\":\n        \"\"\"Resolve all paths to absolute if keep_paths_relative is False.\n\n        Uses object.__setattr__() to bypass validate_assignment and avoid recursion.\n        \"\"\"\n        if not self.keep_paths_relative:\n            # Use object.__setattr__() to bypass validation and avoid recursion\n            object.__setattr__(self, \"work_directory\", self.work_directory.resolve())\n            object.__setattr__(\n                self, \"output_directory\", self.output_directory.resolve()\n            )\n\n            # Resolve log file path if provided\n            if self.log_file is not None:\n                object.__setattr__(self, \"log_file\", self.log_file.resolve())\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def _set_default_log_file(self) -&gt; \"CalibrationWorkflow\":\n        \"\"\"Set default log file path if not provided.\"\"\"\n        if self.log_file is None:\n            # Use object.__setattr__() to bypass validation\n            object.__setattr__(self, \"log_file\", self.work_directory / \"cal_disp.log\")\n\n        return self\n\n    def validate_ready_to_run(self) -&gt; ValidationResult:\n        \"\"\"Check if workflow is ready to run.\"\"\"\n        errors = []\n        warnings = []\n\n        if self.input_options is None:\n            errors.append(\"input_options must be provided\")\n        else:\n            if self.input_options.disp_file is None:\n                errors.append(\"disp_file must be provided in input_options\")\n            if self.input_options.calibration_reference_grid is None:\n                errors.append(\n                    \"calibration_reference_grid must be provided in input_options\"\n                )\n\n        # Check dynamic ancillaries\n        if self.dynamic_ancillary_options is None:\n            errors.append(\"dynamic_ancillary_options must be provided\")\n\n        # Check for missing files only if required options are set\n        if self.input_options is not None:\n            missing = self.get_missing_files()\n            if missing:\n                warnings.append(f\"Missing files: {', '.join(missing)}\")\n\n        return {\n            \"ready\": len(errors) == 0,\n            \"errors\": errors,\n            \"warnings\": warnings,\n        }\n\n    def validate_input_files_exist(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Check if all input files exist.\"\"\"\n        results = {}\n\n        if self.input_options:\n            results.update(self.input_options.validate_files_exist())\n\n        if self.dynamic_ancillary_options:\n            results.update(self.dynamic_ancillary_options.validate_files_exist())\n\n        if self.static_ancillary_options:\n            results.update(self.static_ancillary_options.validate_files_exist())\n\n        return results\n\n    def get_missing_files(self) -&gt; List[str]:\n        \"\"\"Get list of missing required input files.\"\"\"\n        status = self.validate_input_files_exist()\n        return [name for name, info in status.items() if not info[\"exists\"]]\n\n    def create_directories(self, exist_ok: bool = True) -&gt; None:\n        \"\"\"Create work and output directories if they don't exist.\n\n        Parameters\n        ----------\n        exist_ok : bool, default=True\n            If True, don't raise error if directories already exist.\n\n        \"\"\"\n        self.work_directory.mkdir(parents=True, exist_ok=exist_ok)\n        self.output_directory.mkdir(parents=True, exist_ok=exist_ok)\n\n        # Also create parent directory for log file\n        if self.log_file is not None:\n            self.log_file.parent.mkdir(parents=True, exist_ok=exist_ok)\n\n    def setup_logging(\n        self, level: int = 20, format_string: Optional[str] = None  # logging.INFO\n    ):\n        \"\"\"Set up logging configuration for the workflow.\n\n        Parameters\n        ----------\n        level : int, default=20 (INFO)\n            Logging level (DEBUG=10, INFO=20, WARNING=30, ERROR=40, CRITICAL=50).\n        format_string : str, optional\n            Custom format string for log messages.\n\n        Returns\n        -------\n        logging.Logger\n            Configured logger instance.\n\n        \"\"\"\n        import logging\n\n        if format_string is None:\n            format_string = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\n        logger = logging.getLogger(\"calibration_workflow\")\n        logger.setLevel(level)\n        logger.handlers.clear()\n\n        # Console handler\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(level)\n        console_handler.setFormatter(logging.Formatter(format_string))\n        logger.addHandler(console_handler)\n\n        # File handler\n        if self.log_file is not None:\n            self.log_file.parent.mkdir(parents=True, exist_ok=True)\n            file_handler = logging.FileHandler(self.log_file)\n            file_handler.setLevel(level)\n            file_handler.setFormatter(logging.Formatter(format_string))\n            logger.addHandler(file_handler)\n\n        return logger\n\n    def summary(self) -&gt; str:\n        \"\"\"Generate a human-readable summary of the workflow configuration.\n\n        Returns\n        -------\n        str\n            Multi-line summary string.\n\n        \"\"\"\n        lines = [\n            \"Calibration Workflow Configuration\",\n            \"=\" * 70,\n            \"\",\n            \"Directories:\",\n            f\"  Work directory:   {self.work_directory}\",\n            f\"  Output directory: {self.output_directory}\",\n            f\"  Log file:         {self.log_file}\",\n            f\"  Keep relative:    {self.keep_paths_relative}\",\n            \"\",\n        ]\n\n        # Input files\n        if self.input_options:\n            lines.extend(\n                [\n                    \"Input Files:\",\n                    f\"  DISP file:        {self.input_options.disp_file}\",\n                    (\n                        \"  Calibration grid:\"\n                        f\" {self.input_options.calibration_reference_grid}\"\n                    ),\n                    \"\",\n                ]\n            )\n        else:\n            lines.extend(\n                [\n                    \"Input Files:\",\n                    \"   Not configured!\",\n                    \"\",\n                ]\n            )\n\n        # Worker settings\n        lines.extend(\n            [\n                \"Worker Settings:\",\n                f\"  Workers:          {self.worker_settings.n_workers}\",\n                f\"  Threads/worker:   {self.worker_settings.threads_per_worker}\",\n                f\"  Total threads:    {self.worker_settings.total_threads}\",\n                f\"  Block shape:      {self.worker_settings.block_shape}\",\n                \"\",\n            ]\n        )\n\n        # Dynamic ancillary files\n        if self.dynamic_ancillary_options:\n            dynamic_files = self.dynamic_ancillary_options.get_all_files()\n            if dynamic_files:\n                lines.extend([\"Dynamic Ancillary Files:\"])\n                for name, path in list(dynamic_files.items())[:5]:  # Show first 5\n                    lines.append(f\"  {name}: {path}\")\n                if len(dynamic_files) &gt; 5:\n                    lines.append(f\"  ... and {len(dynamic_files) - 5} more\")\n                lines.append(\"\")\n\n        # Static ancillary files\n        if self.static_ancillary_options:\n            static_files = self.static_ancillary_options.get_all_files()\n            if static_files:\n                lines.extend([\"Static Ancillary Files:\"])\n                for name, path in static_files.items():\n                    lines.append(f\"  {name}: {path}\")\n                lines.append(\"\")\n\n        # Check readiness\n        status = self.validate_ready_to_run()\n        if not status[\"ready\"]:\n            lines.extend(\n                [\n                    \"  Workflow Status: NOT READY\",\n                    \"Errors:\",\n                ]\n            )\n            for error in status[\"errors\"]:\n                lines.append(f\"  - {error}\")\n        else:\n            lines.extend(\n                [\n                    \" Workflow Status: READY\",\n                ]\n            )\n\n        if status[\"warnings\"]:\n            lines.extend(\n                [\n                    \"\",\n                    \"Warnings:\",\n                ]\n            )\n            for warning in status[\"warnings\"]:\n                lines.append(f\"  \u26a0\ufe0f  {warning}\")\n\n        lines.append(\"=\" * 70)\n\n        return \"\\n\".join(lines)\n\n    @classmethod\n    def create_example(cls) -&gt; \"CalibrationWorkflow\":\n        \"\"\"Create an example workflow configuration.\n\n        Returns\n        -------\n        CalibrationWorkflow\n            Example configuration with placeholder values.\n\n        \"\"\"\n        return cls(\n            work_directory=Path(\"./work\"),\n            output_directory=Path(\"./output\"),\n            input_options=InputFileGroup(\n                disp_file=Path(\"input/disp.nc\"),\n                calibration_reference_grid=Path(\"input/cal_grid.parquet\"),\n                frame_id=1,\n            ),\n            dynamic_ancillary_options=DynamicAncillaryFileGroup(\n                algorithm_parameters_file=\"algorithm.yaml\",\n                static_los_file=\"line_of_sight_enu.tif\",\n                static_dem_file=\"dem.tif\",\n            ),\n            worker_settings=WorkerSettings.create_standard(),\n            keep_paths_relative=True,\n        )\n\n    @classmethod\n    def create_minimal(cls) -&gt; \"CalibrationWorkflow\":\n        \"\"\"Create a minimal workflow configuration without input files.\n\n        Returns\n        -------\n        CalibrationWorkflow\n            Minimal configuration. Input files must be added before running.\n\n        \"\"\"\n        return cls(\n            work_directory=Path(\"./work\"),\n            output_directory=Path(\"./output\"),\n        )\n\n    model_config = STRICT_CONFIG_WITH_ALIASES\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.create_directories","title":"create_directories","text":"<pre><code>create_directories(exist_ok: bool = True) -&gt; None\n</code></pre> <p>Create work and output directories if they don't exist.</p> <p>Parameters:</p> Name Type Description Default <code>exist_ok</code> <code>bool</code> <p>If True, don't raise error if directories already exist.</p> <code>True</code> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>def create_directories(self, exist_ok: bool = True) -&gt; None:\n    \"\"\"Create work and output directories if they don't exist.\n\n    Parameters\n    ----------\n    exist_ok : bool, default=True\n        If True, don't raise error if directories already exist.\n\n    \"\"\"\n    self.work_directory.mkdir(parents=True, exist_ok=exist_ok)\n    self.output_directory.mkdir(parents=True, exist_ok=exist_ok)\n\n    # Also create parent directory for log file\n    if self.log_file is not None:\n        self.log_file.parent.mkdir(parents=True, exist_ok=exist_ok)\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.create_example","title":"create_example  <code>classmethod</code>","text":"<pre><code>create_example() -&gt; 'CalibrationWorkflow'\n</code></pre> <p>Create an example workflow configuration.</p> <p>Returns:</p> Type Description <code>CalibrationWorkflow</code> <p>Example configuration with placeholder values.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>@classmethod\ndef create_example(cls) -&gt; \"CalibrationWorkflow\":\n    \"\"\"Create an example workflow configuration.\n\n    Returns\n    -------\n    CalibrationWorkflow\n        Example configuration with placeholder values.\n\n    \"\"\"\n    return cls(\n        work_directory=Path(\"./work\"),\n        output_directory=Path(\"./output\"),\n        input_options=InputFileGroup(\n            disp_file=Path(\"input/disp.nc\"),\n            calibration_reference_grid=Path(\"input/cal_grid.parquet\"),\n            frame_id=1,\n        ),\n        dynamic_ancillary_options=DynamicAncillaryFileGroup(\n            algorithm_parameters_file=\"algorithm.yaml\",\n            static_los_file=\"line_of_sight_enu.tif\",\n            static_dem_file=\"dem.tif\",\n        ),\n        worker_settings=WorkerSettings.create_standard(),\n        keep_paths_relative=True,\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.create_minimal","title":"create_minimal  <code>classmethod</code>","text":"<pre><code>create_minimal() -&gt; 'CalibrationWorkflow'\n</code></pre> <p>Create a minimal workflow configuration without input files.</p> <p>Returns:</p> Type Description <code>CalibrationWorkflow</code> <p>Minimal configuration. Input files must be added before running.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>@classmethod\ndef create_minimal(cls) -&gt; \"CalibrationWorkflow\":\n    \"\"\"Create a minimal workflow configuration without input files.\n\n    Returns\n    -------\n    CalibrationWorkflow\n        Minimal configuration. Input files must be added before running.\n\n    \"\"\"\n    return cls(\n        work_directory=Path(\"./work\"),\n        output_directory=Path(\"./output\"),\n    )\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.get_missing_files","title":"get_missing_files","text":"<pre><code>get_missing_files() -&gt; List[str]\n</code></pre> <p>Get list of missing required input files.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>def get_missing_files(self) -&gt; List[str]:\n    \"\"\"Get list of missing required input files.\"\"\"\n    status = self.validate_input_files_exist()\n    return [name for name, info in status.items() if not info[\"exists\"]]\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(level: int = 20, format_string: Optional[str] = None)\n</code></pre> <p>Set up logging configuration for the workflow.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Logging level (DEBUG=10, INFO=20, WARNING=30, ERROR=40, CRITICAL=50).</p> <code>20 (INFO)</code> <code>format_string</code> <code>str</code> <p>Custom format string for log messages.</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>Configured logger instance.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>def setup_logging(\n    self, level: int = 20, format_string: Optional[str] = None  # logging.INFO\n):\n    \"\"\"Set up logging configuration for the workflow.\n\n    Parameters\n    ----------\n    level : int, default=20 (INFO)\n        Logging level (DEBUG=10, INFO=20, WARNING=30, ERROR=40, CRITICAL=50).\n    format_string : str, optional\n        Custom format string for log messages.\n\n    Returns\n    -------\n    logging.Logger\n        Configured logger instance.\n\n    \"\"\"\n    import logging\n\n    if format_string is None:\n        format_string = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\n    logger = logging.getLogger(\"calibration_workflow\")\n    logger.setLevel(level)\n    logger.handlers.clear()\n\n    # Console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(level)\n    console_handler.setFormatter(logging.Formatter(format_string))\n    logger.addHandler(console_handler)\n\n    # File handler\n    if self.log_file is not None:\n        self.log_file.parent.mkdir(parents=True, exist_ok=True)\n        file_handler = logging.FileHandler(self.log_file)\n        file_handler.setLevel(level)\n        file_handler.setFormatter(logging.Formatter(format_string))\n        logger.addHandler(file_handler)\n\n    return logger\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.summary","title":"summary","text":"<pre><code>summary() -&gt; str\n</code></pre> <p>Generate a human-readable summary of the workflow configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Multi-line summary string.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>def summary(self) -&gt; str:\n    \"\"\"Generate a human-readable summary of the workflow configuration.\n\n    Returns\n    -------\n    str\n        Multi-line summary string.\n\n    \"\"\"\n    lines = [\n        \"Calibration Workflow Configuration\",\n        \"=\" * 70,\n        \"\",\n        \"Directories:\",\n        f\"  Work directory:   {self.work_directory}\",\n        f\"  Output directory: {self.output_directory}\",\n        f\"  Log file:         {self.log_file}\",\n        f\"  Keep relative:    {self.keep_paths_relative}\",\n        \"\",\n    ]\n\n    # Input files\n    if self.input_options:\n        lines.extend(\n            [\n                \"Input Files:\",\n                f\"  DISP file:        {self.input_options.disp_file}\",\n                (\n                    \"  Calibration grid:\"\n                    f\" {self.input_options.calibration_reference_grid}\"\n                ),\n                \"\",\n            ]\n        )\n    else:\n        lines.extend(\n            [\n                \"Input Files:\",\n                \"   Not configured!\",\n                \"\",\n            ]\n        )\n\n    # Worker settings\n    lines.extend(\n        [\n            \"Worker Settings:\",\n            f\"  Workers:          {self.worker_settings.n_workers}\",\n            f\"  Threads/worker:   {self.worker_settings.threads_per_worker}\",\n            f\"  Total threads:    {self.worker_settings.total_threads}\",\n            f\"  Block shape:      {self.worker_settings.block_shape}\",\n            \"\",\n        ]\n    )\n\n    # Dynamic ancillary files\n    if self.dynamic_ancillary_options:\n        dynamic_files = self.dynamic_ancillary_options.get_all_files()\n        if dynamic_files:\n            lines.extend([\"Dynamic Ancillary Files:\"])\n            for name, path in list(dynamic_files.items())[:5]:  # Show first 5\n                lines.append(f\"  {name}: {path}\")\n            if len(dynamic_files) &gt; 5:\n                lines.append(f\"  ... and {len(dynamic_files) - 5} more\")\n            lines.append(\"\")\n\n    # Static ancillary files\n    if self.static_ancillary_options:\n        static_files = self.static_ancillary_options.get_all_files()\n        if static_files:\n            lines.extend([\"Static Ancillary Files:\"])\n            for name, path in static_files.items():\n                lines.append(f\"  {name}: {path}\")\n            lines.append(\"\")\n\n    # Check readiness\n    status = self.validate_ready_to_run()\n    if not status[\"ready\"]:\n        lines.extend(\n            [\n                \"  Workflow Status: NOT READY\",\n                \"Errors:\",\n            ]\n        )\n        for error in status[\"errors\"]:\n            lines.append(f\"  - {error}\")\n    else:\n        lines.extend(\n            [\n                \" Workflow Status: READY\",\n            ]\n        )\n\n    if status[\"warnings\"]:\n        lines.extend(\n            [\n                \"\",\n                \"Warnings:\",\n            ]\n        )\n        for warning in status[\"warnings\"]:\n            lines.append(f\"  \u26a0\ufe0f  {warning}\")\n\n    lines.append(\"=\" * 70)\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.validate_input_files_exist","title":"validate_input_files_exist","text":"<pre><code>validate_input_files_exist() -&gt; Dict[str, Dict[str, Any]]\n</code></pre> <p>Check if all input files exist.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>def validate_input_files_exist(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Check if all input files exist.\"\"\"\n    results = {}\n\n    if self.input_options:\n        results.update(self.input_options.validate_files_exist())\n\n    if self.dynamic_ancillary_options:\n        results.update(self.dynamic_ancillary_options.validate_files_exist())\n\n    if self.static_ancillary_options:\n        results.update(self.static_ancillary_options.validate_files_exist())\n\n    return results\n</code></pre>"},{"location":"api/#cal_disp.config.workflow.CalibrationWorkflow.validate_ready_to_run","title":"validate_ready_to_run","text":"<pre><code>validate_ready_to_run() -&gt; ValidationResult\n</code></pre> <p>Check if workflow is ready to run.</p> Source code in <code>src/cal_disp/config/workflow.py</code> <pre><code>def validate_ready_to_run(self) -&gt; ValidationResult:\n    \"\"\"Check if workflow is ready to run.\"\"\"\n    errors = []\n    warnings = []\n\n    if self.input_options is None:\n        errors.append(\"input_options must be provided\")\n    else:\n        if self.input_options.disp_file is None:\n            errors.append(\"disp_file must be provided in input_options\")\n        if self.input_options.calibration_reference_grid is None:\n            errors.append(\n                \"calibration_reference_grid must be provided in input_options\"\n            )\n\n    # Check dynamic ancillaries\n    if self.dynamic_ancillary_options is None:\n        errors.append(\"dynamic_ancillary_options must be provided\")\n\n    # Check for missing files only if required options are set\n    if self.input_options is not None:\n        missing = self.get_missing_files()\n        if missing:\n            warnings.append(f\"Missing files: {', '.join(missing)}\")\n\n    return {\n        \"ready\": len(errors) == 0,\n        \"errors\": errors,\n        \"warnings\": warnings,\n    }\n</code></pre>"},{"location":"api/#browse-image","title":"Browse Image","text":""},{"location":"api/#cal_disp.browse_image","title":"cal_disp.browse_image","text":"<p>Module for creating browse images for the output product.</p>"},{"location":"api/#cal_disp.browse_image.make_browse_image_from_arr","title":"make_browse_image_from_arr","text":"<pre><code>make_browse_image_from_arr(output_filename: PathOrStr, arr: ArrayLike, mask: ArrayLike, max_dim_allowed: int = 2048, cmap: str = DEFAULT_CMAP, vmin: float = -0.1, vmax: float = 0.1) -&gt; None\n</code></pre> <p>Create a PNG browse image for the output product from given array.</p> Source code in <code>src/cal_disp/browse_image.py</code> <pre><code>def make_browse_image_from_arr(\n    output_filename: PathOrStr,\n    arr: ArrayLike,\n    mask: ArrayLike,\n    max_dim_allowed: int = 2048,\n    cmap: str = DEFAULT_CMAP,\n    vmin: float = -0.10,\n    vmax: float = 0.10,\n) -&gt; None:\n    \"\"\"Create a PNG browse image for the output product from given array.\"\"\"\n    arr[mask == 0] = np.nan\n    arr = _resize_to_max_pixel_dim(arr, max_dim_allowed)\n    _save_to_disk_as_color(arr, output_filename, cmap, vmin, vmax)\n</code></pre>"},{"location":"api/#cal_disp.browse_image.make_browse_image_from_nc","title":"make_browse_image_from_nc","text":"<pre><code>make_browse_image_from_nc(output_filename: PathOrStr, input_filename: PathOrStr, max_dim_allowed: int = 2048, cmap: str = DEFAULT_CMAP, vmin: float = -0.1, vmax: float = 0.1) -&gt; None\n</code></pre> <p>Create a PNG browse image for the output product from product in NetCDF file.</p> Source code in <code>src/cal_disp/browse_image.py</code> <pre><code>def make_browse_image_from_nc(\n    output_filename: PathOrStr,\n    input_filename: PathOrStr,\n    max_dim_allowed: int = 2048,\n    cmap: str = DEFAULT_CMAP,\n    vmin: float = -0.10,\n    vmax: float = 0.10,\n) -&gt; None:\n    \"\"\"Create a PNG browse image for the output product from product in NetCDF file.\"\"\"\n    arr = np.nanarray(input_filename)  # placeholder\n    mask = np.nanarray  # placeholder\n\n    make_browse_image_from_arr(\n        output_filename, arr, mask, max_dim_allowed, cmap, vmin, vmax\n    )\n</code></pre>"},{"location":"tutorials/config/","title":"Configuration","text":"In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\nfrom cal_disp.config import CalibrationWorkflow\n\n# Create minimal config - just directories\nconfig = CalibrationWorkflow.create_minimal()\nprint(config.summary())\n</pre> from pathlib import Path  from cal_disp.config import CalibrationWorkflow  # Create minimal config - just directories config = CalibrationWorkflow.create_minimal() print(config.summary()) <p>Note: This config isn't ready to run yet. It needs input files.</p> In\u00a0[\u00a0]: Copied! <pre>from cal_disp.config import DynamicAncillaryFileGroup, InputFileGroup\n\n# Set up input files\nconfig.input_options = InputFileGroup(\n    disp_file=Path(\"data/OPERA_L3_DISP-S1_T001-000001-IFG_20240101_20240113_v1.0.nc\"),\n    calibration_reference_grid=Path(\"data/reference_grid.parquet\"),\n    frame_id=1,\n)\n\n# Set up dynamic ancillaries\nconfig.dynamic_ancillary_file_options = DynamicAncillaryFileGroup(\n    dem_file=Path(\"data/dem.tif\"),\n    los_east_file=Path(\"data/los_east.tif\"),\n    los_north_file=Path(\"data/los_north.tif\"),\n)\n\n# Check if ready to run\nstatus = config.validate_ready_to_run()\nprint(f\"Ready to run: {status['ready']}\")\nif status['errors']:\n    print(\"Errors:\")\n    for error in status['errors']:\n        print(f\"  - {error}\")\n</pre> from cal_disp.config import DynamicAncillaryFileGroup, InputFileGroup  # Set up input files config.input_options = InputFileGroup(     disp_file=Path(\"data/OPERA_L3_DISP-S1_T001-000001-IFG_20240101_20240113_v1.0.nc\"),     calibration_reference_grid=Path(\"data/reference_grid.parquet\"),     frame_id=1, )  # Set up dynamic ancillaries config.dynamic_ancillary_file_options = DynamicAncillaryFileGroup(     dem_file=Path(\"data/dem.tif\"),     los_east_file=Path(\"data/los_east.tif\"),     los_north_file=Path(\"data/los_north.tif\"), )  # Check if ready to run status = config.validate_ready_to_run() print(f\"Ready to run: {status['ready']}\") if status['errors']:     print(\"Errors:\")     for error in status['errors']:         print(f\"  - {error}\") In\u00a0[\u00a0]: Copied! <pre># Save to YAML\nyaml_path = Path(\"workflow_config.yaml\")\nconfig.to_yaml(yaml_path)\nprint(f\"Saved to {yaml_path}\")\n\n# View the YAML\nprint(\"\\nYAML contents:\")\nprint(yaml_path.read_text())\n</pre> # Save to YAML yaml_path = Path(\"workflow_config.yaml\") config.to_yaml(yaml_path) print(f\"Saved to {yaml_path}\")  # View the YAML print(\"\\nYAML contents:\") print(yaml_path.read_text()) In\u00a0[\u00a0]: Copied! <pre># Load from YAML\nloaded_config = CalibrationWorkflow.from_yaml(yaml_path)\nprint(loaded_config.summary())\n</pre> # Load from YAML loaded_config = CalibrationWorkflow.from_yaml(yaml_path) print(loaded_config.summary()) In\u00a0[\u00a0]: Copied! <pre>from cal_disp.config import WorkerSettings\n\n# Default settings (auto-detect CPU count)\nconfig.worker_settings = WorkerSettings.create_standard()\nprint(f\"Workers: {config.worker_settings.n_workers}\")\nprint(f\"Threads per worker: {config.worker_settings.threads_per_worker}\")\nprint(f\"Total threads: {config.worker_settings.total_threads}\")\n\n# Custom settings for a specific machine\nconfig.worker_settings = WorkerSettings(\n    n_workers=4,\n    threads_per_worker=2,\n    block_shape=(512, 512),\n)\nprint(f\"\\nCustom: {config.worker_settings.total_threads} total threads\")\n</pre> from cal_disp.config import WorkerSettings  # Default settings (auto-detect CPU count) config.worker_settings = WorkerSettings.create_standard() print(f\"Workers: {config.worker_settings.n_workers}\") print(f\"Threads per worker: {config.worker_settings.threads_per_worker}\") print(f\"Total threads: {config.worker_settings.total_threads}\")  # Custom settings for a specific machine config.worker_settings = WorkerSettings(     n_workers=4,     threads_per_worker=2,     block_shape=(512, 512), ) print(f\"\\nCustom: {config.worker_settings.total_threads} total threads\") In\u00a0[\u00a0]: Copied! <pre># Check all input files\nfile_status = config.validate_input_files_exist()\n\nprint(\"File validation results:\")\nfor filename, info in file_status.items():\n    status = \"\u2713\" if info['exists'] else \"\u2717\"\n    print(f\"  {status} {filename}: {info['path']}\")\n\n# Get just the missing files\nmissing = config.get_missing_files()\nif missing:\n    print(f\"\\nMissing files: {', '.join(missing)}\")\nelse:\n    print(\"\\nAll files exist!\")\n</pre> # Check all input files file_status = config.validate_input_files_exist()  print(\"File validation results:\") for filename, info in file_status.items():     status = \"\u2713\" if info['exists'] else \"\u2717\"     print(f\"  {status} {filename}: {info['path']}\")  # Get just the missing files missing = config.get_missing_files() if missing:     print(f\"\\nMissing files: {', '.join(missing)}\") else:     print(\"\\nAll files exist!\") In\u00a0[\u00a0]: Copied! <pre>from cal_disp.config import StaticAncillaryFileGroup\n\nconfig.static_ancillary_file_options = StaticAncillaryFileGroup(\n    algorithm_parameters_overrides_json=Path(\"data/custom_params.json\"),\n    # Add other static files as needed\n)\n\nprint(config.summary())\n</pre> from cal_disp.config import StaticAncillaryFileGroup  config.static_ancillary_file_options = StaticAncillaryFileGroup(     algorithm_parameters_overrides_json=Path(\"data/custom_params.json\"),     # Add other static files as needed )  print(config.summary()) In\u00a0[\u00a0]: Copied! <pre># Create directories\nconfig.create_directories()\nprint(f\"Work directory: {config.work_directory}\")\nprint(f\"Output directory: {config.output_directory}\")\nprint(f\"Log file: {config.log_file}\")\n\n# Set up logging\nimport logging\n\nlogger = config.setup_logging(level=logging.INFO)\nlogger.info(\"Workflow initialized\")\n</pre> # Create directories config.create_directories() print(f\"Work directory: {config.work_directory}\") print(f\"Output directory: {config.output_directory}\") print(f\"Log file: {config.log_file}\")  # Set up logging import logging  logger = config.setup_logging(level=logging.INFO) logger.info(\"Workflow initialized\") In\u00a0[\u00a0]: Copied! <pre># Create a complete config in one go\ncomplete_config = CalibrationWorkflow(\n    work_directory=Path(\"./processing/work\"),\n    output_directory=Path(\"./processing/output\"),\n    input_options=InputFileGroup(\n        disp_file=Path(\"data/disp.nc\"),\n        calibration_reference_grid=Path(\"data/ref_grid.parquet\"),\n        frame_id=1,\n    ),\n    dynamic_ancillary_file_options=DynamicAncillaryFileGroup(\n        dem_file=Path(\"data/dem.tif\"),\n        los_east_file=Path(\"data/los_east.tif\"),\n        los_north_file=Path(\"data/los_north.tif\"),\n    ),\n    worker_settings=WorkerSettings(\n        n_workers=4,\n        threads_per_worker=2,\n    ),\n    keep_paths_relative=True,  # Keep relative for portability\n)\n\n# Save and verify\ncomplete_config.to_yaml(\"production_config.yaml\")\nprint(complete_config.summary())\n</pre> # Create a complete config in one go complete_config = CalibrationWorkflow(     work_directory=Path(\"./processing/work\"),     output_directory=Path(\"./processing/output\"),     input_options=InputFileGroup(         disp_file=Path(\"data/disp.nc\"),         calibration_reference_grid=Path(\"data/ref_grid.parquet\"),         frame_id=1,     ),     dynamic_ancillary_file_options=DynamicAncillaryFileGroup(         dem_file=Path(\"data/dem.tif\"),         los_east_file=Path(\"data/los_east.tif\"),         los_north_file=Path(\"data/los_north.tif\"),     ),     worker_settings=WorkerSettings(         n_workers=4,         threads_per_worker=2,     ),     keep_paths_relative=True,  # Keep relative for portability )  # Save and verify complete_config.to_yaml(\"production_config.yaml\") print(complete_config.summary()) In\u00a0[\u00a0]: Copied! <pre># Absolute paths (default)\nabs_config = CalibrationWorkflow(\n    work_directory=Path(\"./work\"),\n    output_directory=Path(\"./output\"),\n    keep_paths_relative=False,\n)\nprint(f\"Work dir: {abs_config.work_directory}\")\n\n# Relative paths (for portability)\nrel_config = CalibrationWorkflow(\n    work_directory=Path(\"./work\"),\n    output_directory=Path(\"./output\"),\n    keep_paths_relative=True,\n)\nprint(f\"Work dir: {rel_config.work_directory}\")\n</pre> # Absolute paths (default) abs_config = CalibrationWorkflow(     work_directory=Path(\"./work\"),     output_directory=Path(\"./output\"),     keep_paths_relative=False, ) print(f\"Work dir: {abs_config.work_directory}\")  # Relative paths (for portability) rel_config = CalibrationWorkflow(     work_directory=Path(\"./work\"),     output_directory=Path(\"./output\"),     keep_paths_relative=True, ) print(f\"Work dir: {rel_config.work_directory}\") In\u00a0[\u00a0]: Copied! <pre>try:\n    # This will fail - invalid type for n_workers\n    bad_config = CalibrationWorkflow(\n        worker_settings=WorkerSettings(n_workers=\"not a number\")\n    )\nexcept Exception as e:\n    print(f\"Validation error: {e}\")\n</pre> try:     # This will fail - invalid type for n_workers     bad_config = CalibrationWorkflow(         worker_settings=WorkerSettings(n_workers=\"not a number\")     ) except Exception as e:     print(f\"Validation error: {e}\") In\u00a0[\u00a0]: Copied! <pre># Check readiness before running\nincomplete_config = CalibrationWorkflow.create_minimal()\nstatus = incomplete_config.validate_ready_to_run()\n\nif not status['ready']:\n    print(\"Not ready to run!\")\n    print(\"Errors:\")\n    for error in status['errors']:\n        print(f\"  - {error}\")\nelse:\n    print(\"Ready to run!\")\n</pre> # Check readiness before running incomplete_config = CalibrationWorkflow.create_minimal() status = incomplete_config.validate_ready_to_run()  if not status['ready']:     print(\"Not ready to run!\")     print(\"Errors:\")     for error in status['errors']:         print(f\"  - {error}\") else:     print(\"Ready to run!\")"},{"location":"tutorials/config/#configuration","title":"Configuration\u00b6","text":"<p>This tutorial covers the configuration system for the displacement calibration workflow.</p> <p>Philosophy: The config system uses Pydantic for validation at serialization boundaries. This means:</p> <ul> <li>Type-safe configurations with automatic validation</li> <li>Clear error messages when something is wrong</li> <li>YAML files that map directly to Python objects</li> <li>No scattered validation checks throughout the codebase</li> </ul>"},{"location":"tutorials/config/#quick-start-minimal-configuration","title":"Quick Start: Minimal Configuration\u00b6","text":"<p>Let's start with the simplest possible config:</p>"},{"location":"tutorials/config/#step-1-adding-required-input-files","title":"Step 1: Adding Required Input Files\u00b6","text":"<p>Every workflow needs:</p> <ol> <li>A displacement file (DISP-S1 product)</li> <li>A calibration reference grid</li> <li>Dynamic ancillary files (DEM, LOS, masks, etc.)</li> </ol>"},{"location":"tutorials/config/#step-2-working-with-yaml-files","title":"Step 2: Working with YAML Files\u00b6","text":"<p>Configs are typically stored as YAML files. Here's the round-trip:</p>"},{"location":"tutorials/config/#step-3-worker-configuration","title":"Step 3: Worker Configuration\u00b6","text":"<p>Control parallelism and memory usage:</p>"},{"location":"tutorials/config/#step-4-file-validation","title":"Step 4: File Validation\u00b6","text":"<p>Check which files exist before running:</p>"},{"location":"tutorials/config/#step-5-static-ancillary-files-optional","title":"Step 5: Static Ancillary Files (Optional)\u00b6","text":"<p>For algorithm overrides, custom databases, etc:</p>"},{"location":"tutorials/config/#step-6-directory-management","title":"Step 6: Directory Management\u00b6","text":"<p>Create output directories and set up logging:</p>"},{"location":"tutorials/config/#complete-example-from-scratch","title":"Complete Example: From Scratch\u00b6","text":"<p>Putting it all together:</p>"},{"location":"tutorials/config/#path-resolution","title":"Path Resolution\u00b6","text":"<p>By default, paths are resolved to absolute. Control this with <code>keep_paths_relative</code>:</p>"},{"location":"tutorials/config/#error-handling","title":"Error Handling\u00b6","text":"<p>The config system validates at creation time:</p>"},{"location":"tutorials/config/#command-line-usage","title":"Command-Line Usage\u00b6","text":"<p>Typical workflow from the command line:</p> <pre># Create a template config\npython -c \"from cal_disp.config import CalibrationWorkflow; \\\n           CalibrationWorkflow.create_example().to_yaml('config.yaml')\"\n\n# Edit config.yaml with your paths\nvim config.yaml\n\n# Run the workflow\ncal-disp run config.yaml\n</pre>"},{"location":"tutorials/config/#tips-and-best-practices","title":"Tips and Best Practices\u00b6","text":"<ol> <li>Start with <code>create_minimal()</code> or <code>create_example()</code> - don't build configs from scratch</li> <li>Use relative paths when configs need to be portable across machines</li> <li>Always call <code>validate_ready_to_run()</code> before starting a long job</li> <li>Set up logging early with <code>setup_logging()</code> to catch issues</li> <li>Version control your YAML configs - they're small and readable</li> <li>Use <code>summary()</code> to sanity-check your configuration before running</li> </ol>"},{"location":"tutorials/config/#common-patterns","title":"Common Patterns\u00b6","text":""},{"location":"tutorials/config/#pattern-1-load-modify-save","title":"Pattern 1: Load, Modify, Save\u00b6","text":"<pre># Load existing config\nconfig = CalibrationWorkflow.from_yaml(\"config.yaml\")\n\n# Modify one thing\nconfig.worker_settings.n_workers = 8\n\n# Save as new config\nconfig.to_yaml(\"config_8workers.yaml\")\n</pre>"},{"location":"tutorials/config/#pattern-2-batch-processing","title":"Pattern 2: Batch Processing\u00b6","text":"<pre># Create configs for multiple frames\nbase_config = CalibrationWorkflow.from_yaml(\"base_config.yaml\")\n\nfor frame_id in [1, 2, 3, 4]:\n    config = base_config.model_copy(deep=True)\n    config.input_options.frame_id = frame_id\n    config.input_options.disp_file = Path(f\"data/frame_{frame_id}.nc\")\n    config.output_directory = Path(f\"output/frame_{frame_id}\")\n    config.to_yaml(f\"config_frame_{frame_id}.yaml\")\n</pre>"},{"location":"tutorials/config/#pattern-3-conditional-configuration","title":"Pattern 3: Conditional Configuration\u00b6","text":"<pre>import os\n\n# Different settings for local vs HPC\nif os.getenv(\"SLURM_JOB_ID\"):\n    # On HPC cluster\n    config.worker_settings = WorkerSettings(\n        n_workers=int(os.getenv(\"SLURM_CPUS_PER_TASK\", 16)),\n        threads_per_worker=1,\n    )\nelse:\n    # Local machine\n    config.worker_settings = WorkerSettings.create_standard()\n</pre>"},{"location":"tutorials/config/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>See <code>docs/tutorials/workflow.ipynb</code> for running the full calibration workflow</li> <li>Check the API docs for all available options</li> <li>Look at <code>examples/</code> directory for real-world configurations</li> </ul>"},{"location":"tutorials/download/","title":"Download","text":"In\u00a0[1]: Copied! <pre>from datetime import datetime\nfrom pathlib import Path\n\nfrom cal_disp.download import (\n    download_disp,\n    download_tropo,\n    download_unr_grid,\n    generate_s1_burst_tiles,\n)\nfrom cal_disp.download.utils import extract_sensing_times_from_file\n</pre> from datetime import datetime from pathlib import Path  from cal_disp.download import (     download_disp,     download_tropo,     download_unr_grid,     generate_s1_burst_tiles, ) from cal_disp.download.utils import extract_sensing_times_from_file <pre>/u/aurora-r0/govorcin/miniconda/miniforge/envs/my-cal-disp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre>from folium import LayerControl\nfrom opera_utils import get_frame_geojson\n\ndisp_df = get_frame_geojson(as_geodataframe=True)\n\n# Filter to OPERA DISP geogrphical scope\ndisp_df = disp_df[disp_df.is_north_america]\nm = disp_df[disp_df.orbit_pass.str.startswith('D')].explore(name='DSC')\nm = disp_df[disp_df.orbit_pass.str.startswith('A')].explore(name='ASC', m=m)\nLayerControl().add_to(m)\nm\n</pre> from folium import LayerControl from opera_utils import get_frame_geojson  disp_df = get_frame_geojson(as_geodataframe=True)  # Filter to OPERA DISP geogrphical scope disp_df = disp_df[disp_df.is_north_america] m = disp_df[disp_df.orbit_pass.str.startswith('D')].explore(name='DSC') m = disp_df[disp_df.orbit_pass.str.startswith('A')].explore(name='ASC', m=m) LayerControl().add_to(m) m Out[2]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[3]: Copied! <pre># Frame and dates\nframe_id = 8882\nstart_date = datetime(2022, 7, 22)\nend_date = datetime(2022, 7, 22)\n\n# Base data directory\ndata_dir = Path(\"data\")\ndata_dir.mkdir(exist_ok=True)\n\n# Output directories for each product type\ndisp_dir = data_dir / \"disp\"\ntropo_dir = data_dir / \"tropo\"\nunr_dir = data_dir / \"unr\"\ntiles_dir = data_dir / \"s1_burst_bounds\"\n\nprint(f\"Frame ID: {frame_id}\")\nprint(f\"Date range: {start_date.date()} \u2192 {end_date.date()}\")\nprint(f\"Base directory: {data_dir.absolute()}\")\n</pre> # Frame and dates frame_id = 8882 start_date = datetime(2022, 7, 22) end_date = datetime(2022, 7, 22)  # Base data directory data_dir = Path(\"data\") data_dir.mkdir(exist_ok=True)  # Output directories for each product type disp_dir = data_dir / \"disp\" tropo_dir = data_dir / \"tropo\" unr_dir = data_dir / \"unr\" tiles_dir = data_dir / \"s1_burst_bounds\"  print(f\"Frame ID: {frame_id}\") print(f\"Date range: {start_date.date()} \u2192 {end_date.date()}\") print(f\"Base directory: {data_dir.absolute()}\") <pre>Frame ID: 8882\nDate range: 2022-07-22 \u2192 2022-07-22\nBase directory: /u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data\n</pre> In\u00a0[4]: Copied! <pre># Download products\ndownload_disp(\n    frame_id=frame_id,\n    output_dir=disp_dir,\n    start=start_date,\n    end=end_date,\n    num_workers=4,\n)\n\nprint(\"Download complete!\")\ndisp_files = sorted((disp_dir).glob(\"*.nc\"))\nprint(f\"Downloaded {len(disp_files)} DISP products\\n\")\n\n# Show first 5 files\nfor f in disp_files[:5]:\n    print(f\"  {f.name}\")\n</pre> # Download products download_disp(     frame_id=frame_id,     output_dir=disp_dir,     start=start_date,     end=end_date,     num_workers=4, )  print(\"Download complete!\") disp_files = sorted((disp_dir).glob(\"*.nc\")) print(f\"Downloaded {len(disp_files)} DISP products\\n\")  # Show first 5 files for f in disp_files[:5]:     print(f\"  {f.name}\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [03:01&lt;00:00, 181.72s/it]</pre> <pre>Download complete!\nDownloaded 7 DISP products\n\n  OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc\n  OPERA_L3_DISP-S1_IW_F08882_VV_20230717T002702Z_20240113T002702Z_v1.0_20251028T001623Z.nc\n  OPERA_L3_DISP-S1_IW_F08882_VV_20230717T002702Z_20240125T002701Z_v1.0_20251028T001623Z.nc\n  OPERA_L3_DISP-S1_IW_F08882_VV_20240125T002701Z_20240218T002701Z_v1.0_20251028T074515Z.nc\n  OPERA_L3_DISP-S1_IW_F08882_VV_20240125T002701Z_20240301T002701Z_v1.0_20251028T074515Z.nc\n</pre> <pre>\n</pre> In\u00a0[6]: Copied! <pre># Use first DISP product to determine bounds and dates\ndisp_path = disp_files[0]\n# Extract reference and secondary date from OPERA DISP-S1 product filename\nsensing_times = extract_sensing_times_from_file(disp_files[0])\n\n# Download TROPO products with interpolation\ndownload_tropo(\n    output_dir=tropo_dir,\n    disp_times=sensing_times,\n    num_workers=2,\n    interp=True,  # Download products for interpolation\n)\n\ntropo_files = list((tropo_dir).glob(\"*.nc\"))\nprint(f\"Downloaded {len(tropo_files)} troposphere files\")\nfor f in tropo_files:\n    print(f\"  {f.name}\")\n</pre> # Use first DISP product to determine bounds and dates disp_path = disp_files[0] # Extract reference and secondary date from OPERA DISP-S1 product filename sensing_times = extract_sensing_times_from_file(disp_files[0])  # Download TROPO products with interpolation download_tropo(     output_dir=tropo_dir,     disp_times=sensing_times,     num_workers=2,     interp=True,  # Download products for interpolation )  tropo_files = list((tropo_dir).glob(\"*.nc\")) print(f\"Downloaded {len(tropo_files)} troposphere files\") for f in tropo_files:     print(f\"  {f.name}\") <pre>/u/aurora-r0/govorcin/miniconda/miniforge/envs/my-cal-disp/lib/python3.11/site-packages/asf_search/download/download.py:66: UserWarning: File already exists, skipping download: data/tropo/OPERA_L4_TROPO-ZENITH_20220111T060000Z_20250923T231305Z_HRES_v1.0.nc\n  warnings.warn(f'File already exists, skipping download: {os.path.join(path, filename)}')\n/u/aurora-r0/govorcin/miniconda/miniforge/envs/my-cal-disp/lib/python3.11/site-packages/asf_search/download/download.py:66: UserWarning: File already exists, skipping download: data/tropo/OPERA_L4_TROPO-ZENITH_20220111T000000Z_20250923T224940Z_HRES_v1.0.nc\n  warnings.warn(f'File already exists, skipping download: {os.path.join(path, filename)}')\n/u/aurora-r0/govorcin/miniconda/miniforge/envs/my-cal-disp/lib/python3.11/site-packages/asf_search/download/download.py:66: UserWarning: File already exists, skipping download: data/tropo/OPERA_L4_TROPO-ZENITH_20220722T060000Z_20250923T230822Z_HRES_v1.0.nc\n  warnings.warn(f'File already exists, skipping download: {os.path.join(path, filename)}')\n/u/aurora-r0/govorcin/miniconda/miniforge/envs/my-cal-disp/lib/python3.11/site-packages/asf_search/download/download.py:66: UserWarning: File already exists, skipping download: data/tropo/OPERA_L4_TROPO-ZENITH_20220722T000000Z_20250923T233421Z_HRES_v1.0.nc\n  warnings.warn(f'File already exists, skipping download: {os.path.join(path, filename)}')\n</pre> <pre>Downloaded 6 troposphere files\n  OPERA_L4_TROPO-ZENITH_20240113T000000Z_20250924T032011Z_HRES_v1.0.nc\n  OPERA_L4_TROPO-ZENITH_20230717T000000Z_20250924T010719Z_HRES_v1.0.nc\n  OPERA_L4_TROPO-ZENITH_20220111T060000Z_20250923T231305Z_HRES_v1.0.nc\n  OPERA_L4_TROPO-ZENITH_20220111T000000Z_20250923T224940Z_HRES_v1.0.nc\n  OPERA_L4_TROPO-ZENITH_20220722T000000Z_20250923T233421Z_HRES_v1.0.nc\n  OPERA_L4_TROPO-ZENITH_20220722T060000Z_20250923T230822Z_HRES_v1.0.nc\n</pre> In\u00a0[7]: Copied! <pre>download_unr_grid(\n        frame_id=frame_id,\n        output_dir=unr_dir,\n        start=None, # download whole record\n        end=None,\n        margin_deg=0.5, # in deg\n    )\n\nunr_files = list((unr_dir).glob(\"*.parquet\"))\nprint(f\"Downloaded {len(unr_files)} UNR station files\")\nfor f in unr_files:\n    print(f\"  {f.name}\")\n</pre> download_unr_grid(         frame_id=frame_id,         output_dir=unr_dir,         start=None, # download whole record         end=None,         margin_deg=0.5, # in deg     )  unr_files = list((unr_dir).glob(\"*.parquet\")) print(f\"Downloaded {len(unr_files)} UNR station files\") for f in unr_files:     print(f\"  {f.name}\") <pre>Loading GPS data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 169/169 [00:09&lt;00:00, 16.98it/s]\n</pre> <pre>Downloaded 1 UNR station files\n  unr_grid_frame8882.parquet\n</pre> In\u00a0[8]: Copied! <pre># Vizualize\nimport geopandas as gpd\nimport pandas as pd\n\ndf = pd.read_parquet(unr_files[0])\ngdf = gpd.GeoDataFrame(df,\n                       geometry=gpd.points_from_xy(x=df.lon,\n                                                   y=df.lat),\n                       crs=4326)\n\nout = (\n    gdf.groupby('id', as_index=False)\n       .first()\n       .set_crs(epsg=4326)\n)\n\nout.explore(column=\"east\")\n</pre> # Vizualize import geopandas as gpd import pandas as pd  df = pd.read_parquet(unr_files[0]) gdf = gpd.GeoDataFrame(df,                        geometry=gpd.points_from_xy(x=df.lon,                                                    y=df.lat),                        crs=4326)  out = (     gdf.groupby('id', as_index=False)        .first()        .set_crs(epsg=4326) )  out.explore(column=\"east\") Out[8]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[9]: Copied! <pre>sensing_times = extract_sensing_times_from_file(disp_files[0])\nsensing_times\n\nfor sensing_time in sensing_times:\n    print(f\"Downloading burst bounds for {sensing_time}\")\n    generate_s1_burst_tiles(\n        frame_id=frame_id,\n        sensing_time=sensing_time,\n        output_dir=tiles_dir,\n    )\n\nbbounds = list((tiles_dir).glob(\"*.geojson\"))\nprint(f\"Downloaded {len(bbounds)} S1 burst geometry files\")\nfor f in bbounds:\n    print(f\"  {f.name}\")\n</pre> sensing_times = extract_sensing_times_from_file(disp_files[0]) sensing_times  for sensing_time in sensing_times:     print(f\"Downloading burst bounds for {sensing_time}\")     generate_s1_burst_tiles(         frame_id=frame_id,         sensing_time=sensing_time,         output_dir=tiles_dir,     )  bbounds = list((tiles_dir).glob(\"*.geojson\")) print(f\"Downloaded {len(bbounds)} S1 burst geometry files\") for f in bbounds:     print(f\"  {f.name}\") <pre>Downloading burst bounds for 2022-01-11 00:26:51\n</pre> <pre>Processing CSLC bounds: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 27/27 [03:51&lt;00:00,  8.57s/it]\n</pre> <pre>Downloading burst bounds for 2022-07-22 00:26:57\n</pre> <pre>Processing CSLC bounds: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 27/27 [03:46&lt;00:00,  8.40s/it]\n</pre> <pre>Downloaded 4 S1 burst geometry files\n  2023-07-17_tiles.geojson\n  2024-01-13_tiles.geojson\n  2022-01-11_tiles.geojson\n  2022-07-22_tiles.geojson\n</pre> In\u00a0[10]: Copied! <pre># Use the burst bounds to get burst overalp in the DISP merging strategy.\ngpd.read_file(str(bbounds[0].resolve())).explore()\n</pre> # Use the burst bounds to get burst overalp in the DISP merging strategy. gpd.read_file(str(bbounds[0].resolve())).explore() Out[10]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[11]: Copied! <pre>def get_dir_size(directory: Path) -&gt; float:\n    \"\"\"Get directory size in GB.\"\"\"\n    if not directory.exists():\n        return 0.0\n    total = sum(f.stat().st_size for f in directory.rglob('*') if f.is_file())\n    return total / 1024**3\n\nprint(\"Download Summary\")\nprint(\"=\"*60)\nprint(f\"Frame ID: {frame_id}\")\nprint(f\"Date range: {start_date.date()} \u2192 {end_date.date()}\")\nprint(\"\\nProducts downloaded:\")\nprint(f\"  \u2713 DISP products: {len(list(disp_dir.glob('*.nc')))} files ({get_dir_size(disp_dir):.2f} GB)\")\nprint(f\"  \u2713 UNR products: {len(list(unr_dir.glob('*.parquet')))} files ({get_dir_size(unr_dir):.2f} GB)\")\nprint(f\"  \u2713 TROPO products: {len(list(tropo_dir.glob('*.nc')))} files ({get_dir_size(tropo_dir):.2f} GB)\")\nprint(f\"\\nTotal size: {get_dir_size(data_dir):.2f} GB\")\nprint(f\"\\nData directory: {data_dir.absolute()}\")\n</pre> def get_dir_size(directory: Path) -&gt; float:     \"\"\"Get directory size in GB.\"\"\"     if not directory.exists():         return 0.0     total = sum(f.stat().st_size for f in directory.rglob('*') if f.is_file())     return total / 1024**3  print(\"Download Summary\") print(\"=\"*60) print(f\"Frame ID: {frame_id}\") print(f\"Date range: {start_date.date()} \u2192 {end_date.date()}\") print(\"\\nProducts downloaded:\") print(f\"  \u2713 DISP products: {len(list(disp_dir.glob('*.nc')))} files ({get_dir_size(disp_dir):.2f} GB)\") print(f\"  \u2713 UNR products: {len(list(unr_dir.glob('*.parquet')))} files ({get_dir_size(unr_dir):.2f} GB)\") print(f\"  \u2713 TROPO products: {len(list(tropo_dir.glob('*.nc')))} files ({get_dir_size(tropo_dir):.2f} GB)\") print(f\"\\nTotal size: {get_dir_size(data_dir):.2f} GB\") print(f\"\\nData directory: {data_dir.absolute()}\") <pre>Download Summary\n============================================================\nFrame ID: 8882\nDate range: 2022-07-22 \u2192 2022-07-22\n\nProducts downloaded:\n  \u2713 DISP products: 7 files (3.62 GB)\n  \u2713 UNR products: 1 files (0.02 GB)\n  \u2713 TROPO products: 6 files (12.32 GB)\n\nTotal size: 26.89 GB\n\nData directory: /u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data\n</pre>"},{"location":"tutorials/download/#download","title":"Download\u00b6","text":"<p>This notebook demonstrates downloading all required products: 0. Burst Database - Frame geometries</p> <ol> <li>DISP-S1 - Displacement products</li> <li>DISP-S1-STATIC - DISP static layers DEM, ENU LOS unit vector</li> <li>OPERA-TROPO-ZENITH - Atmospheric delay</li> <li>UNR GNSS - Ground truth data</li> <li>S1 Burst Bounds - S1 burst bounds used for DISP-S1</li> </ol> <p>Each section shows both Python API and CLI usage.</p>"},{"location":"tutorials/download/#find-opera-disp-s1-frame-id","title":"Find OPERA DISP-S1 frame id\u00b6","text":""},{"location":"tutorials/download/#configuration","title":"Configuration\u00b6","text":""},{"location":"tutorials/download/#1-download-disp-s1-products","title":"1. Download DISP-S1 Products\u00b6","text":"<p>Download displacement products for a specific frame and time range.</p>"},{"location":"tutorials/download/#python-api","title":"Python API\u00b6","text":""},{"location":"tutorials/download/#cli-equivalent","title":"CLI Equivalent\u00b6","text":"<pre>cal-disp download disp-s1 \\\n  --frame-id 8882 \\\n  -s 2022-07-22 \\\n  -e 2022-07-22 \\\n  -o data/disp_input\n</pre>"},{"location":"tutorials/download/#2-download-disp-s1_static-data","title":"2. Download DISP-S1_STATIC Data\u00b6","text":"<p>Download OPERA DISP-S1 STATIC data for the frame bounds:</p> <ul> <li>OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_dem.tif</li> <li>OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_layover_shadow_mask.tif</li> <li>OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_line_of_sight_enu.tif</li> </ul> <p>TBD</p>"},{"location":"tutorials/download/#3-download-tropo-zenith-products","title":"3. Download TROPO-ZENITH Products\u00b6","text":"<p>Download tropospheric products for the DISP acquisition dates.</p>"},{"location":"tutorials/download/#python-api","title":"Python API\u00b6","text":""},{"location":"tutorials/download/#cli-equivalent","title":"CLI Equivalent\u00b6","text":"<pre>cal-disp download tropo \\\n  -i data/disp_input/OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc \\\n  -o data/tropo_input \\\n  --interp\n</pre>"},{"location":"tutorials/download/#4-download-unr-gnss-data","title":"4. Download UNR GNSS Data\u00b6","text":"<p>Download UNR GNSS grid for the frame.</p>"},{"location":"tutorials/download/#python-api","title":"Python API\u00b6","text":""},{"location":"tutorials/download/#cli-equivalent","title":"CLI Equivalent\u00b6","text":"<pre>cal-disp download unr \\\n  --frame-id 8882 \\\n  -o data/unr_input/\n</pre>"},{"location":"tutorials/download/#5-optional-download-burst-bounds","title":"5. [OPTIONAL] Download Burst Bounds\u00b6","text":"<p>Download the S1 burst boundary geometries for frame geometries.</p> <p>NOTE: useful if calibration needs to take into account burst jumps casued by ionosphere</p>"},{"location":"tutorials/download/#cli-equivalent","title":"CLI Equivalent\u00b6","text":"<pre>cal-disp download burst-bounds \\\n  -i /u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/notebooks/disp_input/OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc \\\n  -o /u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/notebooks/tiles_input/\n</pre>"},{"location":"tutorials/download/#7-download-summary","title":"7. Download Summary\u00b6","text":""},{"location":"tutorials/download/#complete-cli-workflow","title":"Complete CLI Workflow\u00b6","text":"<p>To download all products using CLI commands:</p> <pre># Set variables\nFRAME_ID=8882\nSTART_DATE=2022-07-22\nEND_DATE=2022-07-22\nDATA_DIR=data\n\n# 1. Download burst database\ncal-disp download burst-bounds -o ${DATA_DIR}/burst_db.sqlite\n\n# 2. Download DISP products\ncal-disp download disp-s1 \\\n  --frame-id ${FRAME_ID} \\\n  -s ${START_DATE} \\\n  -e ${END_DATE} \\\n  -o ${DATA_DIR}/disp_input\n\n# 3. Download TROPO products (using first DISP product)\nDISP_FILE=$(ls ${DATA_DIR}/disp_input/*.nc | head -1)\ncal-disp download tropo \\\n  -i ${DISP_FILE} \\\n  -o ${DATA_DIR}/tropo_input \\\n  --interp\n\n# 4. Download UNR GNSS data\ncal-disp download unr \\\n  --frame-id ${FRAME_ID} \\\n  -o ${DATA_DIR}/unr_input/\n\n# 5. (Optional) Download burst tiles\ncal-disp download burst-bounds \\\n  -i ${DISP_FILE} \\\n  -o ${DATA_DIR}/tiles_input/\n</pre>"},{"location":"tutorials/download/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that all data is downloaded:</p> <ol> <li>Prepare inputs - Use <code>prep/</code> module to prepare data for calibration</li> <li>Run calibration - Use <code>core/</code> module to compute calibration</li> <li>Create output - Generate <code>CalProduct</code> with results</li> </ol> <p>See next notebook: <code>02_prep_and_calibrate.ipynb</code></p>"},{"location":"tutorials/product/","title":"Product Classes","text":"In\u00a0[37]: Copied! <pre>from cal_disp.product import (\n    CalProduct,\n    DispProduct,\n    StaticLayer,\n    TropoProduct,\n    UnrGrid,\n    bounds_contains,\n    check_bounds_coverage,\n    compute_los_correction,\n    interpolate_to_dem_surface,\n)\n</pre>  from cal_disp.product import (     CalProduct,     DispProduct,     StaticLayer,     TropoProduct,     UnrGrid,     bounds_contains,     check_bounds_coverage,     compute_los_correction,     interpolate_to_dem_surface, ) In\u00a0[4]: Copied! <pre># Load DISP product\ndisp_path = \"data/disp_input/OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc\"\ndisp = DispProduct.from_path(disp_path)\n\nprint(disp)\nprint(f\"\\nFrame: {disp.frame_id}\")\nprint(f\"Mode: {disp.mode}\")\nprint(f\"Polarization: {disp.polarization}\")\nprint(f\"Baseline: {disp.baseline_days} days\")\n</pre> # Load DISP product disp_path = \"data/disp_input/OPERA_L3_DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251027T005420Z.nc\" disp = DispProduct.from_path(disp_path)  print(disp) print(f\"\\nFrame: {disp.frame_id}\") print(f\"Mode: {disp.mode}\") print(f\"Polarization: {disp.polarization}\") print(f\"Baseline: {disp.baseline_days} days\") <pre>DispProduct(frame=8882, 2022-01-11 \u2192 2022-07-22, VV)\n\nFrame: 8882\nMode: IW\nPolarization: VV\nBaseline: 192 days\n</pre> In\u00a0[5]: Copied! <pre># Open main displacement dataset\nds = disp.open_dataset()\nprint(\"Main dataset variables:\")\nprint(list(ds.data_vars))\n\n# Access displacement\ndisplacement = ds[\"displacement\"]\nprint(f\"\\nDisplacement shape: {displacement.shape}\")\nprint(f\"Units: {displacement.attrs.get('units', 'N/A')}\")\n</pre> # Open main displacement dataset ds = disp.open_dataset() print(\"Main dataset variables:\") print(list(ds.data_vars))  # Access displacement displacement = ds[\"displacement\"] print(f\"\\nDisplacement shape: {displacement.shape}\") print(f\"Units: {displacement.attrs.get('units', 'N/A')}\") <pre>Main dataset variables:\n['spatial_ref', 'reference_time', 'displacement', 'short_wavelength_displacement', 'recommended_mask', 'connected_component_labels', 'temporal_coherence', 'estimated_phase_quality', 'persistent_scatterer_mask', 'shp_counts', 'water_mask', 'phase_similarity', 'timeseries_inversion_residuals']\n\nDisplacement shape: (7733, 9464)\nUnits: meters\n</pre> In\u00a0[6]: Copied! <pre># Get bounds in different projections\nbounds_native = disp.get_bounds()\nbounds_wgs84 = disp.get_bounds_wgs84()\n\nprint(\"Native projection bounds:\")\nfor key, val in bounds_native.items():\n    print(f\"  {key}: {val:.2f}\")\n\nprint(\"\\nWGS84 bounds:\")\nfor key, val in bounds_wgs84.items():\n    print(f\"  {key}: {val:.4f}\u00b0\")\n</pre> # Get bounds in different projections bounds_native = disp.get_bounds() bounds_wgs84 = disp.get_bounds_wgs84()  print(\"Native projection bounds:\") for key, val in bounds_native.items():     print(f\"  {key}: {val:.2f}\")  print(\"\\nWGS84 bounds:\") for key, val in bounds_wgs84.items():     print(f\"  {key}: {val:.4f}\u00b0\") <pre>Native projection bounds:\n  left: 71985.00\n  bottom: 3153945.00\n  right: 355875.00\n  top: 3385905.00\n\nWGS84 bounds:\n  west: -97.4595\u00b0\n  south: 28.4420\u00b0\n  east: -94.4727\u00b0\n  north: 30.5970\u00b0\n</pre> In\u00a0[7]: Copied! <pre># Open corrections group\nds_corr = disp.open_corrections()\nprint(\"Correction layers:\")\nprint(list(ds_corr.data_vars))\n\n# Access specific correction\nif \"ionospheric_delay\" in ds_corr:\n    iono = ds_corr[\"ionospheric_delay\"]\n    print(f\"\\nIonospheric delay shape: {iono.shape}\")\n</pre> # Open corrections group ds_corr = disp.open_corrections() print(\"Correction layers:\") print(list(ds_corr.data_vars))  # Access specific correction if \"ionospheric_delay\" in ds_corr:     iono = ds_corr[\"ionospheric_delay\"]     print(f\"\\nIonospheric delay shape: {iono.shape}\") <pre>Correction layers:\n['spatial_ref', 'ionospheric_delay', 'solid_earth_tide', 'perpendicular_baseline', 'reference_point']\n\nIonospheric delay shape: (7733, 9464)\n</pre> In\u00a0[8]: Copied! <pre># Export displacement layer\noutput_path = disp.to_geotiff(\n    layer=\"displacement\",\n    output_path=\"output/displacement.tif\",\n)\nprint(f\"Exported to: {output_path}\")\n</pre> # Export displacement layer output_path = disp.to_geotiff(     layer=\"displacement\",     output_path=\"output/displacement.tif\", ) print(f\"Exported to: {output_path}\") <pre>Exported to: output/displacement.tif\n</pre> In\u00a0[11]: Copied! <pre># Load TROPO product\ntropo_path = \"data/tropo/OPERA_L4_TROPO-ZENITH_20220111T000000Z_20250923T224940Z_HRES_v1.0.nc\"\ntropo = TropoProduct.from_path(tropo_path)\n\nprint(tropo)\nprint(f\"\\nModel: {tropo.model}\")\nprint(f\"Reference time: {tropo.date}\")\n</pre> # Load TROPO product tropo_path = \"data/tropo/OPERA_L4_TROPO-ZENITH_20220111T000000Z_20250923T224940Z_HRES_v1.0.nc\" tropo = TropoProduct.from_path(tropo_path)  print(tropo) print(f\"\\nModel: {tropo.model}\") print(f\"Reference time: {tropo.date}\") <pre>TropoProduct(date=2022-01-11T00:00:00, model=HRES)\n\nModel: HRES\nReference time: 2022-01-11 00:00:00\n</pre> In\u00a0[17]: Copied! <pre># Get total zenith delay (wet + hydrostatic)\ndelay = tropo.get_total_delay(\n    time_idx=0,\n    bounds=list(bounds_wgs84.values()),\n    max_height=11e3,  # 11 km\n    bounds_buffer=0.2,\n)\n\nprint(f\"Total delay shape: {delay.shape}\")\nprint(f\"Dimensions: {delay.dims}\")\nprint(f\"Coordinates: {list(delay.coords)}\")\n</pre> # Get total zenith delay (wet + hydrostatic) delay = tropo.get_total_delay(     time_idx=0,     bounds=list(bounds_wgs84.values()),     max_height=11e3,  # 11 km     bounds_buffer=0.2, )  print(f\"Total delay shape: {delay.shape}\") print(f\"Dimensions: {delay.dims}\") print(f\"Coordinates: {list(delay.coords)}\") <pre>Total delay shape: (68, 36, 48)\nDimensions: ('height', 'latitude', 'longitude')\nCoordinates: ['time', 'height', 'latitude', 'longitude', 'spatial_ref']\n</pre> In\u00a0[23]: Copied! <pre>from cal_disp.product import interpolate_in_time\n\n# Load two TROPO products\ntropo_early = TropoProduct.from_path(\"data/tropo_input/OPERA_L4_TROPO-ZENITH_20220111T000000Z_20250923T224940Z_HRES_v1.0.nc\")\ntropo_late = TropoProduct.from_path(\"data/tropo_input/OPERA_L4_TROPO-ZENITH_20220111T060000Z_20250923T231305Z_HRES_v1.0.nc\")\n\n# Interpolate to target time\ntarget_time = disp.primary_date # DISP primary date\n\ndelay_interp = interpolate_in_time(\n    tropo_early=tropo_early,\n    tropo_late=tropo_late,\n    target_datetime=target_time,\n    bounds=list(bounds_wgs84.values()),\n    max_height=11e3,\n    bounds_buffer=0.2,\n)\n\nprint(f\"Interpolated delay shape: {delay_interp.shape}\")\nprint(f\"Target date: {delay_interp.attrs.get('target_date')}\")\nprint(f\"Interpolation weight: {delay_interp.attrs.get('interpolation_weight'):.4f}\")\n</pre> from cal_disp.product import interpolate_in_time  # Load two TROPO products tropo_early = TropoProduct.from_path(\"data/tropo_input/OPERA_L4_TROPO-ZENITH_20220111T000000Z_20250923T224940Z_HRES_v1.0.nc\") tropo_late = TropoProduct.from_path(\"data/tropo_input/OPERA_L4_TROPO-ZENITH_20220111T060000Z_20250923T231305Z_HRES_v1.0.nc\")  # Interpolate to target time target_time = disp.primary_date # DISP primary date  delay_interp = interpolate_in_time(     tropo_early=tropo_early,     tropo_late=tropo_late,     target_datetime=target_time,     bounds=list(bounds_wgs84.values()),     max_height=11e3,     bounds_buffer=0.2, )  print(f\"Interpolated delay shape: {delay_interp.shape}\") print(f\"Target date: {delay_interp.attrs.get('target_date')}\") print(f\"Interpolation weight: {delay_interp.attrs.get('interpolation_weight'):.4f}\") <pre>Interpolated delay shape: (68, 36, 48)\nTarget date: 2022-01-11T00:26:51\nInterpolation weight: 0.0746\n</pre> In\u00a0[31]: Copied! <pre># Load TROPO files for reference and secondary datetime\nref_tropo_1 = TropoProduct.from_path('/u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data/tropo_input/OPERA_L4_TROPO-ZENITH_20220111T000000Z_20250923T224940Z_HRES_v1.0.nc')\nref_tropo_2 = TropoProduct.from_path('/u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data/tropo_input/OPERA_L4_TROPO-ZENITH_20220111T060000Z_20250923T231305Z_HRES_v1.0.nc')\nsec_tropo_1 = TropoProduct.from_path('/u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data/tropo_input/OPERA_L4_TROPO-ZENITH_20220722T000000Z_20250923T233421Z_HRES_v1.0.nc')\nsec_tropo_2 = TropoProduct.from_path('/u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data/tropo_input/OPERA_L4_TROPO-ZENITH_20220722T060000Z_20250923T230822Z_HRES_v1.0.nc')\n</pre> # Load TROPO files for reference and secondary datetime ref_tropo_1 = TropoProduct.from_path('/u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data/tropo_input/OPERA_L4_TROPO-ZENITH_20220111T000000Z_20250923T224940Z_HRES_v1.0.nc') ref_tropo_2 = TropoProduct.from_path('/u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data/tropo_input/OPERA_L4_TROPO-ZENITH_20220111T060000Z_20250923T231305Z_HRES_v1.0.nc') sec_tropo_1 = TropoProduct.from_path('/u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data/tropo_input/OPERA_L4_TROPO-ZENITH_20220722T000000Z_20250923T233421Z_HRES_v1.0.nc') sec_tropo_2 = TropoProduct.from_path('/u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data/tropo_input/OPERA_L4_TROPO-ZENITH_20220722T060000Z_20250923T230822Z_HRES_v1.0.nc') In\u00a0[32]: Copied! <pre># Load DEM\ndem_path = \"data/static_input/OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_dem.tif\"\ndem = StaticLayer.from_path(dem_path)\ndem = dem.to_dataset()\ndem = dem.rio.write_crs(dem.attrs['crs_wkt'])\nmax_height = dem.dem.max().values + 3e3\n</pre> # Load DEM dem_path = \"data/static_input/OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_dem.tif\" dem = StaticLayer.from_path(dem_path) dem = dem.to_dataset() dem = dem.rio.write_crs(dem.attrs['crs_wkt']) max_height = dem.dem.max().values + 3e3 In\u00a0[34]: Copied! <pre># Interpolate nearest TROPO file to DISP reference and secondary datetime\ntropo_ref_ds = interpolate_in_time(ref_tropo_1, ref_tropo_2,\n                    target_datetime=disp.primary_date,\n                    bounds=list(disp.get_bounds_wgs84().values()),\n                    max_height=max_height, bounds_buffer=0.2)\n\ntropo_sec_ds = interpolate_in_time(sec_tropo_1, sec_tropo_2,\n                    target_datetime=disp.secondary_date,\n                    bounds=list(disp.get_bounds_wgs84().values()),\n                    max_height=max_height, bounds_buffer=0.2)\n</pre> # Interpolate nearest TROPO file to DISP reference and secondary datetime tropo_ref_ds = interpolate_in_time(ref_tropo_1, ref_tropo_2,                     target_datetime=disp.primary_date,                     bounds=list(disp.get_bounds_wgs84().values()),                     max_height=max_height, bounds_buffer=0.2)  tropo_sec_ds = interpolate_in_time(sec_tropo_1, sec_tropo_2,                     target_datetime=disp.secondary_date,                     bounds=list(disp.get_bounds_wgs84().values()),                     max_height=max_height, bounds_buffer=0.2) In\u00a0[36]: Copied! <pre># Intersect TROPO to DEM elevation\ntropo_ref_dem = interpolate_to_dem_surface(tropo_ref_ds, dem['dem'])\ntropo_sec_dem = interpolate_to_dem_surface(tropo_sec_ds, dem['dem'])\n</pre> # Intersect TROPO to DEM elevation tropo_ref_dem = interpolate_to_dem_surface(tropo_ref_ds, dem['dem']) tropo_sec_dem = interpolate_to_dem_surface(tropo_sec_ds, dem['dem']) In\u00a0[38]: Copied! <pre># Load LOS dataset\nlos_path = \"data/static_input/OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_line_of_sight_enu.tif\"\nlos = StaticLayer.from_path(los_path)\nlos_ds = los.to_dataset()\n\n# Project zentith to LOS slant range and save as geotif\n_ = compute_los_correction(tropo_ref_dem,\n                       los_ds['los_up'],\n                       output_path=f'/u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data/tropo_input/tropospheric_correction_{disp.primary_date.date().strftime(\"%Y%m%d\")}.tif')\n_ = compute_los_correction(tropo_sec_dem,\n                       los_ds['los_up'],\n                       output_path=f'/u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data/tropo_input/tropospheric_correction_{disp.secondary_date.date().strftime(\"%Y%m%d\")}.tif')\n</pre> # Load LOS dataset los_path = \"data/static_input/OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_line_of_sight_enu.tif\" los = StaticLayer.from_path(los_path) los_ds = los.to_dataset()  # Project zentith to LOS slant range and save as geotif _ = compute_los_correction(tropo_ref_dem,                        los_ds['los_up'],                        output_path=f'/u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data/tropo_input/tropospheric_correction_{disp.primary_date.date().strftime(\"%Y%m%d\")}.tif') _ = compute_los_correction(tropo_sec_dem,                        los_ds['los_up'],                        output_path=f'/u/aurora-r0/govorcin/01_OPERA/CAL/cal-disp/data/tropo_input/tropospheric_correction_{disp.secondary_date.date().strftime(\"%Y%m%d\")}.tif') In\u00a0[27]: Copied! <pre># Load DEM layer\ndem_path = \"data/static_input/OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_dem.tif\"\ndem = StaticLayer.from_path(dem_path)\n\nprint(dem)\nprint(f\"\\nLayer: {dem.layer_type}\")\nprint(f\"Frame: {dem.frame_id}\")\n</pre> # Load DEM layer dem_path = \"data/static_input/OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_dem.tif\" dem = StaticLayer.from_path(dem_path)  print(dem) print(f\"\\nLayer: {dem.layer_type}\") print(f\"Frame: {dem.frame_id}\") <pre>StaticLayer(frame=8882, layer=dem, bands=1)\n\nLayer: dem\nFrame: 8882\n</pre> In\u00a0[28]: Copied! <pre># Convert to xarray Dataset\nds_dem = dem.to_dataset()\nprint(\"DEM dataset:\")\nprint(ds_dem)\n\n# Access DEM data\ndem_data = ds_dem[\"dem\"]\nprint(f\"\\nDEM shape: {dem_data.shape}\")\nprint(f\"Min elevation: {float(dem_data.min()):.1f} m\")\nprint(f\"Max elevation: {float(dem_data.max()):.1f} m\")\n</pre> # Convert to xarray Dataset ds_dem = dem.to_dataset() print(\"DEM dataset:\") print(ds_dem)  # Access DEM data dem_data = ds_dem[\"dem\"] print(f\"\\nDEM shape: {dem_data.shape}\") print(f\"Min elevation: {float(dem_data.min()):.1f} m\") print(f\"Max elevation: {float(dem_data.max()):.1f} m\") <pre>DEM dataset:\n&lt;xarray.Dataset&gt; Size: 293MB\nDimensions:  (y: 7733, x: 9464)\nCoordinates:\n  * y        (y) float64 62kB 3.386e+06 3.386e+06 ... 3.154e+06 3.154e+06\n  * x        (x) float64 76kB 7.198e+04 7.202e+04 ... 3.558e+05 3.559e+05\nData variables:\n    dem      (y, x) float32 293MB 161.6 160.5 160.6 ... -26.86 -26.86 -26.86\nAttributes:\n    frame_id:        8882\n    satellite:       S1A\n    version:         1.0\n    reference_date:  2014-04-03T00:00:00\n    layer_type:      dem\n    crs_wkt:         PROJCS[\"WGS 84 / UTM zone 15N\",GEOGCS[\"WGS 84\",DATUM[\"WG...\n    epsg:            32615\n    transform:       [30.0, 0.0, 71970.0, 0.0, -30.0, 3385920.0, 0.0, 0.0, 1.0]\n    nodata:          None\n\nDEM shape: (7733, 9464)\nMin elevation: -41.4 m\nMax elevation: 205.2 m\n</pre> In\u00a0[29]: Copied! <pre># Load LOS layer\nlos_path = \"data/static_input/OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_line_of_sight_enu.tif\"\nlos = StaticLayer.from_path(los_path)\n\nprint(los)\nprint(f\"\\nLayer: {los.layer_type}\")\nprint(f\"Frame: {los.frame_id}\")\n</pre> # Load LOS layer los_path = \"data/static_input/OPERA_L3_DISP-S1-STATIC_F08882_20140403_S1A_v1.0_line_of_sight_enu.tif\" los = StaticLayer.from_path(los_path)  print(los) print(f\"\\nLayer: {los.layer_type}\") print(f\"Frame: {los.frame_id}\") <pre>StaticLayer(frame=8882, layer=line_of_sight_enu, bands=3)\n\nLayer: line_of_sight_enu\nFrame: 8882\n</pre> In\u00a0[30]: Copied! <pre>from matplotlib import pyplot as plt\n\nlos_ds = los.to_dataset()\nfig, ax = plt.subplots(1,3, figsize=(12,4),sharey=True)\nim1= los_ds.los_east.plot.imshow(ax=ax[0], add_colorbar=False)\nim2= los_ds.los_north.plot.imshow(ax=ax[1], add_colorbar=False)\nim3= los_ds.los_up.plot.imshow(ax=ax[2], add_colorbar=False)\nfor im, txt, a in zip([im1, im2, im3], ['EW', 'NS', 'UD'],ax):\n    fig.colorbar(im, ax=a, location='bottom')\n    a.set_title(txt)\n</pre> from matplotlib import pyplot as plt  los_ds = los.to_dataset() fig, ax = plt.subplots(1,3, figsize=(12,4),sharey=True) im1= los_ds.los_east.plot.imshow(ax=ax[0], add_colorbar=False) im2= los_ds.los_north.plot.imshow(ax=ax[1], add_colorbar=False) im3= los_ds.los_up.plot.imshow(ax=ax[2], add_colorbar=False) for im, txt, a in zip([im1, im2, im3], ['EW', 'NS', 'UD'],ax):     fig.colorbar(im, ax=a, location='bottom')     a.set_title(txt) In\u00a0[40]: Copied! <pre># Load UNR grid\nunr_path = \"data/unr_input/unr_grid_frame8882.parquet\"\nunr = UnrGrid.from_path(unr_path)\n\nprint(unr)\nprint(f\"\\nFrame ID: {unr.frame_id}\")\nprint(f\"Stations: {unr.get_grid_count()}\")\n</pre> # Load UNR grid unr_path = \"data/unr_input/unr_grid_frame8882.parquet\" unr = UnrGrid.from_path(unr_path)  print(unr) print(f\"\\nFrame ID: {unr.frame_id}\") print(f\"Stations: {unr.get_grid_count()}\") <pre>UnrGrid(frame=8882, points=169)\n\nFrame ID: 8882\nStations: 169\n</pre> In\u00a0[43]: Copied! <pre># Load data\ngdf = unr.load()\n\nprint(\"GNSS stations:\")\nprint(gdf[['id', 'lon', 'lat', 'east', 'north', 'up']].head())\n\n# Get bounds\nbounds = unr.get_bounds()\nprint(\"\\nBounds:\")\nfor key, val in bounds.items():\n    print(f\"  {key}: {val:.4f}\u00b0\")\n</pre> # Load data gdf = unr.load()  print(\"GNSS stations:\") print(gdf[['id', 'lon', 'lat', 'east', 'north', 'up']].head())  # Get bounds bounds = unr.get_bounds() print(\"\\nBounds:\") for key, val in bounds.items():     print(f\"  {key}: {val:.4f}\u00b0\") <pre>GNSS stations:\n     id       lon       lat      east     north        up\n0  4570 -95.96719  28.62083  0.078726  0.014528  0.020513\n1  4570 -95.96719  28.62083  0.080680  0.012950  0.019130\n2  4570 -95.96719  28.62083  0.079684  0.013302  0.017483\n3  4570 -95.96719  28.62083  0.080020  0.012809  0.016243\n4  4570 -95.96719  28.62083  0.078697  0.013651  0.022810\n\nBounds:\n  west: -97.8665\u00b0\n  south: 28.1892\u00b0\n  east: -94.0336\u00b0\n  north: 30.9950\u00b0\n</pre> In\u00a0[44]: Copied! <pre># Get metadata from parquet file\nmetadata = unr.get_metadata()\nprint(\"Metadata:\")\nfor key, val in metadata.items():\n    print(f\"  {key}: {val}\")\n</pre> # Get metadata from parquet file metadata = unr.get_metadata() print(\"Metadata:\") for key, val in metadata.items():     print(f\"  {key}: {val}\") <pre>Metadata:\n  reference_frame: IGS20\n  ARROW:schema: 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\n  source: UNR grid\n  frame_id: 8882\n  description: Time series of east/north/up displacements and uncertainties\n  pandas: {\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"stop\": 850070, \"step\": 1}], \"column_indexes\": [{\"name\": null, \"field_name\": null, \"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": {\"encoding\": \"UTF-8\"}}], \"columns\": [{\"name\": \"id\", \"field_name\": \"id\", \"pandas_type\": \"int64\", \"numpy_type\": \"int64\", \"metadata\": null}, {\"name\": \"date\", \"field_name\": \"date\", \"pandas_type\": \"datetime\", \"numpy_type\": \"datetime64[ns]\", \"metadata\": null}, {\"name\": \"east\", \"field_name\": \"east\", \"pandas_type\": \"float64\", \"numpy_type\": \"float64\", \"metadata\": null}, {\"name\": \"north\", \"field_name\": \"north\", \"pandas_type\": \"float64\", \"numpy_type\": \"float64\", \"metadata\": null}, {\"name\": \"up\", \"field_name\": \"up\", \"pandas_type\": \"float64\", \"numpy_type\": \"float64\", \"metadata\": null}, {\"name\": \"sigma_east\", \"field_name\": \"sigma_east\", \"pandas_type\": \"float64\", \"numpy_type\": \"float64\", \"metadata\": null}, {\"name\": \"sigma_north\", \"field_name\": \"sigma_north\", \"pandas_type\": \"float64\", \"numpy_type\": \"float64\", \"metadata\": null}, {\"name\": \"sigma_up\", \"field_name\": \"sigma_up\", \"pandas_type\": \"float64\", \"numpy_type\": \"float64\", \"metadata\": null}, {\"name\": \"corr_en\", \"field_name\": \"corr_en\", \"pandas_type\": \"float64\", \"numpy_type\": \"float64\", \"metadata\": null}, {\"name\": \"corr_eu\", \"field_name\": \"corr_eu\", \"pandas_type\": \"float64\", \"numpy_type\": \"float64\", \"metadata\": null}, {\"name\": \"corr_nu\", \"field_name\": \"corr_nu\", \"pandas_type\": \"float64\", \"numpy_type\": \"float64\", \"metadata\": null}, {\"name\": \"lon\", \"field_name\": \"lon\", \"pandas_type\": \"float64\", \"numpy_type\": \"float64\", \"metadata\": null}, {\"name\": \"lat\", \"field_name\": \"lat\", \"pandas_type\": \"float64\", \"numpy_type\": \"float64\", \"metadata\": null}, {\"name\": \"alt\", \"field_name\": \"alt\", \"pandas_type\": \"float64\", \"numpy_type\": \"float64\", \"metadata\": null}], \"attributes\": {}, \"creator\": {\"library\": \"pyarrow\", \"version\": \"22.0.0\"}, \"pandas_version\": \"2.3.3\"}\n</pre> In\u00a0[45]: Copied! <pre>import numpy as np\nimport xarray as xr\n\n# Create sample calibration data (zeros for demo)\nds_disp = disp.open_dataset()\nshape = ds_disp[\"displacement\"].shape\ny = ds_disp.y.values\nx = ds_disp.x.values\n\n# Full resolution calibration\ncalibration = xr.DataArray(\n    np.zeros(shape, dtype=np.float32),\n    coords={\"y\": y, \"x\": x},\n    dims=[\"y\", \"x\"],\n    attrs={\"units\": \"meters\"},\n)\n\ncalibration_std = xr.DataArray(\n    np.zeros(shape, dtype=np.float32),\n    coords={\"y\": y, \"x\": x},\n    dims=[\"y\", \"x\"],\n    attrs={\"units\": \"meters\"},\n)\n\n# Coarse resolution 3D model (every 167th point ~ 5km)\ncoarse_y = y[::167]\ncoarse_x = x[::167]\ncoarse_shape = (len(coarse_y), len(coarse_x))\n\nmodel_3d = {\n    \"north_south\": xr.DataArray(\n        np.zeros(coarse_shape, dtype=np.float32),\n        coords={\"y\": coarse_y, \"x\": coarse_x},\n        dims=[\"y\", \"x\"],\n    ),\n    \"east_west\": xr.DataArray(\n        np.zeros(coarse_shape, dtype=np.float32),\n        coords={\"y\": coarse_y, \"x\": coarse_x},\n        dims=[\"y\", \"x\"],\n    ),\n    \"up_down\": xr.DataArray(\n        np.zeros(coarse_shape, dtype=np.float32),\n        coords={\"y\": coarse_y, \"x\": coarse_x},\n        dims=[\"y\", \"x\"],\n    ),\n}\n\n# Create CalProduct\ncal = CalProduct.create(\n    calibration=calibration,\n    disp_product=disp,\n    output_dir=\"output/\",\n    sensor=\"S1\",\n    calibration_std=calibration_std,\n    model_3d=model_3d,\n    metadata={\n        \"gnss_reference_epoch\": \"2020-01-01T00:00:00Z\",\n        \"model_3d_resolution\": \"5km\",\n        \"calibration_resolution\": \"30m\",\n    },\n    version=\"1.0\",\n)\n\nprint(f\"Created: {cal.filename}\")\nprint(cal)\n</pre> import numpy as np import xarray as xr  # Create sample calibration data (zeros for demo) ds_disp = disp.open_dataset() shape = ds_disp[\"displacement\"].shape y = ds_disp.y.values x = ds_disp.x.values  # Full resolution calibration calibration = xr.DataArray(     np.zeros(shape, dtype=np.float32),     coords={\"y\": y, \"x\": x},     dims=[\"y\", \"x\"],     attrs={\"units\": \"meters\"}, )  calibration_std = xr.DataArray(     np.zeros(shape, dtype=np.float32),     coords={\"y\": y, \"x\": x},     dims=[\"y\", \"x\"],     attrs={\"units\": \"meters\"}, )  # Coarse resolution 3D model (every 167th point ~ 5km) coarse_y = y[::167] coarse_x = x[::167] coarse_shape = (len(coarse_y), len(coarse_x))  model_3d = {     \"north_south\": xr.DataArray(         np.zeros(coarse_shape, dtype=np.float32),         coords={\"y\": coarse_y, \"x\": coarse_x},         dims=[\"y\", \"x\"],     ),     \"east_west\": xr.DataArray(         np.zeros(coarse_shape, dtype=np.float32),         coords={\"y\": coarse_y, \"x\": coarse_x},         dims=[\"y\", \"x\"],     ),     \"up_down\": xr.DataArray(         np.zeros(coarse_shape, dtype=np.float32),         coords={\"y\": coarse_y, \"x\": coarse_x},         dims=[\"y\", \"x\"],     ), }  # Create CalProduct cal = CalProduct.create(     calibration=calibration,     disp_product=disp,     output_dir=\"output/\",     sensor=\"S1\",     calibration_std=calibration_std,     model_3d=model_3d,     metadata={         \"gnss_reference_epoch\": \"2020-01-01T00:00:00Z\",         \"model_3d_resolution\": \"5km\",         \"calibration_resolution\": \"30m\",     },     version=\"1.0\", )  print(f\"Created: {cal.filename}\") print(cal) <pre>Created: OPERA_L4_CAL-DISP-S1_IW_F08882_VV_20220111T002651Z_20220722T002657Z_v1.0_20251231T040308Z.nc\nCalProduct(sensor=S1, frame=8882, 2022-01-11 \u2192 2022-07-22, VV+model_3d)\n</pre> In\u00a0[46]: Copied! <pre># Open main group\nds_main = cal.open_dataset()\nprint(\"Main group variables:\")\nprint(list(ds_main.data_vars))\nprint(f\"Calibration shape: {ds_main['calibration'].shape}\")\n\n# Check for model_3d group\nif cal.has_model_3d():\n    ds_model = cal.open_model_3d()\n    print(\"\\nModel 3D group variables:\")\n    print(list(ds_model.data_vars))\n    print(f\"Up-down shape: {ds_model['up_down'].shape}\")\n</pre> # Open main group ds_main = cal.open_dataset() print(\"Main group variables:\") print(list(ds_main.data_vars)) print(f\"Calibration shape: {ds_main['calibration'].shape}\")  # Check for model_3d group if cal.has_model_3d():     ds_model = cal.open_model_3d()     print(\"\\nModel 3D group variables:\")     print(list(ds_model.data_vars))     print(f\"Up-down shape: {ds_model['up_down'].shape}\") <pre>Main group variables:\n['calibration', 'calibration_std']\nCalibration shape: (7733, 9464)\n\nModel 3D group variables:\n['north_south', 'east_west', 'up_down']\nUp-down shape: (47, 57)\n</pre> In\u00a0[47]: Copied! <pre># Export main calibration\ncal.to_geotiff(\"calibration\", \"output/calibration.tif\")\n\n# Export 3D model component\nif cal.has_model_3d():\n    cal.to_geotiff(\"up_down\", \"output/model_up.tif\", group=\"model_3d\")\n\nprint(\"Exported calibration layers\")\n</pre> # Export main calibration cal.to_geotiff(\"calibration\", \"output/calibration.tif\")  # Export 3D model component if cal.has_model_3d():     cal.to_geotiff(\"up_down\", \"output/model_up.tif\", group=\"model_3d\")  print(\"Exported calibration layers\") <pre>Exported calibration layers\n</pre> In\u00a0[48]: Copied! <pre># Get statistics\nsummary = cal.get_calibration_summary()\n\nprint(\"Calibration Summary:\")\nprint(\"\\nMain group:\")\nfor var, stats in summary[\"main\"].items():\n    print(f\"  {var}:\")\n    print(f\"    mean = {stats['mean']:.6f}\")\n    print(f\"    std  = {stats['std']:.6f}\")\n\nif \"model_3d\" in summary:\n    print(\"\\nModel 3D group:\")\n    for var, stats in summary[\"model_3d\"].items():\n        print(f\"  {var}: mean = {stats['mean']:.6f}\")\n</pre> # Get statistics summary = cal.get_calibration_summary()  print(\"Calibration Summary:\") print(\"\\nMain group:\") for var, stats in summary[\"main\"].items():     print(f\"  {var}:\")     print(f\"    mean = {stats['mean']:.6f}\")     print(f\"    std  = {stats['std']:.6f}\")  if \"model_3d\" in summary:     print(\"\\nModel 3D group:\")     for var, stats in summary[\"model_3d\"].items():         print(f\"  {var}: mean = {stats['mean']:.6f}\") <pre>Calibration Summary:\n\nMain group:\n  calibration:\n    mean = 0.000000\n    std  = 0.000000\n  calibration_std:\n    mean = 0.000000\n    std  = 0.000000\n\nModel 3D group:\n  north_south: mean = 0.000000\n  east_west: mean = 0.000000\n  up_down: mean = 0.000000\n</pre> In\u00a0[49]: Copied! <pre># Check if UNR covers DISP\ndisp_bounds = disp.get_bounds_wgs84()\nunr_bounds = unr.get_bounds()\n\n# Simple containment check\ncontains = bounds_contains(unr_bounds, disp_bounds, buffer=0.2)\nprint(f\"UNR covers DISP (with 0.2\u00b0 buffer): {contains}\")\n</pre> # Check if UNR covers DISP disp_bounds = disp.get_bounds_wgs84() unr_bounds = unr.get_bounds()  # Simple containment check contains = bounds_contains(unr_bounds, disp_bounds, buffer=0.2) print(f\"UNR covers DISP (with 0.2\u00b0 buffer): {contains}\") <pre>UNR covers DISP (with 0.2\u00b0 buffer): True\n</pre> In\u00a0[52]: Copied! <pre># Detailed coverage analysis\ncoverage = check_bounds_coverage(disp_bounds, unr_bounds, buffer=0.2)\n\nprint(f\"Contains: {coverage['contains']}\")\nif not coverage['contains']:\n    print(\"\\nGaps:\")\n    for direction, gap in coverage['gaps'].items():\n        if gap &gt; 0:\n            print(f\"  {direction}: {gap:.4f}\u00b0 missing\")\n        elif gap &lt; 0:\n            print(f\"  {direction}: {abs(gap):.4f}\u00b0 extra coverage\")\n</pre> # Detailed coverage analysis coverage = check_bounds_coverage(disp_bounds, unr_bounds, buffer=0.2)  print(f\"Contains: {coverage['contains']}\") if not coverage['contains']:     print(\"\\nGaps:\")     for direction, gap in coverage['gaps'].items():         if gap &gt; 0:             print(f\"  {direction}: {gap:.4f}\u00b0 missing\")         elif gap &lt; 0:             print(f\"  {direction}: {abs(gap):.4f}\u00b0 extra coverage\") <pre>Contains: True\n</pre>"},{"location":"tutorials/product/#product-classes","title":"Product Classes\u00b6","text":"<p>Learn how to work with OPERA products using clean, minimal Python classes:</p> <ul> <li>DispProduct - DISP-S1 displacement</li> <li>TropoProduct - TROPO-ZENITH atmospheric delay</li> <li>StaticLayer - DISP-S1-STATIC layers (DEM, LOS, masks)</li> <li>UnrGrid - UNR GNSS data</li> <li>CalProduct - CAL-DISP calibration</li> </ul>"},{"location":"tutorials/product/#1-dispproduct-displacement-products","title":"1. DispProduct - Displacement Products\u00b6","text":"<p>Load and work with OPERA DISP-S1 displacement products.</p>"},{"location":"tutorials/product/#load-from-path","title":"Load from path\u00b6","text":""},{"location":"tutorials/product/#open-dataset","title":"Open dataset\u00b6","text":""},{"location":"tutorials/product/#get-bounds","title":"Get bounds\u00b6","text":""},{"location":"tutorials/product/#access-corrections-group","title":"Access corrections group\u00b6","text":""},{"location":"tutorials/product/#export-to-geotiff","title":"Export to GeoTIFF\u00b6","text":""},{"location":"tutorials/product/#2-tropoproduct-tropospheric-products","title":"2. TropoProduct - Tropospheric Products\u00b6","text":"<p>Work with OPERA TROPO-ZENITH atmospheric delay products.</p>"},{"location":"tutorials/product/#load-and-inspect","title":"Load and inspect\u00b6","text":""},{"location":"tutorials/product/#get-total-delay","title":"Get total delay\u00b6","text":""},{"location":"tutorials/product/#temporal-interpolation","title":"Temporal interpolation\u00b6","text":""},{"location":"tutorials/product/#intersect-with-dem-and-project-to-los","title":"Intersect with DEM and project to LOS\u00b6","text":""},{"location":"tutorials/product/#3-staticlayer-static-layers","title":"3. StaticLayer - Static Layers\u00b6","text":"<p>Work with OPERA DISP-S1-STATIC layers (DEM, LOS, masks).</p>"},{"location":"tutorials/product/#load-individual-layer","title":"Load individual layer\u00b6","text":""},{"location":"tutorials/product/#convert-to-dataset","title":"Convert to dataset\u00b6","text":""},{"location":"tutorials/product/#4-unrgrid-gnss-data","title":"4. UnrGrid - GNSS Data\u00b6","text":"<p>Work with UNR GNSS grid data.</p>"},{"location":"tutorials/product/#load-and-inspect","title":"Load and inspect\u00b6","text":""},{"location":"tutorials/product/#load-as-geodataframe","title":"Load as GeoDataFrame\u00b6","text":""},{"location":"tutorials/product/#get-metadata","title":"Get metadata\u00b6","text":""},{"location":"tutorials/product/#5-calproduct-calibration-products","title":"5. CalProduct - Calibration Products\u00b6","text":"<p>Create and work with calibration products.</p>"},{"location":"tutorials/product/#create-calproduct","title":"Create CalProduct\u00b6","text":""},{"location":"tutorials/product/#access-calproduct-groups","title":"Access CalProduct groups\u00b6","text":""},{"location":"tutorials/product/#export-calproduct-layers","title":"Export CalProduct layers\u00b6","text":""},{"location":"tutorials/product/#get-calibration-summary","title":"Get calibration summary\u00b6","text":""},{"location":"tutorials/product/#6-utility-functions","title":"6. Utility Functions\u00b6","text":"<p>Use bounds checking utilities.</p>"},{"location":"tutorials/product/#check-if-bounds-contain","title":"Check if bounds contain\u00b6","text":""},{"location":"tutorials/product/#detailed-coverage-check","title":"Detailed coverage check\u00b6","text":""},{"location":"tutorials/product/#summary","title":"Summary\u00b6","text":"<p>You've learned how to:</p> <ul> <li>Load and inspect DISP, TROPO, Static, UNR, and Cal products</li> <li>Access product metadata and bounds</li> <li>Open datasets and access data arrays</li> <li>Export layers to GeoTIFF</li> <li>Work with multi-group NetCDF files (CalProduct)</li> <li>Use utility functions for bounds checking</li> </ul> <p>Next steps: See workflow tutorials for end-to-end calibration pipelines.</p>"}]}